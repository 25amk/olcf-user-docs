

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Summit User Guide &mdash; OLCF User Documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme_overrides.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  
    <link rel="canonical" href="https://docs.olcf.ornl.govsystems/summit_user_guide.html"/>
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Rhea User Guide" href="rhea_user_guide.html" />
    <link rel="prev" title="Systems" href="index.html" />

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #efefef" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/olcf_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/index.html">New User Quick Start</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../accounts/index.html">Accounts and Projects</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html">Request a New Allocation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#what-are-the-differences-between-project-types">What are the differences between project types?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#what-happens-after-a-project-request-is-approved">What happens after a project request is approved?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/accounts_and_projects.html#guidance-on-summit-allocation-requests">Guidance on Summit Allocation Requests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#applying-for-a-user-account">Applying for a user account</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#checking-the-status-of-your-application">Checking the status of your application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/accounts_and_projects.html#get-access-to-additional-projects">Get access to additional projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/frequently_asked_questions.html">Frequently Asked Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#how-do-i-apply-for-an-account">How do I apply for an account?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#what-is-the-status-of-my-application">What is the status of my application?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#how-should-i-acknowledge-the-olcf-in-my-publications-and-presentations">How should I acknowledge the OLCF in my publications and presentations?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#what-is-a-subproject">What is a subproject?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#i-no-longer-need-my-account-who-should-i-inform-and-what-should-i-do-with-my-olcf-issued-rsa-securid-token">I no longer need my account. Who should I inform and what should I do with my OLCF issued RSA SecurID token?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#my-securid-token-is-broken-expired-what-should-i-do">My SecurID token is broken/expired. What should I do?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#getting-help">Getting Help</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/frequently_asked_questions.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/documents_and_forms.html">Documents and Forms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-for-requesting-a-project-allocation">Forms for Requesting a Project Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-for-requesting-an-account">Forms for Requesting an Account</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#forms-to-request-changes-to-computers-jobs-or-accounts">Forms to Request Changes to Computers, Jobs, or Accounts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#report-templates">Report Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/documents_and_forms.html#miscellaneous-forms">Miscellaneous Forms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/olcf_policy_guide.html">OLCF Policy Guides</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#olcf-acknowledgement">OLCF Acknowledgement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#computing-policy">Computing Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#computer-use">Computer Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-use">Data Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software-use">Software Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#user-accountability">User Accountability</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-management-policy">Data Management Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-retention-purge-quotas">Data Retention, Purge, &amp; Quotas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-prohibitions-safeguards">Data Prohibitions &amp; Safeguards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#software">Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#security-policy">Security Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#scope">Scope</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#personal-use">Personal Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#accessing-olcf-computational-resources">Accessing OLCF Computational Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-management">Data Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#sensitive-data">Sensitive Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#data-transfer">Data Transfer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#incite-allocation-under-utilization-policy">INCITE Allocation Under-utilization Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#project-reporting-policy">Project Reporting Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#non-proprietary-institutional-user-agreement-policy">Non-proprietary Institutional User Agreement Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#access">Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#rules-and-regulations">Rules and Regulations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#safety-and-health">Safety and Health</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#intent-to-publish">Intent to Publish</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#export-control">Export Control</a></li>
<li class="toctree-l4"><a class="reference internal" href="../accounts/olcf_policy_guide.html#intellectual-property">Intellectual Property</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../accounts/olcf_policy_guide.html#special-requests-and-policy-exemptions">Special Requests and Policy Exemptions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/glossary.html">Glossary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../accounts/index.html#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../connecting/index.html">Connecting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#connecting-for-the-first-time">Connecting for the first time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#activating-a-new-securid-fob">Activating a new SecurID fob</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#pins-passcodes-and-tokencodes">PINs, Passcodes, and Tokencodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#x11-forwarding">X11 Forwarding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#id2">Systems Available to All Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#olcf-system-hostnames">OLCF System Hostnames</a></li>
<li class="toctree-l2"><a class="reference internal" href="../connecting/index.html#checking-system-availability">Checking System Availability</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Systems</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Summit User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#summit-documentation-resources">Summit Documentation Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#summit-nodes">Summit Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#node-types">Node Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#system-interconnect">System Interconnect</a></li>
<li class="toctree-l4"><a class="reference internal" href="#file-systems">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operating-system">Operating System</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hardware-threads">Hardware Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#connecting">Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-and-storage">Data and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#software">Software</a></li>
<li class="toctree-l3"><a class="reference internal" href="#shell-programming-environments">Shell &amp; Programming Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#default-shell">Default Shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="#environment-management-with-lmod">Environment Management with Lmod</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#compilers">Compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linking-in-libraries">Linking in Libraries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#login-launch-and-compute-nodes">Login, Launch, and Compute Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batch-scripts">Batch Scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interactive-jobs">Interactive Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-bsub-options">Common bsub Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-states">Job States</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scheduling-policy">Scheduling Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-dependencies">Job Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-launcher-jsrun">Job Launcher (jsrun)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cuda-aware-mpi">CUDA-Aware MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-jobs">Monitoring Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interacting-with-jobs">Interacting With Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other-lsf-commands">Other LSF Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pbs-torque-moab-to-lsf-translation">PBS/Torque/MOAB-to-LSF Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#easy-mode-vs-expert-mode">Easy Mode vs. Expert Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#system-service-core-isolation">System Service Core Isolation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resource-accounting">Resource Accounting</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other-notes">Other Notes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#debugging">Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#arm-ddt">Arm DDT</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gdb">GDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="#valgrind">Valgrind</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#optimizing-and-profiling">Optimizing and Profiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#profiling-gpu-code-with-nvidia-developer-tools">Profiling GPU Code with NVIDIA Developer Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="#score-p">Score-P</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vampir">Vampir</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#nvidia-tesla-v100">NVIDIA V100 GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nvidia-v100-sm">NVIDIA V100 SM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hbm2">HBM2</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nvidia-nvlink">NVIDIA NVLink</a></li>
<li class="toctree-l4"><a class="reference internal" href="#volta-multi-process-service">Volta Multi-Process Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unified-memory">Unified Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#independent-thread-scheduling">Independent Thread Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tensor-cores">Tensor Cores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tesla-v100-specifications">Tesla V100 Specifications</a></li>
<li class="toctree-l4"><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#burst-buffer">Burst Buffer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nvme-xfs">NVMe (XFS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#current-nvme-usage">Current NVMe Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interactive-jobs-using-the-nvme">Interactive Jobs Using the NVMe</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nvme-usage-example">NVMe Usage Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#spectral-library">Spectral Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#known-issues">Known Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#open-issues">Open Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resolved-issues">Resolved Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cuda-10-1-known-issues">CUDA 10.1 Known Issues</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training-system-ascent">Training System (Ascent)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id26">File Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-access-to-ascent">Obtaining Access to Ascent</a></li>
<li class="toctree-l4"><a class="reference internal" href="#logging-in-to-ascent">Logging In to Ascent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-for-frontier">Preparing For Frontier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hip">HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-hip-on-summit">Using HIP on Summit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#learning-to-program-with-hip">Learning to Program with HIP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#previous-frontier-training-events">Previous Frontier Training Events</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rhea_user_guide.html">Rhea User Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="rhea_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#compute-nodes">Compute nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#login-nodes">Login nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#file-systems">File systems</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rhea_user_guide.html#shell-and-programming-environments">Shell and programming environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#default-shell">Default shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#environment-management-with-lmod">Environment management with lmod</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#installed-software">Installed Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rhea_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#available-compilers">Available compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#changing-compilers">Changing compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#compiler-wrappers">Compiler wrappers</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#compiling-threaded-codes">Compiling threaded codes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rhea_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#login-vs-compute-nodes-on-commodity-clusters">Login vs Compute Nodes on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#slurm">Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#interactive-batch-jobs-on-commodity-clusters">Interactive Batch Jobs on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#common-batch-options-to-slurm">Common Batch Options to Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#modifying-batch-jobs">Modifying Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#monitoring-batch-jobs">Monitoring Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#job-execution">Job Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#batch-queues-on-rhea">Batch Queues on Rhea</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#job-accounting-on-rhea">Job Accounting on Rhea</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rhea_user_guide.html#visualization-tools">Visualization tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#paraview">ParaView</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#visit">VisIt</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#remote-visualization-using-vnc-non-gpu">Remote Visualization using VNC (non-GPU)</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#remote-visualization-using-vnc-gpu-nodes">Remote Visualization using VNC (GPU nodes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="rhea_user_guide.html#remote-visualization-using-nice-dcv-gpu-nodes-only">Remote Visualization using Nice DCV (GPU nodes only)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="home_user_guide.html">Home</a><ul>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#system-overview">System Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#access-connecting">Access &amp; Connecting</a></li>
<li class="toctree-l3"><a class="reference internal" href="home_user_guide.html#usage">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="home_user_guide.html#acceptable-tasks">Acceptable Tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="home_user_guide.html#unacceptable-tasks">Unacceptable Tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dtn_user_guide.html">Data Transfer Nodes (DTNs)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#system-overview">System Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="dtn_user_guide.html#access-connecting">Access &amp; Connecting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hpss_user_guide.html">High Performance Storage System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hpss_user_guide.html#system-overview">System Overview</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ascent_user_guide.html">Ascent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ascent_user_guide.html#system-overview">System Overview</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="andes_user_guide.html">Andes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#system-overview">System Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compute-nodes">Compute nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#login-nodes">Login nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#file-systems">File systems</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#shell-and-programming-environments">Shell and programming environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#default-shell">Default shell</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#environment-management-with-lmod">Environment management with lmod</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#installed-software">Installed Software</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#compiling">Compiling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#available-compilers">Available compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#changing-compilers">Changing compilers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compiler-wrappers">Compiler wrappers</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#compiling-threaded-codes">Compiling threaded codes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#running-jobs">Running Jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#login-vs-compute-nodes-on-commodity-clusters">Login vs Compute Nodes on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#slurm">Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#interactive-batch-jobs-on-commodity-clusters">Interactive Batch Jobs on Commodity Clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#common-batch-options-to-slurm">Common Batch Options to Slurm</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#batch-environment-variables">Batch Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#modifying-batch-jobs">Modifying Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#monitoring-batch-jobs">Monitoring Batch Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#job-execution">Job Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#batch-queues-on-andes">Batch Queues on Andes</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#job-accounting-on-andes">Job Accounting on Andes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="andes_user_guide.html#visualization-tools">Visualization tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#paraview">ParaView</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#visit">VisIt</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-vnc-non-gpu">Remote Visualization using VNC (non-GPU)</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-vnc-gpu-nodes">Remote Visualization using VNC (GPU nodes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="andes_user_guide.html#remote-visualization-using-nice-dcv-gpu-nodes-only">Remote Visualization using Nice DCV (GPU nodes only)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../services_and_applications/index.html">Services and Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/slate/index.html">Slate</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-slate">What is Slate?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-kubernetes">What is Kubernetes?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/overview.html#what-is-openshift">What is OpenShift?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/getting_started.html">Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#requesting-a-slate-project-allocation">Requesting A Slate Project Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#logging-in">Logging in</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#slate-namespaces">Slate Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#install-the-oc-tool">Install the OC tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/getting_started.html#test-login-with-oc-tool">Test login with OC Tool</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html">Guided Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html#creating-your-project">Creating your project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial.html#guided-web-gui-tutorial">Guided Web GUI Tutorial</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial_cli.html">Guided Tutorial: CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/guided_tutorial_cli.html#adding-a-pod-to-your-project">Adding a Pod to your Project</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/image_building.html">Image Building</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#build-types">Build Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/image_building.html#logging-into-the-registry-externally">Logging into the registry externally</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/workloads/index.html">Workloads</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workloads/pod.html">Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/workloads/deployment.html">Deployments</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/networking/index.html">Networking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/services.html">Services</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/nodeport.html">NodePorts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/route.html">Routes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/networkpolicy.html">Network Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/networking/port_forwarding.html">Quick Access from Outside Slate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/storage.html">Persistent Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#creating-a-persistent-volume-claim">Creating A Persistent Volume Claim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#adding-pvc-to-pod">Adding PVC To Pod</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/storage.html#backups">Backups</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/use_cases/index.html">Application Deployment Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/simple_website.html">Build and Deploy Simple Website</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/mongodb_service.html">Deploy MongoDB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/nginx_hello_world.html">Deploy NGINX with Hello World</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/helm_example.html">Deploy Packages with Helm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/use_cases/minio.html">MinIO Object Store (On an NCCS Filesystem)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/index.html">Access OLCF Resources From Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/job_submit.html">Batch Job Submission</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/access_olcf_resources/mount_fs.html">Mount OLCF Filesystems</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/other_resources.html">Schedule Other Slate Resources</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/other_resources.html#gpus">GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/olcf_provided_applications/index.html">OLCF-Provided Applications on Slate</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/index.html">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/fix-writable-directories.html">Fix Container Image Permissions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/troubleshooting/debugging.html">Debugging</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/examples.html">YAML Object Quick Reference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#cronjobs">CronJobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#deployments-and-stateful-sets">Deployments and Stateful Sets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#pods">Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#roles-and-rolebindings">Roles and Rolebindings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#routes-services-and-nodeports">Routes, Services and Nodeports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/slate/examples.html#persistent-volume-claims">Persistent Volume Claims</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/slate/glossary.html">Glossary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../services_and_applications/myolcf/index.html">myOLCF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/overview.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#what-is-myolcf">What is myOLCF?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#what-can-it-do">What can it do?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/overview.html#can-i-suggest-a-feature">Can I suggest a feature?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html">Authenticating</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html#olcf-moderate-accounts">OLCF Moderate Accounts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/authenticating.html#olcf-open-accounts">OLCF Open Accounts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html">Project Pages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#project-context">Project Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#switching-project-contexts">Switching Project Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/project_pages/project_pages.html#available-pages">Available Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html">Account Pages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html#account-context">Account Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/account_pages.html#available-pages">Available Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../services_and_applications/myolcf/account_pages/processing_membership_requests.html">Processing Project Membership Requests</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../data/index.html">Data Storage and Transfers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../data/storage_overview.html">Storage Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/storage_overview.html#storage-areas">Storage Areas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/storage_overview.html#alpine-ibm-spectrum-scale-filesystem">Alpine IBM Spectrum Scale Filesystem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/storage_overview.html#performance-under-not-ideal-workload">Performance under not ideal workload</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/storage_overview.html#tips">Tips</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/storage_overview.html#major-difference-between-lustre-and-ibm-spectrum-scale">Major difference between Lustre and IBM Spectrum Scale</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/policies.html">Policy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/policies.html#information">Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/policies.html#purge">Purge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/policies.html#data-retention">Data Retention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/user_centric.html">User-Centric Data Storage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/user_centric.html#user-home-directories-nfs">User Home Directories (NFS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/user_centric.html#user-home-quotas">User Home Quotas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/user_centric.html#user-home-permissions">User Home Permissions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/user_centric.html#user-home-backups">User Home Backups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/user_centric.html#user-website-directory">User Website Directory</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/user_centric.html#user-archive-directories-hpss">User Archive Directories (HPSS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/user_centric.html#user-archive-access">User Archive Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/user_centric.html#user-archive-accounting">User Archive Accounting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/project_centric.html">Project-Centric Data Storage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/project_centric.html#project-home-directories-nfs">Project Home Directories (NFS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#project-home-path">Project Home Path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#project-home-quotas">Project Home Quotas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#project-home-permissions">Project Home Permissions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#project-home-backups">Project Home Backups</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/project_centric.html#project-work-areas">Project Work Areas</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#three-project-work-areas-to-facilitate-collaboration">Three Project Work Areas to Facilitate Collaboration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#backups">Backups</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/project_centric.html#project-archive-directories">Project Archive Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#three-project-archive-areas-facilitae-collaboration-on-archival-data">Three Project Archive Areas Facilitae Collaboration on Archival Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/project_centric.html#project-archive-access">Project Archive Access</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/transferring.html">Transferring Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/transferring.html#using-common-terminal-tools">Using common terminal tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/transferring.html#using-globus-from-your-local-machine">Using Globus from your local machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/archiving.html">HPSS Data Archival System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/archiving.html#using-globus">Using Globus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/archiving.html#using-hsi">Using HSI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/archiving.html#additional-hsi-documentation">Additional HSI Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../data/archiving.html#using-htar">Using HTAR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/archiving.html#htar-limitations">HTAR Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../data/archiving.html#additional-htar-documentation">Additional HTAR Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/index.html#burst-buffer-and-spectral-library">Burst Buffer and Spectral Library</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../software/index.html">Software</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../software/software-news.html">Software News</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/analytics/index.html">ML/DL, Analytics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html">IBM Watson Machine Learning CE -&gt; Open CE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#getting-started">Getting Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#running-distributed-deep-learning-jobs">Running Distributed Deep Learning Jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#setting-up-custom-environments">Setting up Custom Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#best-distributed-deep-learning-performance">Best Distributed Deep Learning Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/analytics/ibm-wml-ce.html#example">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../software/analytics/pbdR.html">Programming with Big Data in R (pbdR)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/python.html">Python on OLCF Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software/profiling/index.html">Profiling Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/profiling/TAU.html">Tuning and Analysis Utilities (TAU)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#run-time-environment-variables">Run-Time Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#compile-time-environment-variables">Compile-Time Environment Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#miniweather-example-application">MiniWeather Example Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#cuda-profiling-tools-interface">CUDA Profiling Tools Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#tracing">Tracing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#selective-instrumentation">Selective Instrumentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#dynamic-phase">Dynamic Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#static-phase">Static Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../software/profiling/TAU.html#openmp-offload">OpenMP Offload</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../software/UMS/index.html">User-Managed Software</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../software/UMS/Flux.html">Flux</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../training/index.html">Training</a><ul>
<li class="toctree-l2"><a class="reference external" href="https://www.olcf.ornl.gov/for-users/training/training-calendar" target="_blank">OLCF Training Calendar</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/olcf-tutorials" target="_blank">OLCF Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/training_archive.html">OLCF Training Archive</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/olcf_gpu_hackathons.html">OLCF GPU Hackathons</a></li>
<li class="toctree-l2"><a class="reference external" href="https://vimeo.com/channels/olcftraining" target="_blank">OLCF Vimeo Channel</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to these docs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#submitting-suggestions">Submitting suggestions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#authoring-content">Authoring content</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#setup-authoring-environment">Setup authoring environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#edit-the-docs">Edit the docs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/index.html#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/index.html#github-guidelines">GitHub Guidelines</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OLCF User Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Systems</a> &raquo;</li>
        
      <li>Summit User Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/olcf/olcf-user-docs/blob/master/systems/summit_user_guide.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="summit-user-guide">
<span id="id1"></span><h1>Summit User Guide<a class="headerlink" href="#summit-user-guide" title="Permalink to this headline">¶</a></h1>
<div class="section" id="summit-documentation-resources">
<span id="id2"></span><h2>Summit Documentation Resources<a class="headerlink" href="#summit-documentation-resources" title="Permalink to this headline">¶</a></h2>
<p>In addition to this Summit User Guide, there are other sources of
documentation, instruction, and tutorials that could be useful for
Summit users.</p>
<p>The <a class="reference internal" href="../training/training_archive.html#training-archive"><span class="std std-ref">OLCF Training Archive</span></a> provides a list of previous training
events, including multi-day Summit Workshops. Some examples of topics addressed during
these workshops include using Summit’s NVME burst buffers, CUDA-aware MPI, advanced
networking and MPI, and multiple ways of programming multiple GPUs per node. You can also
find simple tutorials and code examples for some common programming and running tasks in
our <a class="reference external" href="https://github.com/olcf-tutorials" target="_blank">Github tutorial page</a> .</p>
</div>
<div class="section" id="system-overview">
<span id="id3"></span><h2>System Overview<a class="headerlink" href="#system-overview" title="Permalink to this headline">¶</a></h2>
<p>Summit is an IBM system located at the Oak Ridge Leadership Computing
Facility. With a theoretical peak double-precision performance of
approximately 200 PF, it is one of the most capable systems in the world
for a wide range of traditional computational science applications. It
is also one of the “smartest” computers in the world for deep learning
applications with a mixed-precision capability in excess of 3 EF.</p>
<div class="section" id="summit-nodes">
<span id="id4"></span><h3>Summit Nodes<a class="headerlink" href="#summit-nodes" title="Permalink to this headline">¶</a></h3>
<img alt="Summit node architecture diagram" class="align-center" src="../_images/summit_node_architecture.png" />
<p>The basic building block of Summit is the IBM Power System AC922 node.
Each of the approximately 4,600 compute nodes on Summit contains two IBM
POWER9 processors and six <a class="reference internal" href="#nvidia-tesla-v100">NVIDIA Tesla V100</a> accelerators and provides
a theoretical double-precision capability of
approximately 40 TF. Each POWER9 processor is connected via dual NVLINK
bricks, each capable of a 25GB/s transfer rate in each direction.</p>
<p>Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9
processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators,
and 1.6TB of non-volatile memory that can be used as a burst buffer. A small
number of nodes (54) are configured as “high memory” nodes. These nodes contain 2TB of
DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.</p>
<p>The POWER9 processor is built around IBM’s SIMD
Multi-Core (SMC). The processor provides 22 SMCs with separate 32kB L1
data and instruction caches. Pairs of SMCs share a 512kB L2 cache and a
10MB L3 cache. SMCs support Simultaneous Multi-Threading (SMT) up to a
level of 4, meaning each physical core supports up to 4 <a class="reference internal" href="#hardware-threads"><span class="std std-ref">Hardware Threads</span></a>.</p>
<p>The POWER9 processors and V100
accelerators are cooled with cold plate technology. The remaining
components are cooled through more traditional methods, although exhaust
is passed through a back-of-cabinet heat exchanger prior to being
released back into the room. Both the cold plate and heat exchanger
operate using medium temperature water which is more cost-effective for
the center to maintain than chilled water used by older systems.</p>
</div>
<div class="section" id="node-types">
<h3>Node Types<a class="headerlink" href="#node-types" title="Permalink to this headline">¶</a></h3>
<p>On Summit, there are three major types of nodes you will encounter:
Login, Launch, and Compute. While all of these are similar in terms of
hardware (see: <a class="reference internal" href="#summit-nodes"><span class="std std-ref">Summit Nodes</span></a>), they differ considerably in their intended
use.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Node Type</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Login</td>
<td>When you connect to Summit, you’re placed on a login node. This
is the place to write/edit/compile your code, manage data, submit jobs, etc. You
should never launch parallel jobs from a login node nor should you run threaded
jobs on a login node. Login nodes are shared resources that are in use by many
users simultaneously.</td>
</tr>
<tr class="row-odd"><td>Launch</td>
<td>When your batch script (or interactive batch job) starts
running, it will execute on a Launch Node. (If you were a user of Titan,
these are similar in function to service nodes on that system). All commands
within your job script (or the commands you run in an interactive job) will run
on a launch node. Like login nodes, these are shared resources so you should not
run multiprocessor/threaded programs on Launch nodes. It is appropriate to
launch the jsrun command from launch nodes.</td>
</tr>
<tr class="row-even"><td>Compute</td>
<td>Most of the nodes on Summit are compute nodes. These are where
your parallel job executes. They’re accessed via the jsrun command.</td>
</tr>
</tbody>
</table>
<p>Although the nodes are logically organized into different types, they
all contain similar hardware. As a result of this homogeneous
architecture there is not a need to cross-compile when building on a
login node. Since login nodes have similar hardware resources as compute
nodes, any tests that are run by your build process (especially by
utilities such as <code class="docutils literal notranslate"><span class="pre">autoconf</span></code> and <code class="docutils literal notranslate"><span class="pre">cmake</span></code>) will have access to the
same type of hardware that is on compute nodes and should not require
intervention that might be required on non-homogeneous systems.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs.
Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.</p>
</div>
</div>
<div class="section" id="system-interconnect">
<h3>System Interconnect<a class="headerlink" href="#system-interconnect" title="Permalink to this headline">¶</a></h3>
<p>Summit nodes are connected to a dual-rail EDR InfiniBand network
providing a node injection bandwidth of 23 GB/s. Nodes are
interconnected in a Non-blocking Fat Tree topology. This interconnect is
a three-level tree implemented by a switch to connect nodes within each
cabinet (first level) along with Director switches (second and third
level) that connect cabinets together.</p>
</div>
<div class="section" id="file-systems">
<h3>File Systems<a class="headerlink" href="#file-systems" title="Permalink to this headline">¶</a></h3>
<p>Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB
of storage capacity with a peak write speed of 2.5 TB/s. Summit also has
access to the center-wide NFS-based filesystem (which provides user and
project home areas) and has access to the center’s High Performance
Storage System (HPSS) for user and project archival storage.</p>
</div>
<div class="section" id="operating-system">
<h3>Operating System<a class="headerlink" href="#operating-system" title="Permalink to this headline">¶</a></h3>
<p>Summit is running Red Hat Enterprise Linux (RHEL) version 7.6.</p>
</div>
<div class="section" id="hardware-threads">
<span id="id5"></span><h3>Hardware Threads<a class="headerlink" href="#hardware-threads" title="Permalink to this headline">¶</a></h3>
<p>The IBM POWER9 processor supports Hardware Threads. Each of the POWER9’s
physical cores has 4 “slices”. These slices provide Simultaneous Multi
Threading (SMT) support within the core. Three SMT modes are supported:
SMT4, SMT2, and SMT1. In SMT4 mode, each of the slices operates
independently of the other three. This would permit four separate
streams of execution (i.e. OpenMP threads or MPI tasks) on each physical
core. In SMT2 mode, pairs of slices work together to run tasks. Finally,
in SMT1 mode the four slices work together to execute the task/thread
assigned to the physical core. Regardless of the SMT mode used, the four
slices share the physical core’s L1 instruction &amp; data caches.
<a class="reference external" href="https://vimeo.com/283756938" target="_blank">https://vimeo.com/283756938</a></p>
</div>
<div class="section" id="gpus">
<span id="id6"></span><h3>GPUs<a class="headerlink" href="#gpus" title="Permalink to this headline">¶</a></h3>
<p>Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100
accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and
contributes to a majority of the computational work performed on Summit. Each
V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory
nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to
the SMs. The GigaThread Engine is responsible for distributing work among the
SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on
high-memory nodes) of HBM2 memory. The V100 uses NVIDIA’s NVLink interconnect
to pass data between GPUs as well as from CPU-to-GPU. We provide a more in-depth
look into the <a class="reference internal" href="#nvidia-tesla-v100">NVIDIA Tesla V100</a> later in the Summit Guide.</p>
</div>
</div>
<div class="section" id="connecting">
<span id="id7"></span><h2>Connecting<a class="headerlink" href="#connecting" title="Permalink to this headline">¶</a></h2>
<p>To connect to Summit, ssh to summit.olcf.ornl.gov. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ssh</span> <span class="n">username</span><span class="nd">@summit</span><span class="o">.</span><span class="n">olcf</span><span class="o">.</span><span class="n">ornl</span><span class="o">.</span><span class="n">gov</span>
</pre></div>
</div>
<p>For more information on connecting to OLCF resources, see <a class="reference internal" href="../connecting/index.html#connecting-to-olcf"><span class="std std-ref">Connecting for the first time</span></a>.</p>
</div>
<div class="section" id="data-and-storage">
<h2>Data and Storage<a class="headerlink" href="#data-and-storage" title="Permalink to this headline">¶</a></h2>
<p>For more information about center-wide file systems and data archiving available
on Summit, please refer to the pages on <a class="reference internal" href="../data/index.html#data-storage-and-transfers"><span class="std std-ref">Data Storage and Transfers</span></a>.</p>
<p>Each compute node on Summit has a 1.6TB&nbsp;<strong>N</strong>on-<strong>V</strong>olatile <strong>Me</strong>mory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a “Burst Buffer” with
theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading.
The NVMes could be used to reduce the time that applications wait for
I/O.  More information can be found later in the <a class="reference internal" href="#id22">Burst Buffer</a> section.</p>
</div>
<div class="section" id="software">
<span id="id8"></span><h2>Software<a class="headerlink" href="#software" title="Permalink to this headline">¶</a></h2>
<p>Visualization and analysis tasks should be done on the Rhea cluster. There are a
few tools provided for various visualization tasks, as described in the
<a class="reference internal" href="rhea_user_guide.html#visualization-tools"><span class="std std-ref">Visualization tools</span></a> section of the <a class="reference internal" href="rhea_user_guide.html#rhea-user-guide"><span class="std std-ref">Rhea User Guide</span></a>.</p>
<p>For a full list of software available at the OLCF, please see the
Software section (coming soon).</p>
</div>
<div class="section" id="shell-programming-environments">
<span id="id9"></span><h2>Shell &amp; Programming Environments<a class="headerlink" href="#shell-programming-environments" title="Permalink to this headline">¶</a></h2>
<p>OLCF systems provide many software packages and scientific
libraries pre-installed at the system-level for users to take advantage
of. To facilitate this, environment management tools are employed to
handle necessary changes to the shell. The sections below provide
information about using these management tools on Summit.</p>
<div class="section" id="default-shell">
<h3>Default Shell<a class="headerlink" href="#default-shell" title="Permalink to this headline">¶</a></h3>
<p>A user’s default shell is selected when completing the User Account
Request form. The chosen shell is set across all OLCF resources, and is
the shell interface a user will be presented with upon login to any OLCF
system. Currently, supported shells include:</p>
<ul class="simple">
<li>bash</li>
<li>tcsh</li>
<li>csh</li>
<li>ksh</li>
</ul>
<p>If you would like to have your default shell changed, please contact the
<a class="reference external" href="https://www.olcf.ornl.gov/for-users/user-assistance/" target="_blank">OLCF User Assistance Center</a> at
<a class="reference external" href="mailto:help&#37;&#52;&#48;nccs&#46;gov" target="_blank">help<span>&#64;</span>nccs<span>&#46;</span>gov</a>.</p>
</div>
<div class="section" id="environment-management-with-lmod">
<span id="id10"></span><h3>Environment Management with Lmod<a class="headerlink" href="#environment-management-with-lmod" title="Permalink to this headline">¶</a></h3>
<p>Environment modules are provided through <a class="reference external" href="https://lmod.readthedocs.io/en/latest/" target="_blank">Lmod</a>, a Lua-based module system for
dynamically altering shell environments. By managing changes to the shell’s
environment variables (such as <code class="docutils literal notranslate"><span class="pre">PATH</span></code>, <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>, and
<code class="docutils literal notranslate"><span class="pre">PKG_CONFIG_PATH</span></code>), Lmod allows you to alter the software available in your
shell environment without the risk of creating package and version combinations
that cannot coexist in a single environment.</p>
<p>Lmod is a recursive environment module system, meaning it is aware of module
compatibility and actively alters the environment to protect against conflicts.
Messages to stderr are issued upon Lmod implicitly altering the environment.
Environment modules are structured hierarchically by compiler family such that
packages built with a given compiler will only be accessible if the compiler
family is first present in the environment.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Lmod can interpret both Lua modulefiles and legacy Tcl
modulefiles. However, long and logic-heavy Tcl modulefiles may require
porting to Lua.</p>
</div>
<div class="section" id="general-usage">
<h4>General Usage<a class="headerlink" href="#general-usage" title="Permalink to this headline">¶</a></h4>
<p>Typical use of Lmod is very similar to that of interacting with
modulefiles on other OLCF systems. The interface to Lmod is provided by
the <code class="docutils literal notranslate"><span class="pre">module</span></code> command:</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="32%" />
<col width="68%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>module -t list</td>
<td>Shows a terse list of the currently loaded modules.</td>
</tr>
<tr class="row-odd"><td>module avail</td>
<td>Shows a table of the currently available modules</td>
</tr>
<tr class="row-even"><td>module help &lt;modulename&gt;</td>
<td>Shows help information about &lt;modulename&gt;</td>
</tr>
<tr class="row-odd"><td>module show &lt;modulename&gt;</td>
<td>Shows the environment changes made by the &lt;modulename&gt; modulefile</td>
</tr>
<tr class="row-even"><td>module spider &lt;string&gt;</td>
<td>Searches all possible modules according to &lt;string&gt;</td>
</tr>
<tr class="row-odd"><td>module load &lt;modulename&gt; […]</td>
<td>Loads the given &lt;modulename&gt;(s) into the current environment</td>
</tr>
<tr class="row-even"><td>module use &lt;path&gt;</td>
<td>Adds &lt;path&gt; to the modulefile search cache and <code class="docutils literal notranslate"><span class="pre">MODULESPATH</span></code></td>
</tr>
<tr class="row-odd"><td>module unuse &lt;path&gt;</td>
<td>Removes &lt;path&gt; from the modulefile search cache and <code class="docutils literal notranslate"><span class="pre">MODULESPATH</span></code></td>
</tr>
<tr class="row-even"><td>module purge</td>
<td>Unloads all modules</td>
</tr>
<tr class="row-odd"><td>module reset</td>
<td>Resets loaded modules to system defaults</td>
</tr>
<tr class="row-even"><td>module update</td>
<td>Reloads all currently loaded modules</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Modules are changed recursively. Some commands, such as
<code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">swap</span></code>, are available to maintain compatibility with scripts
using Tcl Environment Modules, but are not necessary since Lmod
recursively processes loaded modules and automatically resolves
conflicts.</p>
</div>
</div>
<div class="section" id="searching-for-modules">
<h4>Searching for modules<a class="headerlink" href="#searching-for-modules" title="Permalink to this headline">¶</a></h4>
<p>Modules with dependencies are only available when the underlying dependencies,
such as compiler families, are loaded. Thus, <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">avail</span></code> will only display
modules that are compatible with the current state of the environment. To search
the entire hierarchy across all possible dependencies, the <code class="docutils literal notranslate"><span class="pre">spider</span></code>
sub-command can be used as summarized in the following table.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="32%" />
<col width="68%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>module spider</td>
<td>Shows the entire possible graph of modules</td>
</tr>
<tr class="row-odd"><td>module spider &lt;modulename&gt;</td>
<td>Searches for modules named &lt;modulename&gt; in the graph of possible modules</td>
</tr>
<tr class="row-even"><td>module spider &lt;modulename&gt;/&lt;version&gt;</td>
<td>Searches for a specific version of &lt;modulename&gt; in the graph of possible modules</td>
</tr>
<tr class="row-odd"><td>module spider &lt;string&gt;</td>
<td>Searches for modulefiles containing &lt;string&gt;</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="defining-custom-module-collections">
<h4>Defining custom module collections<a class="headerlink" href="#defining-custom-module-collections" title="Permalink to this headline">¶</a></h4>
<p>Lmod supports caching commonly used collections of environment modules on a
per-user basis in <code class="docutils literal notranslate"><span class="pre">$HOME/.lmod.d</span></code>. To create a collection called “NAME” from
the currently loaded modules, simply call <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">save</span> <span class="pre">NAME</span></code>. Omitting “NAME”
will set the user’s default collection. Saved collections can be recalled and
examined with the commands summarized in the following table.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="30%" />
<col width="70%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>module restore NAME</td>
<td>Recalls a specific saved user collection titled “NAME”</td>
</tr>
<tr class="row-odd"><td>module restore</td>
<td>Recalls the user-defined defaults</td>
</tr>
<tr class="row-even"><td>module reset</td>
<td>Resets loaded modules to system defaults</td>
</tr>
<tr class="row-odd"><td>module restore system</td>
<td>Recalls the system defaults</td>
</tr>
<tr class="row-even"><td>module savelist</td>
<td>Shows the list user-defined saved collections</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You should use unique names when creating collections to
specify the application (and possibly branch) you are working on. For
example, <code class="docutils literal notranslate"><span class="pre">app1-development</span></code>, <code class="docutils literal notranslate"><span class="pre">app1-production</span></code>, and
<code class="docutils literal notranslate"><span class="pre">app2-production</span></code>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In order to avoid conflicts between user-defined collections
on multiple compute systems that share a home file system (e.g.
<code class="docutils literal notranslate"><span class="pre">/ccs/home/[userid]</span></code>), lmod appends the hostname of each system to the
files saved in in your <code class="docutils literal notranslate"><span class="pre">~/.lmod.d</span></code> directory (using the environment
variable <code class="docutils literal notranslate"><span class="pre">LMOD_SYSTEM_NAME</span></code>). This ensures that only collections
appended with the name of the current system are visible.</p>
</div>
<p>The following screencast shows an example of setting up user-defined
module collections on Summit. <a class="reference external" href="https://vimeo.com/293582400" target="_blank">https://vimeo.com/293582400</a></p>
</div>
</div>
</div>
<div class="section" id="compiling">
<span id="id11"></span><h2>Compiling<a class="headerlink" href="#compiling" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compilers">
<h3>Compilers<a class="headerlink" href="#compilers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="available-compilers">
<h4>Available Compilers<a class="headerlink" href="#available-compilers" title="Permalink to this headline">¶</a></h4>
<p>The following compilers are available on Summit:</p>
<p><strong>XL:</strong> IBM XL Compilers <em>(loaded by default)</em></p>
<p><strong>LLVM:</strong> LLVM compiler infrastructure</p>
<p><strong>PGI:</strong> Portland Group compiler suite</p>
<p><strong>GNU:</strong> GNU Compiler Collection</p>
<p><strong>NVCC</strong>: CUDA C compiler</p>
<p>Upon login, the default versions of the XL compiler suite and Spectrum Message
Passing Interface (MPI) are added to each user’s environment through the modules
system. No changes to the environment are needed to make use of the defaults.</p>
<p>Multiple versions of each compiler family are provided, and can be inspected
using the modules system:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>summit$ module -t avail pgi
/sw/summit/modulefiles/site/linux-rhel7-ppc64le/Core:
pgi/18.7
pgi/18.10
pgi/19.1
pgi/19.4
pgi/19.5
pgi/19.7
</pre></div>
</div>
</div>
<div class="section" id="c-compilation">
<h4>C compilation<a class="headerlink" href="#c-compilation" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">type char is unsigned by default</p>
</div>
<table border="1" class="docutils align-default">
<colgroup>
<col width="11%" />
<col width="14%" />
<col width="12%" />
<col width="14%" />
<col width="14%" />
<col width="21%" />
<col width="15%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Vendor</strong></th>
<th class="head"><strong>Module</strong></th>
<th class="head"><strong>Compiler</strong></th>
<th class="head"><strong>Enable C99</strong></th>
<th class="head"><strong>Enable C11</strong></th>
<th class="head"><strong>Default signed char</strong></th>
<th class="head"><strong>Define macro</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>IBM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">xl</span></code></td>
<td>xlc xlc_r</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu99</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-qchar=signed</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-WF,-D</span></code></td>
</tr>
<tr class="row-odd"><td><strong>GNU</strong></td>
<td>system default</td>
<td>gcc</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu99</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-fsigned-char</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-even"><td><strong>GNU</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">gcc</span></code></td>
<td>gcc</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu99</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-fsigned-char</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-odd"><td><strong>LLVM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">llvm</span></code></td>
<td>clang</td>
<td>default</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-fsigned-char</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-even"><td><strong>PGI</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">pgi</span></code></td>
<td>pgcc</td>
<td><code class="docutils literal notranslate"><span class="pre">-c99</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-c11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-Mschar</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="c-compilations">
<h4>C++ compilations<a class="headerlink" href="#c-compilations" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">type char is unsigned by default</p>
</div>
<table border="1" class="docutils align-default">
<colgroup>
<col width="9%" />
<col width="11%" />
<col width="12%" />
<col width="20%" />
<col width="20%" />
<col width="17%" />
<col width="12%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Vendor</strong></th>
<th class="head"><strong>Module</strong></th>
<th class="head"><strong>Compiler</strong></th>
<th class="head"><strong>Enable C++11</strong></th>
<th class="head"><strong>Enable C++14</strong></th>
<th class="head"><strong>Default signed char</strong></th>
<th class="head"><strong>Define macro</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>IBM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">xl</span></code></td>
<td>xlc++, xlc++_r</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++1y</span></code> (PARTIAL)*</td>
<td><code class="docutils literal notranslate"><span class="pre">-qchar=signed</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-WF,-D</span></code></td>
</tr>
<tr class="row-odd"><td><strong>GNU</strong></td>
<td>system default</td>
<td>g++</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++1y</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-fsigned-char</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-even"><td><strong>GNU</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">gcc</span></code></td>
<td>g++</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++1y</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-fsigned-char</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-odd"><td><strong>LLVM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">llvm</span></code></td>
<td>clang++</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++11</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=gnu++1y</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-fsigned-char</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-even"><td><strong>PGI</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">pgi</span></code></td>
<td>pgc++</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=c++11</span> <span class="pre">-gnu_extensions</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=c++14</span> <span class="pre">-gnu_extensions</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-Mschar</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="fortran-compilation">
<h4>Fortran compilation<a class="headerlink" href="#fortran-compilation" title="Permalink to this headline">¶</a></h4>
<table border="1" class="docutils align-default">
<colgroup>
<col width="8%" />
<col width="11%" />
<col width="21%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="12%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Vendor</strong></th>
<th class="head"><strong>Module</strong></th>
<th class="head"><strong>Compiler</strong></th>
<th class="head"><strong>Enable F90</strong></th>
<th class="head"><strong>Enable F2003</strong></th>
<th class="head"><strong>Enable F2008</strong></th>
<th class="head"><strong>Define macro</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>IBM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">xl</span></code></td>
<td>xlf xlf90 xlf95 xlf2003 xlf2008</td>
<td><code class="docutils literal notranslate"><span class="pre">-qlanglvl=90std</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-qlanglvl=2003std</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-qlanglvl=2008std</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-WF,-D</span></code></td>
</tr>
<tr class="row-odd"><td><strong>GNU</strong></td>
<td>system default</td>
<td>gfortran</td>
<td><code class="docutils literal notranslate"><span class="pre">-std=f90</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=f2003</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-std=f2008</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-even"><td><strong>LLVM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">llvm</span></code></td>
<td>xlflang</td>
<td>n/a</td>
<td>n/a</td>
<td>n/a</td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
<tr class="row-odd"><td><strong>PGI</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">pgi</span></code></td>
<td>pgfortran</td>
<td>use <code class="docutils literal notranslate"><span class="pre">.F90</span></code> source file
suffix</td>
<td>use <code class="docutils literal notranslate"><span class="pre">.F03</span></code> source file
suffix</td>
<td>use <code class="docutils literal notranslate"><span class="pre">.F08</span></code> source file
suffix</td>
<td><code class="docutils literal notranslate"><span class="pre">-D</span></code></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The xlflang module currently conflicts with the clang
module. This restriction is expected to be lifted in future releases.</p>
</div>
</div>
<div class="section" id="mpi">
<h4>MPI<a class="headerlink" href="#mpi" title="Permalink to this headline">¶</a></h4>
<p>MPI on Summit is provided by IBM Spectrum MPI. Spectrum MPI provides compiler
wrappers that automatically choose the proper compiler to build your
application.</p>
<p>The following compiler wrappers are available:</p>
<p><strong>C</strong>: <code class="docutils literal notranslate"><span class="pre">mpicc</span></code></p>
<p><strong>C++</strong>: <code class="docutils literal notranslate"><span class="pre">mpic++</span></code>, <code class="docutils literal notranslate"><span class="pre">mpiCC</span></code></p>
<p><strong>Fortran</strong>: <code class="docutils literal notranslate"><span class="pre">mpifort</span></code>, <code class="docutils literal notranslate"><span class="pre">mpif77</span></code>, <code class="docutils literal notranslate"><span class="pre">mpif90</span></code></p>
<p>While these wrappers conveniently abstract away linking of Spectrum MPI, it’s
sometimes helpful to see exactly what’s happening when invoked. The <code class="docutils literal notranslate"><span class="pre">--showme</span></code>
flag will display the full link lines, without actually compiling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>summit$ mpicc --showme
/sw/summit/xl/16.1.1-beta6/xlC/16.1.1/bin/xlc -I/autofs/nccs-svm1_sw/summit/.swci/1-compute/opt/spack/20171006/linux-rhel7-ppc64le/xl-16.1.1-beta6/spectrum-mpi-10.2.0.7-20180830-eyo7zxm2piusmyffr3iytmgwdacl67ju/include -pthread -L/autofs/nccs-svm1_sw/summit/.swci/1-compute/opt/spack/20171006/linux-rhel7-ppc64le/xl-16.1.1-beta6/spectrum-mpi-10.2.0.7-20180830-eyo7zxm2piusmyffr3iytmgwdacl67ju/lib -lmpiprofilesupport -lmpi_ibm
</pre></div>
</div>
</div>
<div class="section" id="openmp">
<h4>OpenMP<a class="headerlink" href="#openmp" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using OpenMP with IBM XL compilers, the thread-safe
compiler variant is required; These variants have the same name as the
non-thread-safe compilers with an additional <code class="docutils literal notranslate"><span class="pre">_r</span></code> suffix. e.g. to
compile OpenMPI C code one would use <code class="docutils literal notranslate"><span class="pre">xlc_r</span></code></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">OpenMP offloading support is still under active development.
Performance and debugging capabilities in particular are expected to
improve as the implementations mature.</p>
</div>
<table border="1" class="docutils align-default">
<colgroup>
<col width="10%" />
<col width="12%" />
<col width="14%" />
<col width="12%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Vendor</strong></th>
<th class="head"><strong>3.1 Support</strong></th>
<th class="head"><strong>Enable OpenMP</strong></th>
<th class="head"><strong>4.x Support</strong></th>
<th class="head"><strong>Enable OpenMP 4.x Offload</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>IBM</strong></td>
<td>FULL</td>
<td><code class="docutils literal notranslate"><span class="pre">-qsmp=omp</span></code></td>
<td>FULL</td>
<td><code class="docutils literal notranslate"><span class="pre">-qsmp=omp</span> <span class="pre">-qoffload</span></code></td>
</tr>
<tr class="row-odd"><td><strong>GNU</strong></td>
<td>FULL</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></td>
<td>PARTIAL</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></td>
</tr>
<tr class="row-even"><td><strong>clang</strong></td>
<td>FULL</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></td>
<td>PARTIAL</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenmp</span> <span class="pre">-fopenmp-targets=nvptx64-nvidia-cuda</span> <span class="pre">--cuda-path=${OLCF_CUDA_ROOT}</span></code></td>
</tr>
<tr class="row-odd"><td><strong>xlflang</strong></td>
<td>FULL</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenmp</span></code></td>
<td>PARTIAL</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenmp</span> <span class="pre">-fopenmp-targets=nvptx64-nvidia-cuda</span></code></td>
</tr>
<tr class="row-even"><td><strong>PGI</strong></td>
<td>FULL</td>
<td><code class="docutils literal notranslate"><span class="pre">-mp</span></code></td>
<td>NONE</td>
<td>NONE</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="openacc">
<h4>OpenACC<a class="headerlink" href="#openacc" title="Permalink to this headline">¶</a></h4>
<table border="1" class="docutils align-default">
<colgroup>
<col width="17%" />
<col width="24%" />
<col width="27%" />
<col width="32%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Vendor</strong></th>
<th class="head"><strong>Module</strong></th>
<th class="head"><strong>OpenACC Support</strong></th>
<th class="head"><strong>Enable OpenACC</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>IBM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">xl</span></code></td>
<td>NONE</td>
<td>NONE</td>
</tr>
<tr class="row-odd"><td><strong>GNU</strong></td>
<td>system default</td>
<td>NONE</td>
<td>NONE</td>
</tr>
<tr class="row-even"><td><strong>GNU</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">gcc</span></code></td>
<td>2.5</td>
<td><code class="docutils literal notranslate"><span class="pre">-fopenacc</span></code></td>
</tr>
<tr class="row-odd"><td><strong>LLVM</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">clang</span></code> or
<code class="docutils literal notranslate"><span class="pre">xlflang</span></code></td>
<td>NONE</td>
<td>NONE</td>
</tr>
<tr class="row-even"><td><strong>PGI</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">pgi</span></code></td>
<td>2.5</td>
<td><code class="docutils literal notranslate"><span class="pre">-acc,</span> <span class="pre">-ta=nvidia:cc70</span></code></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="cuda-compilation">
<h4>CUDA compilation<a class="headerlink" href="#cuda-compilation" title="Permalink to this headline">¶</a></h4>
<div class="section" id="nvidia">
<h5>NVIDIA<a class="headerlink" href="#nvidia" title="Permalink to this headline">¶</a></h5>
<p>CUDA C/C++ support is provided through the <code class="docutils literal notranslate"><span class="pre">cuda</span></code> module.</p>
<p><code class="docutils literal notranslate"><span class="pre">nvcc</span></code> : Primary CUDA C/C++ compiler</p>
<p><strong>Language support</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">-std=c++11</span></code> : provide C++11 support</p>
<p><code class="docutils literal notranslate"><span class="pre">--expt-extended-lambda</span></code> : provide experimental host/device lambda support</p>
<p><code class="docutils literal notranslate"><span class="pre">--expt-relaxed-constexpr</span></code> : provide experimental host/device constexpr support</p>
<p><strong>Compiler support</strong></p>
<p>NVCC currently supports XL, GCC, and PGI C++ backends.</p>
<p><code class="docutils literal notranslate"><span class="pre">--ccbin</span></code> : set to host compiler location</p>
</div>
</div>
<div class="section" id="cuda-fortran-compilation">
<h4>CUDA Fortran compilation<a class="headerlink" href="#cuda-fortran-compilation" title="Permalink to this headline">¶</a></h4>
<div class="section" id="ibm">
<h5>IBM<a class="headerlink" href="#ibm" title="Permalink to this headline">¶</a></h5>
<p>The IBM compiler suite is made available through the default loaded xl
module, the cuda module is also required.</p>
<p><code class="docutils literal notranslate"><span class="pre">xlcuf</span></code> : primary Cuda fortran compiler, thread safe</p>
<p><strong>Language support flags</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">-qlanglvl=90std</span></code> : provide Fortran90 support</p>
<p><code class="docutils literal notranslate"><span class="pre">-qlanglvl=95std</span></code> : provide Fortran95 support</p>
<p><code class="docutils literal notranslate"><span class="pre">-qlanglvl=2003std</span></code> : provide Fortran2003 support</p>
<p><code class="docutils literal notranslate"><span class="pre">-qlanglvl=2008std</span></code> : provide Fortran2003 support</p>
</div>
<div class="section" id="pgi">
<h5>PGI<a class="headerlink" href="#pgi" title="Permalink to this headline">¶</a></h5>
<p>The PGI compiler suite is available through the <code class="docutils literal notranslate"><span class="pre">pgi</span></code> module.</p>
<p><code class="docutils literal notranslate"><span class="pre">pgfortran</span></code> : Primary fortran compiler with CUDA Fortran support</p>
<p><strong>Language support:</strong></p>
<p>Files with <code class="docutils literal notranslate"><span class="pre">.cuf</span></code> suffix automatically compiled with cuda fortran support</p>
<p>Standard fortran suffixed source files determines the standard involved,
see the man page for full details</p>
<p><code class="docutils literal notranslate"><span class="pre">-Mcuda</span></code> : Enable CUDA Fortran on provided source file</p>
</div>
</div>
</div>
<div class="section" id="linking-in-libraries">
<h3>Linking in Libraries<a class="headerlink" href="#linking-in-libraries" title="Permalink to this headline">¶</a></h3>
<p>OLCF systems provide many software packages and scientific
libraries pre-installed at the system-level for users to take advantage
of. In order to link these libraries into an application, users must
direct the compiler to their location. The <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">show</span></code> command can
be used to determine the location of a particular library. For example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>summit$ module show essl
------------------------------------------------------------------------------------
   /sw/summit/modulefiles/core/essl/6.1.0-1:
------------------------------------------------------------------------------------
whatis(&quot;ESSL 6.1.0-1 &quot;)
prepend_path(&quot;LD_LIBRARY_PATH&quot;,&quot;/sw/summit/essl/6.1.0-1/essl/6.1/lib64&quot;)
append_path(&quot;LD_LIBRARY_PATH&quot;,&quot;/sw/summit/xl/16.1.1-beta4/lib&quot;)
prepend_path(&quot;MANPATH&quot;,&quot;/sw/summit/essl/6.1.0-1/essl/6.1/man&quot;)
setenv(&quot;OLCF_ESSL_ROOT&quot;,&quot;/sw/summit/essl/6.1.0-1/essl/6.1&quot;)
help([[ESSL 6.1.0-1

]])
</pre></div>
</div>
<p>When this module is loaded, the <code class="docutils literal notranslate"><span class="pre">$OLCF_ESSL_ROOT</span></code> environment variable
holds the path to the ESSL installation, which contains the lib64/ and
include/ directories:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>summit$ module load essl
summit$ echo $OLCF_ESSL_ROOT
/sw/summit/essl/6.1.0-1/essl/6.1
summit$ ls $OLCF_ESSL_ROOT
FFTW3  READMES  REDIST.txt  include  iso-swid  ivps  lap  lib64  man  msg
</pre></div>
</div>
<p>The following screencast shows an example of linking two libraries into
a simple program on Summit. <a class="reference external" href="https://vimeo.com/292015868" target="_blank">https://vimeo.com/292015868</a></p>
</div>
</div>
<div class="section" id="running-jobs">
<span id="id12"></span><h2>Running Jobs<a class="headerlink" href="#running-jobs" title="Permalink to this headline">¶</a></h2>
<p>As is the case on other OLCF systems, computational work on Summit is
performed within jobs. A typical job consists of several components:</p>
<ul class="simple">
<li>A submission script</li>
<li>An executable</li>
<li>Input files needed by the executable</li>
<li>Output files created by the executable</li>
</ul>
<p>In general, the process for running a job is to:</p>
<ol class="arabic simple">
<li>Prepare executables and input files</li>
<li>Write the batch script</li>
<li>Submit the batch script</li>
<li>Monitor the job’s progress before and during execution</li>
</ol>
<p>The following sections will provide more information regarding running
jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as
the batch scheduling system.</p>
<div class="section" id="login-launch-and-compute-nodes">
<span id="id13"></span><h3>Login, Launch, and Compute Nodes<a class="headerlink" href="#login-launch-and-compute-nodes" title="Permalink to this headline">¶</a></h3>
<p>Recall from the <a class="reference internal" href="#system-overview"><span class="std std-ref">System Overview</span></a>
section that Summit has three types of nodes: login, launch, and
compute. When you log into the system, you are placed on a login node.
When your <a class="reference internal" href="#batch-scripts"><span class="std std-ref">Batch Scripts</span></a> or <a class="reference internal" href="#interactive-jobs"><span class="std std-ref">Interactive Jobs</span></a> run,
the resulting shell will run on a launch node. Compute nodes are accessed
via the <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> command. The <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> command should only be issued
from within an LSF job (either batch or interactive) on a launch node.
Otherwise, you will not have any compute nodes allocated and your parallel
job will run on the login node. If this happens, your job will interfere with
(and be interfered with by) other users’ login node tasks. <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> is covered
in-depth in the <a class="reference internal" href="#id17">Job Launcher (jsrun)</a> section.</p>
<div class="section" id="per-user-login-node-resource-limits">
<h4>Per-User Login Node Resource Limits<a class="headerlink" href="#per-user-login-node-resource-limits" title="Permalink to this headline">¶</a></h4>
<p>Because the login nodes are resources shared by all Summit users, we utilize
<code class="docutils literal notranslate"><span class="pre">cgroups</span></code> to help better ensure resource availability for all users of the
shared nodes. By default each user is limited to <strong>16 hardware-threads</strong>, <strong>16GB
of memory</strong>, and <strong>1 GPU</strong>.  Please note that limits are set per user and not
individual login sessions. All user processes on a node are contained within a
single cgroup and share the cgroup’s limits.</p>
<p>If a process from any of a user’s login sessions reaches 4 hours of CPU-time,
all login sessions will be limited to <strong>.5 hardware-thread</strong>. After 8 hours of
CPU-time, the process is automatically killed. To reset the cgroup limits on a
node to default once the 4 hour CPU-time reduction has been reached, kill the
offending process and start a new login session to the node.</p>
<blockquote>
<div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Login node limits are set per user and not per individual login
session.  All user processes on a node are contained within a single cgroup
and will share the cgroup’s limits.</p>
</div>
</div></blockquote>
</div>
</div>
<div class="section" id="batch-scripts">
<span id="id14"></span><h3>Batch Scripts<a class="headerlink" href="#batch-scripts" title="Permalink to this headline">¶</a></h3>
<p>The most common way to interact with the batch system is via batch jobs.
A batch job is simply a shell script with added directives to request
various resources from or provide certain information to the batch
scheduling system. Aside from the lines containing LSF options, the
batch script is simply the series commands needed to set up and run your
job.</p>
<p>To submit a batch script, use the bsub command: <code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">myjob.lsf</span></code></p>
<p>If you’ve previously used LSF, you’re probably used to submitting a job
with input redirection (i.e. <code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">&lt;</span> <span class="pre">myjob.lsf</span></code>). This is not needed
(and will not work) on Summit.</p>
<p>As an example, consider the following batch script:</p>
<div class="highlight-bash notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="highlight"><pre><span></span> <span class="c1">#!/bin/bash</span>
 <span class="c1"># Begin LSF Directives</span>
 <span class="c1">#BSUB -P ABC123</span>
 <span class="c1">#BSUB -W 3:00</span>
 <span class="c1">#BSUB -nnodes 2048</span>
 <span class="c1">#BSUB -alloc_flags gpumps</span>
 <span class="c1">#BSUB -J RunSim123</span>
 <span class="c1">#BSUB -o RunSim123.%J</span>
 <span class="c1">#BSUB -e RunSim123.%J</span>

 <span class="nb">cd</span> <span class="nv">$MEMBERWORK</span>/abc123
 cp <span class="nv">$PROJWORK</span>/abc123/RunData/Input.123 ./Input.123
 date
 jsrun -n <span class="m">4092</span> -r <span class="m">2</span> -a <span class="m">12</span> -g <span class="m">3</span> ./a.out
 cp my_output_file /ccs/proj/abc123/Output.123
</pre></div>
</td></tr></table></div>
<table border="1" class="docutils align-default">
<colgroup>
<col width="9%" />
<col width="11%" />
<col width="81%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Line #</th>
<th class="head">Option</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>&#160;</td>
<td>Shell specification. This script will run under with bash as the shell</td>
</tr>
<tr class="row-odd"><td>2</td>
<td>&#160;</td>
<td>Comment line</td>
</tr>
<tr class="row-even"><td>3</td>
<td>Required</td>
<td>This job will charge to the ABC123 project</td>
</tr>
<tr class="row-odd"><td>4</td>
<td>Required</td>
<td>Maximum walltime for the job is 3 hours</td>
</tr>
<tr class="row-even"><td>5</td>
<td>Required</td>
<td>The job will use 2,048 compute nodes</td>
</tr>
<tr class="row-odd"><td>6</td>
<td>Optional</td>
<td>Enable GPU Multi-Process Service</td>
</tr>
<tr class="row-even"><td>7</td>
<td>Optional</td>
<td>The name of the job is RunSim123</td>
</tr>
<tr class="row-odd"><td>8</td>
<td>Optional</td>
<td>Write standard output to a file named RunSim123.#, where # is the job ID assigned by LSF</td>
</tr>
<tr class="row-even"><td>9</td>
<td>Optional</td>
<td>Write standard error to a file named RunSim123.#, where # is the job ID assigned by LSF</td>
</tr>
<tr class="row-odd"><td>10</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td>Blank line</td>
</tr>
<tr class="row-even"><td>11</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td>Change into one of the scratch filesystems</td>
</tr>
<tr class="row-odd"><td>12</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td>Copy input files into place</td>
</tr>
<tr class="row-even"><td>13</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td>Run the <code class="docutils literal notranslate"><span class="pre">date</span></code> command to write a timestamp to the standard output file</td>
</tr>
<tr class="row-odd"><td>14</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td>Run the executable on the allocated compute nodes</td>
</tr>
<tr class="row-even"><td>15</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td>Copy output files from the scratch area into a more permanent location</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="interactive-jobs">
<span id="id15"></span><h3>Interactive Jobs<a class="headerlink" href="#interactive-jobs" title="Permalink to this headline">¶</a></h3>
<p>Most users will find batch jobs to be the easiest way to interact with
the system, since they permit you to hand off a job to the scheduler and
then work on other tasks; however, it is sometimes preferable to run
interactively on the system. This is especially true when developing,
modifying, or debugging a code.</p>
<p>Since all compute resources are managed/scheduled by LSF, it is not possible
to simply log into the system and begin running a parallel code interactively.
You must request the appropriate resources from the system and, if necessary,
wait until they are available. This is done with an “interactive batch” job.
Interactive batch jobs are submitted via the command line, which
supports the same options that are passed via <code class="docutils literal notranslate"><span class="pre">#BSUB</span></code> parameters in a
batch script. The final options on the command line are what makes the
job “interactive batch”: <code class="docutils literal notranslate"><span class="pre">-Is</span></code> followed by a shell name. For example,
to request an interactive batch job (with bash as the shell) equivalent
to the sample batch script above, you would use the command:
<code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-W</span> <span class="pre">3:00</span> <span class="pre">-nnodes</span> <span class="pre">2048</span> <span class="pre">-P</span> <span class="pre">ABC123</span> <span class="pre">-Is</span> <span class="pre">/bin/bash</span></code></p>
<p>As pointed out in <a class="reference internal" href="#login-launch-and-compute-nodes"><span class="std std-ref">Login, Launch, and Compute Nodes</span></a>, you will be placed on
a launch (a.k.a. “batch”) node upon launching an interactive job and as usual
need to use <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> to access the compute node(s):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ bsub -Is -W 0:10 -nnodes 1 -P STF007 $SHELL
Job &lt;779469&gt; is submitted to default queue &lt;batch&gt;.
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on batch2&gt;&gt;

$ hostname
batch2

$ jsrun -n1 hostname
a35n03
</pre></div>
</div>
</div>
<div class="section" id="common-bsub-options">
<h3>Common bsub Options<a class="headerlink" href="#common-bsub-options" title="Permalink to this headline">¶</a></h3>
<p>The table below summarizes options for submitted jobs. Unless otherwise
noted, these can be used from batch scripts or interactive jobs. For
interactive jobs, the options are simply added to the <code class="docutils literal notranslate"><span class="pre">bsub</span></code> command
line. For batch scripts, they can either be added on the <code class="docutils literal notranslate"><span class="pre">bsub</span></code>
command line or they can appear as a <code class="docutils literal notranslate"><span class="pre">#BSUB</span></code> directive in the batch
script. If conflicting options are specified (i.e. different walltime
specified on the command line versus in the script), the option on the
command line takes precedence. Note that LSF has numerous options; only
the most common ones are described here. For more in-depth information
about other LSF options, see the <code class="docutils literal notranslate"><span class="pre">bsub</span></code> man page.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="14%" />
<col width="28%" />
<col width="58%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Option</th>
<th class="head">Example Usage</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">-W</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">50</span></code></td>
<td>Requested
maximum walltime. NOTE: The format is [hours:]minutes, not
[[hours:]minutes:]seconds like PBS/Torque/Moab</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">-nnodes</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1024</span></code></td>
<td>Number of nodes
NOTE: There is specified with only one hyphen (i.e. -nnodes, not –nnodes)</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">-P</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">ABC123</span></code></td>
<td>Specifies the
project to which the job should be charged</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">-o</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-o</span> <span class="pre">jobout.%J</span></code></td>
<td>File into which
job STDOUT should be directed (%J will be replaced with the job ID number) If
you do not also specify a STDERR file with <code class="docutils literal notranslate"><span class="pre">-e</span></code> or <code class="docutils literal notranslate"><span class="pre">-eo</span></code>, STDERR will also
be written to this file.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">-e</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-e</span> <span class="pre">jobout.%J</span></code></td>
<td>File into which
job STDERR should be directed (%J will be replaced with the job ID number)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">-J</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-J</span> <span class="pre">MyRun123</span></code></td>
<td>Specifies the
name of the job (if not present, LSF will use the name of the job script as the
job’s name)</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">-w</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">ended()</span></code></td>
<td>Place a dependency on the job</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">-N</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-N</span></code></td>
<td>Send a job report via email when the job completes</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">-XF</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-XF</span></code></td>
<td>Use X11 forwarding</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">-alloc_flags</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-alloc_flags</span> <span class="pre">&quot;gpumps</span> <span class="pre">smt1&quot;</span></code></td>
<td>Used to request
GPU Multi-Process Service (MPS) and to set SMT (Simultaneous Multithreading)
levels. Only one “#BSUB alloc_flags” command is recognized so multiple
alloc_flags options need to be enclosed in quotes and space-separated. Setting
gpumps enables NVIDIA’s Multi-Process Service, which allows multiple MPI ranks
to simultaneously access a GPU. Setting smt<em>n</em> (where <em>n</em> is 1, 2, or 4) sets
different SMT levels. To run with 2 hardware threads per physical core, you’d
use smt2. The default level is smt4.</td>
</tr>
</tbody>
</table>
<div class="section" id="allocation-wide-options">
<h4>Allocation-wide Options<a class="headerlink" href="#allocation-wide-options" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span></code> option to <code class="docutils literal notranslate"><span class="pre">bsub</span></code> is used to set allocation-wide options.
These settings are applied to every compute node in a job. Only one instance of
the flag is accepted, and multiple <code class="docutils literal notranslate"><span class="pre">alloc_flags</span></code> values should be enclosed in
quotes and space-separated. For example, <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">&quot;gpumps</span> <span class="pre">smt1</span></code>.</p>
<p>The most common values (<code class="docutils literal notranslate"><span class="pre">smt{1,2,4}</span></code>, <code class="docutils literal notranslate"><span class="pre">gpumps</span></code>, <code class="docutils literal notranslate"><span class="pre">gpudefault</span></code>) are detailed in
the following sections.</p>
<p>This option can also be used to provide additional resources to GPFS service
processes, described in the <a class="reference external" href="#gpfs-system-service-isolation" target="_blank">GPFS System Service Isolation</a> section.</p>
<div class="section" id="id16">
<h5>Hardware Threads<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h5>
<p>Hardware threads are a feature of the POWER9 processor through which
individual physical cores can support multiple execution streams,
essentially looking like one or more virtual cores (similar to
hyperthreading on some Intel<sup>®</sup> microprocessors). This feature is often
called Simultaneous Multithreading or SMT. The POWER9 processor on
Summit supports SMT levels of 1, 2, or 4, meaning (respectively) each
physical core looks like 1, 2, or 4 virtual cores. The SMT level is
controlled by the <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span></code> option to <code class="docutils literal notranslate"><span class="pre">bsub</span></code>. For example, to
set the SMT level to 2, add the line <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">–alloc_flags</span> <span class="pre">smt2</span></code> to your
batch script or add the option <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">smt2</span></code> to you <code class="docutils literal notranslate"><span class="pre">bsub</span></code>
command line.</p>
<p>The default SMT level is 4.</p>
</div>
<div class="section" id="mps">
<h5>MPS<a class="headerlink" href="#mps" title="Permalink to this headline">¶</a></h5>
<p>The Multi-Process Service (MPS) enables multiple processes (e.g. MPI
ranks) to concurrently share the resources on a single GPU. This is
accomplished by starting an MPS server process, which funnels the work
from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single
CUDA context. In some cases, this can increase performance due to better
utilization of the resources. As mentioned in the <a class="reference external" href="#common-bsub-options" target="_blank">Common bsub Options</a>
section above, MPS can be enabled with the <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">&quot;gpumps&quot;</span></code> option to
<code class="docutils literal notranslate"><span class="pre">bsub</span></code>. The following screencast shows an example of how to start an MPS
server process for a job: <a class="reference external" href="https://vimeo.com/292016149" target="_blank">https://vimeo.com/292016149</a></p>
</div>
<div class="section" id="gpu-compute-modes">
<h5>GPU Compute Modes<a class="headerlink" href="#gpu-compute-modes" title="Permalink to this headline">¶</a></h5>
<p>Summit’s V100 GPUs are configured to have a default compute mode of
<code class="docutils literal notranslate"><span class="pre">EXCLUSIVE_PROCESS</span></code>. In this mode, the GPU is assigned to only a single
process at a time, and can accept work from multiple process threads
concurrently.</p>
<p>It may be desirable to change the GPU’s compute mode to <code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code>, which
enables multiple processes and their threads to share and submit work to it
simultaneously. To change the compute mode to <code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code>, use the
<code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">gpudefault</span></code> option.</p>
<p>NVIDIA recommends using the <code class="docutils literal notranslate"><span class="pre">EXCLUSIVE_PROCESS</span></code> compute mode (the default on
Summit) when using the Multi-Process Service, but both MPS and the compute mode
can be changed by providing both values: <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">&quot;gpumps</span> <span class="pre">gpudefault&quot;</span></code>.</p>
</div>
</div>
</div>
<div class="section" id="batch-environment-variables">
<h3>Batch Environment Variables<a class="headerlink" href="#batch-environment-variables" title="Permalink to this headline">¶</a></h3>
<p>LSF provides a number of environment variables in your job’s shell
environment. Many job parameters are stored in environment variables and
can be queried within the batch job. Several of these variables are
summarized in the table below. This is not an all-inclusive list of
variables available to your batch job; in particular only LSF variables
are discussed, not the many “standard” environment variables that will
be available (such as <code class="docutils literal notranslate"><span class="pre">$PATH</span></code>).</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="30%" />
<col width="70%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Variable</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">LSB_JOBID</span></code></td>
<td>The ID assigned to the job by LSF</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">LS_JOBPID</span></code></td>
<td>The job’s process ID</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">LSB_JOBINDEX</span></code></td>
<td>The job’s index (if it belongs to a job array)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">LSB_HOSTS</span></code></td>
<td>The hosts assigned to run the job</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">LSB_QUEUE</span></code></td>
<td>The queue from which the job was dispatched</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">LSB_INTERACTIVE</span></code></td>
<td>Set to “Y” for an interactive job; otherwise unset</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">LS_SUBCWD</span></code></td>
<td>The directory from which the job was submitted</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="job-states">
<h3>Job States<a class="headerlink" href="#job-states" title="Permalink to this headline">¶</a></h3>
<p>A job will progress through a number of states through its lifetime. The
states you’re most likely to see are:</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">State</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>PEND</td>
<td>Job is pending</td>
</tr>
<tr class="row-odd"><td>RUN</td>
<td>Job is running</td>
</tr>
<tr class="row-even"><td>DONE</td>
<td>Job completed normally (with an exit code of 0)</td>
</tr>
<tr class="row-odd"><td>EXIT</td>
<td>Job completed abnormally</td>
</tr>
<tr class="row-even"><td>PSUSP</td>
<td>Job was suspended (either by the user or an administrator) while pending</td>
</tr>
<tr class="row-odd"><td>USUSP</td>
<td>Job was suspended (either by the user or an administrator) after starting</td>
</tr>
<tr class="row-even"><td>SSUSP</td>
<td>Job was suspended by the system after starting</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="scheduling-policy">
<h3>Scheduling Policy<a class="headerlink" href="#scheduling-policy" title="Permalink to this headline">¶</a></h3>
<p>In a simple batch queue system, jobs run in a first-in, first-out (FIFO)
order. This often does not make effective use of the system. A large job
may be next in line to run. If the system is using a strict FIFO queue,
many processors sit idle while the large job waits to run. <em>Backfilling</em>
would allow smaller, shorter jobs to use those otherwise idle resources,
and with the proper algorithm, the start time of the large job would not
be delayed. While this does make more effective use of the system, it
indirectly encourages the submission of smaller jobs.</p>
<div class="section" id="the-doe-leadership-class-job-mandate">
<h4>The DOE Leadership-Class Job Mandate<a class="headerlink" href="#the-doe-leadership-class-job-mandate" title="Permalink to this headline">¶</a></h4>
<p>As a DOE Leadership Computing Facility, the OLCF has a mandate that a
large portion of Summit’s usage come from large, <em>leadership-class</em> (aka
<em>capability</em>) jobs. To ensure the OLCF complies with DOE directives, we
strongly encourage users to run jobs on Summit that are as large as
their code will warrant. To that end, the OLCF implements queue policies
that enable large jobs to run in a timely fashion.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The OLCF implements queue policies that encourage the
submission and timely execution of large, leadership-class jobs on
Summit.</p>
</div>
<p>The basic priority-setting mechanism for jobs waiting in the queue is
the time a job has been waiting relative to other jobs in the queue.</p>
<p>If your jobs require resources outside these queue policies, please
complete the relevant request form on the <a class="reference external" href="https://www.olcf.ornl.gov/for-users/documents-forms/special-request-form/" target="_blank">Special
Requests</a> page. If
you have any questions or comments on the queue policies below, please
direct them to the User Assistance Center.</p>
</div>
<div class="section" id="job-priority-by-processor-count">
<h4>Job Priority by Processor Count<a class="headerlink" href="#job-priority-by-processor-count" title="Permalink to this headline">¶</a></h4>
<p>Jobs are <em>aged</em> according to the job’s requested processor count (older
age equals higher queue priority). Each job’s requested processor count
places it into a specific <em>bin</em>. Each bin has a different aging
parameter, which all jobs in the bin receive.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="9%" />
<col width="16%" />
<col width="16%" />
<col width="30%" />
<col width="28%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Bin</th>
<th class="head">Min Nodes</th>
<th class="head">Max Nodes</th>
<th class="head">Max Walltime (Hours)</th>
<th class="head">Aging Boost (Days)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>2,765</td>
<td>4,608</td>
<td>24.0</td>
<td>15</td>
</tr>
<tr class="row-odd"><td>2</td>
<td>922</td>
<td>2,764</td>
<td>24.0</td>
<td>10</td>
</tr>
<tr class="row-even"><td>3</td>
<td>92</td>
<td>921</td>
<td>12.0</td>
<td>0</td>
</tr>
<tr class="row-odd"><td>4</td>
<td>46</td>
<td>91</td>
<td>6.0</td>
<td>0</td>
</tr>
<tr class="row-even"><td>5</td>
<td>1</td>
<td>45</td>
<td>2.0</td>
<td>0</td>
</tr>
</tbody>
</table>
<div class="section" id="batch-queue-policy">
<h5><code class="docutils literal notranslate"><span class="pre">batch</span></code> Queue Policy<a class="headerlink" href="#batch-queue-policy" title="Permalink to this headline">¶</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">batch</span></code> queue is the default queue for production work on Summit.
Most work on Summit is handled through this queue. It enforces the
following policies:</p>
<ul class="simple">
<li>Limit of (4) <em>eligible-to-run</em> jobs per user.</li>
<li>Jobs in excess of the per user limit above will be placed into a
<em>held</em> state, but will change to eligible-to-run at the appropriate
time.</li>
<li>Users may have only (100) jobs queued at any state at any time.
Additional jobs will be rejected at submit time.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <em>eligible-to-run</em> state is not the <em>running</em> state.
Eligible-to-run jobs have not started and are waiting for resources.
Running jobs are actually executing.</p>
</div>
</div>
<div class="section" id="batch-hm-queue-policy">
<h5><code class="docutils literal notranslate"><span class="pre">batch-hm</span></code> Queue Policy<a class="headerlink" href="#batch-hm-queue-policy" title="Permalink to this headline">¶</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">batch-hm</span></code> queue is used to access Summit’s high-memory nodes.
Jobs may use all 54 nodes. It enforces the following policies:</p>
<ul class="simple">
<li>Limit of (4) <em>eligible-to-run</em> jobs per user.</li>
<li>Jobs in excess of the per user limit above will be placed into a
<em>held</em> state, but will change to eligible-to-run at the appropriate
time.</li>
<li>Users may have only (100) jobs queued at any state at any time.
Additional jobs will be rejected at submit time.</li>
</ul>
<p><strong>batch-hm job limits:</strong></p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="26%" />
<col width="26%" />
<col width="48%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Min Nodes</th>
<th class="head">Max Nodes</th>
<th class="head">Max Walltime (Hours)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>54</td>
<td>24.0</td>
</tr>
</tbody>
</table>
<p>To submit a job to the <code class="docutils literal notranslate"><span class="pre">batch-hm</span></code> queue, add the <code class="docutils literal notranslate"><span class="pre">-q</span> <span class="pre">batch-hm</span></code> option to your
<code class="docutils literal notranslate"><span class="pre">bsub</span></code> command or <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-q</span> <span class="pre">batch-hm</span></code> to your job script.</p>
</div>
<div class="section" id="killable-queue-policy">
<h5><code class="docutils literal notranslate"><span class="pre">killable</span></code> Queue Policy<a class="headerlink" href="#killable-queue-policy" title="Permalink to this headline">¶</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">killable</span></code> queue is a preemptable queue that allows jobs in bins 4 and 5
to request walltimes up to 24 hours. Jobs submitted to the killable queue will
be preemptable once the job reaches the guaranteed runtime limit as shown in the
table below. For example, a job in bin 5 submitted to the killable queue can
request a walltime of 24 hours. The job will be preemptable after two hours of
run time. Similarly, a job in bin 4 will be preemptable after six hours of run
time. Once a job is preempted, the job will be resubmitted by default with the
original limits as requested in the job script and will have the same <code class="docutils literal notranslate"><span class="pre">JOBID</span></code>.</p>
<p><strong>Preemptable job limits:</strong></p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="9%" />
<col width="16%" />
<col width="16%" />
<col width="30%" />
<col width="28%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Bin</th>
<th class="head">Min Nodes</th>
<th class="head">Max Nodes</th>
<th class="head">Max Walltime (Hours)</th>
<th class="head">Guaranteed Walltime</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>4</td>
<td>46</td>
<td>91</td>
<td>24.0</td>
<td>6.0 (hours)</td>
</tr>
<tr class="row-odd"><td>5</td>
<td>1</td>
<td>45</td>
<td>24.0</td>
<td>2.0 (hours)</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If a job in the <code class="docutils literal notranslate"><span class="pre">killable</span></code> queue does not reach its requested
walltime, it will continue to use allocation time with each automatic
resubmission until it either reaches the requested walltime during a single
continuous run, or is manually killed by the user. Allocations are always
charged based on actual compute time used by all jobs.</p>
</div>
<p>To submit a job to the <code class="docutils literal notranslate"><span class="pre">killable</span></code> queue, add the <code class="docutils literal notranslate"><span class="pre">-q</span> <span class="pre">killable</span></code> option to your
<code class="docutils literal notranslate"><span class="pre">bsub</span></code> command or <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-q</span> <span class="pre">killable</span></code> to your job script.</p>
<p>To prevent a preempted job from being automatically requeued, the <code class="docutils literal notranslate"><span class="pre">BSUB</span> <span class="pre">-rn</span></code>
flag can be used at submit time.</p>
</div>
<div class="section" id="debug-queue-policy">
<h5><code class="docutils literal notranslate"><span class="pre">debug</span></code> Queue Policy<a class="headerlink" href="#debug-queue-policy" title="Permalink to this headline">¶</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">debug</span></code> queue can be used to access Summit’s compute resources for short
non-production debug tasks.  The queue provides a higher priority compared
to jobs of the same job size bin in production queues.  Production work and
job chaining in the debug queue is prohibited.  Each user is limited to one
job in any state in the debug queue at any one point. Attempts to submit multiple
jobs to the debug queue will be rejected upon job submission.</p>
<p><strong>debug job limits:</strong></p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="13%" />
<col width="13%" />
<col width="23%" />
<col width="32%" />
<col width="19%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Min Nodes</th>
<th class="head">Max Nodes</th>
<th class="head">Max Walltime (Hours)</th>
<th class="head">Max queued any state (per user)</th>
<th class="head">Aging Boost (Days)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>unlimited</td>
<td>2.0</td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>To submit a job to the <code class="docutils literal notranslate"><span class="pre">debug</span></code> queue, add the <code class="docutils literal notranslate"><span class="pre">-q</span> <span class="pre">debug</span></code> option to your
<code class="docutils literal notranslate"><span class="pre">bsub</span></code> command or <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-q</span> <span class="pre">debug</span></code> to your job script.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Production work and job chaining in the <code class="docutils literal notranslate"><span class="pre">debug</span></code> queue is prohibited.</p>
</div>
</div>
</div>
<div class="section" id="allocation-overuse-policy">
<h4>Allocation Overuse Policy<a class="headerlink" href="#allocation-overuse-policy" title="Permalink to this headline">¶</a></h4>
<p>Projects that overrun their allocation are still allowed to run on OLCF
systems, although at a reduced priority. Like the adjustment for the
number of processors requested above, this is an adjustment to the
apparent submit time of the job. However, this adjustment has the effect
of making jobs appear much younger than jobs submitted under projects
that have not exceeded their allocation. In addition to the priority
change, these jobs are also limited in the amount of wall time that can
be used. For example, consider that <code class="docutils literal notranslate"><span class="pre">job1</span></code> is submitted at the same
time as <code class="docutils literal notranslate"><span class="pre">job2</span></code>. The project associated with <code class="docutils literal notranslate"><span class="pre">job1</span></code> is over its
allocation, while the project for <code class="docutils literal notranslate"><span class="pre">job2</span></code> is not. The batch system will
consider <code class="docutils literal notranslate"><span class="pre">job2</span></code> to have been waiting for a longer time than <code class="docutils literal notranslate"><span class="pre">job1</span></code>.
Additionally, projects that are at 125% of their allocated time will be
limited to only 3 running jobs at a time. The adjustment to the
apparent submit time depends upon the percentage that the project is
over its allocation, as shown in the table below:</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="52%" />
<col width="48%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">% Of Allocation Used</th>
<th class="head">Priority Reduction</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>&lt; 100%</td>
<td>0 days</td>
</tr>
<tr class="row-odd"><td>100% to 125%</td>
<td>30 days</td>
</tr>
<tr class="row-even"><td>&gt; 125%</td>
<td>365 days</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="system-reservation-policy">
<h4>System Reservation Policy<a class="headerlink" href="#system-reservation-policy" title="Permalink to this headline">¶</a></h4>
<p>Projects may request to reserve a set of processors for a period of time
through the reservation request form, which can be found on the <a class="reference external" href="http://www.olcf.ornl.gov/support/getting-started/special-request-form/" target="_blank">Special
Requests</a>
page. If the reservation is granted, the reserved processors will be
blocked from general use for a given period of time. Only users that
have been authorized to use the reservation can utilize those resources.
Since no other users can access the reserved resources, it is crucial
that groups given reservations take care to ensure the utilization on
those resources remains high. To prevent reserved resources from
remaining idle for an extended period of time, reservations are
monitored for inactivity. If activity falls below 50% of the reserved
resources for more than (30) minutes, the reservation will be canceled
and the system will be returned to normal scheduling. A new reservation
must be requested if this occurs.</p>
<p>The requesting project’s allocation is charged according to the time window
granted, regardless of actual utilization. For example, an 8-hour, 2,000
node reservation on Summit would be equivalent to using 16,000 Summit
node-hours of a project’s allocation.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="job-dependencies">
<h3>Job Dependencies<a class="headerlink" href="#job-dependencies" title="Permalink to this headline">¶</a></h3>
<p>As is the case with many other queuing systems, it is possible to place
dependencies on jobs to prevent them from running until other jobs have
started/completed/etc. Several possible dependency settings are
described in the table below:</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="37%" />
<col width="63%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Expression</th>
<th class="head">Meaning</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">started(12345)</span></code></td>
<td>The job will not start until
job 12345 starts. Job 12345 is considered to have started if is in any of the
following states: USUSP, SSUSP, DONE, EXIT or RUN (with any pre-execution
command specified by <code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-E</span></code> completed)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">done(12345)</span></code> <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">12345</span></code></td>
<td>The job will not start until
job 12345 has a state of DONE (i.e. completed normally). If a job ID is given
with no condition, <code class="docutils literal notranslate"><span class="pre">done()</span></code> is assumed.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">exit(12345)</span></code></td>
<td>The job will not start until
job 12345 has a state of EXIT (i.e. completed abnormally)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">ended(12345)</span></code></td>
<td>The job will not start until
job 12345 has a state of EXIT or DONE</td>
</tr>
</tbody>
</table>
<p>Dependency expressions can be combined with logical operators. For
example, if you want a job held until job 12345 is DONE and job 12346
has started, you can use <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-w</span> <span class="pre">&quot;done(12345)</span> <span class="pre">&amp;&amp;</span> <span class="pre">started(12346)&quot;</span></code></p>
</div>
<div class="section" id="job-launcher-jsrun">
<span id="id17"></span><h3>Job Launcher (jsrun)<a class="headerlink" href="#job-launcher-jsrun" title="Permalink to this headline">¶</a></h3>
<p>The default job launcher for Summit is <code class="docutils literal notranslate"><span class="pre">jsrun</span></code>. jsrun was developed by
IBM for the Oak Ridge and Livermore Power systems. The tool will execute
a given program on resources allocated through the LSF batch scheduler;
similar to <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> and <code class="docutils literal notranslate"><span class="pre">aprun</span></code> functionality.</p>
<div class="section" id="compute-node-description">
<h4>Compute Node Description<a class="headerlink" href="#compute-node-description" title="Permalink to this headline">¶</a></h4>
<p>The following compute node image will be used to discuss jsrun resource
sets and layout.</p>
<a class="reference internal image-reference" href="../_images/summit-node-description-1.png"><img alt="../_images/summit-node-description-1.png" class="align-center" src="../_images/summit-node-description-1.png" style="width: 85%;" /></a>
<ul class="simple">
<li>1 node</li>
<li>2 sockets (grey)</li>
<li>42 physical cores* (dark blue)</li>
<li>168 hardware cores (light blue)</li>
<li>6 GPUs (orange)</li>
<li>2 Memory blocks (yellow)</li>
</ul>
<p><strong>*Core Isolation:</strong> 1 core on each socket has been set aside for
overhead and is not available for allocation through jsrun. The core has
been omitted and is not shown in the above image.</p>
</div>
<div class="section" id="resource-sets">
<h4>Resource Sets<a class="headerlink" href="#resource-sets" title="Permalink to this headline">¶</a></h4>
<p>While jsrun performs similar job launching functions as aprun and
mpirun, its syntax is very different. A large reason for syntax
differences is the introduction of the <code class="docutils literal notranslate"><span class="pre">resource</span> <span class="pre">set</span></code> concept. Through
resource sets, jsrun can control how a node appears to each job. Users
can, through jsrun command line flags, control which resources on a node
are visible to a job. Resource sets also allow the ability to run
multiple jsruns simultaneously within a node. Under the covers, a
resource set is a cgroup.</p>
<p>At a high level, a resource set allows users to configure what a node
look like to their job.</p>
<p>jsrun will create one or more resource sets within a node. Each resource
set will contain 1 or more cores and 0 or more GPUs. A resource set can
span sockets, but it may not span a node. While a resource set can span
sockets within a node, consideration should be given to the cost of
cross-socket communication. By creating resource sets only within
sockets, costly communication between sockets can be prevented.</p>
<div class="section" id="subdividing-a-node-with-resource-sets">
<h5>Subdividing a Node with Resource Sets<a class="headerlink" href="#subdividing-a-node-with-resource-sets" title="Permalink to this headline">¶</a></h5>
<p>Resource sets provides the ability to subdivide node’s resources into
smaller groups. The following examples show how a node can be subdivided
and how many resource set could fit on a node.</p>
<img alt="../_images/summit-resource-set-subdivide.png" class="align-center" src="../_images/summit-resource-set-subdivide.png" />
</div>
<div class="section" id="multiple-methods-to-creating-resource-sets">
<h5>Multiple Methods to Creating Resource Sets<a class="headerlink" href="#multiple-methods-to-creating-resource-sets" title="Permalink to this headline">¶</a></h5>
<p>Resource sets should be created to fit code requirements. The following
examples show multiple ways to create resource sets that allow two MPI
tasks access to a single GPU.</p>
<ol class="arabic">
<li><p class="first">6 resource sets per node: 1 GPU, 2 cores per (Titan)</p>
<img alt="https://www.olcf.ornl.gov/wp-content/uploads/2018/03/RS-summit-example-1GPU-2Cores.png" class="align-center" src="https://www.olcf.ornl.gov/wp-content/uploads/2018/03/RS-summit-example-1GPU-2Cores.png" />
<p>In this case, CPUs can only see single assigned GPU.</p>
</li>
<li><p class="first">2 resource sets per node: 3 GPUs and 6 cores per socket</p>
<img alt="https://www.olcf.ornl.gov/wp-content/uploads/2018/03/RS-summit-example-3GPU-6Cores.png" class="align-center" src="https://www.olcf.ornl.gov/wp-content/uploads/2018/03/RS-summit-example-3GPU-6Cores.png" />
<p>In this case, all 6 CPUs can see 3 GPUs. Code must manage CPU -&gt; GPU
communication. CPUs on socket0 can not access GPUs or Memory on socket1.</p>
</li>
<li><p class="first">Single resource set per node: 6 GPUs, 12 cores</p>
<img alt="https://www.olcf.ornl.gov/wp-content/uploads/2018/03/RS-summit-example-6GPU-12Core.png" class="align-center" src="https://www.olcf.ornl.gov/wp-content/uploads/2018/03/RS-summit-example-6GPU-12Core.png" />
<p>In this case, all 12 CPUs can see all node’s 6 GPUs. Code must manage CPU to
GPU communication. CPUs on socket0 can access GPUs and Memory on socket1.
Code must manage cross socket communication.</p>
</li>
</ol>
</div>
<div class="section" id="designing-a-resource-set">
<h5>Designing a Resource Set<a class="headerlink" href="#designing-a-resource-set" title="Permalink to this headline">¶</a></h5>
<p>Resource sets allow each jsrun to control how the node appears to a
code. This method is unique to jsrun, and requires thinking of each job
launch differently than aprun or mpirun. While the method is unique, the
method is not complicated and can be reasoned in a few basic steps.</p>
<p>The first step to creating resource sets is understanding how a code would
like the node to appear. For example, the number of tasks/threads per
GPU. Once this is understood, the next step is to simply calculate the
number of resource sets that can fit on a node. From here, the number of
needed nodes can be calculated and passed to the batch job request.</p>
<p>The basic steps to creating resource sets:</p>
<ol class="arabic">
<li><dl class="first docutils">
<dt>Understand how your code expects to interact with the system.</dt><dd><p>How many tasks/threads per GPU?</p>
<p>Does each task expect to see a single GPU? Do multiple tasks expect
to share a GPU? Is the code written to internally manage task to GPU
workload based on the number of available cores and GPUs?</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Create resource sets containing the needed GPU to task binding</dt><dd><p>Based on how your code expects to interact with the system, you can
create resource sets containing the needed GPU and core resources.
If a code expects to utilize one GPU per task, a resource set would
contain one core and one GPU. If a code expects to pass work to a
single GPU from two tasks, a resource set would contain two cores
and one GPU.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Decide on the number of resource sets needed</dt><dd><p>Once you understand tasks, threads, and GPUs in a resource set, you
simply need to decide the number of resource sets needed.</p>
</dd>
</dl>
</li>
</ol>
<p>As on any system, it is useful to keep in mind the hardware underneath every
execution. This is particularly true when laying out resource sets.</p>
</div>
</div>
<div class="section" id="launching-a-job-with-jsrun">
<h4>Launching a Job with jsrun<a class="headerlink" href="#launching-a-job-with-jsrun" title="Permalink to this headline">¶</a></h4>
<div class="section" id="jsrun-format">
<h5>jsrun Format<a class="headerlink" href="#jsrun-format" title="Permalink to this headline">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jsrun</span>    <span class="p">[</span> <span class="o">-</span><span class="n">n</span> <span class="c1">#resource sets ]   [tasks, threads, and GPUs within each resource set]   program [ program args ]</span>
</pre></div>
</div>
</div>
<div class="section" id="common-jsrun-options">
<h5>Common jsrun Options<a class="headerlink" href="#common-jsrun-options" title="Permalink to this headline">¶</a></h5>
<p>Below are common jsrun options. More flags and details can be found in the jsrun
man page. The defaults listed in the table below are the OLCF defaults and take
precedence over those mentioned in the man page.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="23%" />
<col width="7%" />
<col width="45%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head" colspan="2">Flags</th>
<th class="head" rowspan="2">Description</th>
<th class="head" rowspan="2">Default Value</th>
</tr>
<tr class="row-even"><th class="head">Long</th>
<th class="head">Short</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">--nrs</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-n</span></code></td>
<td>Number of resource sets</td>
<td>All available physical cores</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">--tasks_per_rs</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-a</span></code></td>
<td>Number of MPI tasks (ranks) per resource set</td>
<td>Not set by default, instead
total tasks (-p) set</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">--cpu_per_rs</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-c</span></code></td>
<td>Number of CPUs (cores) per resource set.</td>
<td>1</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">--gpu_per_rs</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-g</span></code></td>
<td>Number of GPUs per resource set</td>
<td>0</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">--bind</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-b</span></code></td>
<td>Binding of tasks within a resource set. Can be none,
rs, or packed:#</td>
<td>packed:1</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">--rs_per_host</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-r</span></code></td>
<td>Number of resource sets per host</td>
<td>No default</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">--latency_priority</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-l</span></code></td>
<td>Latency Priority. Controls layout
priorities. Can currently be cpu-cpu or gpu-cpu</td>
<td>gpu-cpu,cpu-mem,cpu-cpu</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">--launch_distribution</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">-d</span></code></td>
<td>How tasks are started on resource sets</td>
<td>packed</td>
</tr>
</tbody>
</table>
<p>It’s recommended to explicitly specify <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> options and not rely on the
default values. This most often includes <code class="docutils literal notranslate"><span class="pre">--nrs</span></code>,<code class="docutils literal notranslate"><span class="pre">--cpu_per_rs</span></code>,
<code class="docutils literal notranslate"><span class="pre">--gpu_per_rs</span></code>, <code class="docutils literal notranslate"><span class="pre">--tasks_per_rs</span></code>, <code class="docutils literal notranslate"><span class="pre">--bind</span></code>, and <code class="docutils literal notranslate"><span class="pre">--launch_distribution</span></code>.</p>
</div>
</div>
<div class="section" id="jsrun-examples">
<h4>jsrun Examples<a class="headerlink" href="#jsrun-examples" title="Permalink to this headline">¶</a></h4>
<p>The below examples were launched in the following 2 node interactive
batch job:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>summit&gt; bsub -nnodes 2 -Pprj123 -W02:00 -Is $SHELL
</pre></div>
</div>
<div class="section" id="single-mpi-task-single-gpu-per-rs">
<h5>Single MPI Task, single GPU per RS<a class="headerlink" href="#single-mpi-task-single-gpu-per-rs" title="Permalink to this headline">¶</a></h5>
<p>The following example will create 12 resource sets each with 1 MPI task
and 1 GPU. Each MPI task will have access to a single GPU.</p>
<p>Rank 0 will have access to GPU 0 on the first node ( red resource set).
Rank 1 will have access to GPU 1 on the first node ( green resource set).
This pattern will continue until 12 resources sets have been created.</p>
<p>The following jsrun command will request 12 resource sets (<code class="docutils literal notranslate"><span class="pre">-n12</span></code>) 6
per node (<code class="docutils literal notranslate"><span class="pre">-r6</span></code>). Each resource set will contain 1 MPI task (<code class="docutils literal notranslate"><span class="pre">-a1</span></code>),
1 GPU (<code class="docutils literal notranslate"><span class="pre">-g1</span></code>), and 1 core (<code class="docutils literal notranslate"><span class="pre">-c1</span></code>).</p>
<img alt="../_images/summit-jsrun-example-1Core-1GPU.png" class="align-center" src="../_images/summit-jsrun-example-1Core-1GPU.png" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n12</span> <span class="o">-</span><span class="n">r6</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="o">-</span><span class="n">c1</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">0</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n04</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">1</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n04</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">2</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n04</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">3</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">88</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n04</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">4</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">92</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n04</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">5</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">96</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n04</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">5</span>

<span class="n">Rank</span><span class="p">:</span>    <span class="mi">6</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n03</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">7</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n03</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">8</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n03</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">9</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">88</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n03</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">10</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">92</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n03</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">11</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">96</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">h41n03</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
<div class="section" id="multiple-tasks-single-gpu-per-rs">
<h5>Multiple tasks, single GPU per RS<a class="headerlink" href="#multiple-tasks-single-gpu-per-rs" title="Permalink to this headline">¶</a></h5>
<p>The following jsrun command will request 12 resource sets (<code class="docutils literal notranslate"><span class="pre">-n12</span></code>).
Each resource set will contain 2 MPI tasks (<code class="docutils literal notranslate"><span class="pre">-a2</span></code>), 1 GPU
(<code class="docutils literal notranslate"><span class="pre">-g1</span></code>), and 2 cores (<code class="docutils literal notranslate"><span class="pre">-c2</span></code>). 2 MPI tasks will have access to a
single GPU. Ranks 0 - 1 will have access to GPU 0 on the first node (
red resource set). Ranks 2 - 3 will have access to GPU 1 on the first
node ( green resource set). This pattern will continue until 12 resource
sets have been created.</p>
<img alt="../_images/summit-jsrun-example-2taskperGPU.png" class="align-center" src="../_images/summit-jsrun-example-2taskperGPU.png" />
<p><strong>Adding cores to the RS:</strong> The <code class="docutils literal notranslate"><span class="pre">-c</span></code> flag should be used to request
the needed cores for tasks and treads. The default -c core count is 1.
In the above example, if -c is not specified both tasks will run on a
single core.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n12</span> <span class="o">-</span><span class="n">a2</span> <span class="o">-</span><span class="n">g1</span> <span class="o">-</span><span class="n">c2</span> <span class="o">-</span><span class="n">dpacked</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span> <span class="o">|</span> <span class="n">sort</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">0</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">1</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span>

<span class="n">Rank</span><span class="p">:</span>    <span class="mi">2</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">3</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">12</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">1</span>

<span class="n">Rank</span><span class="p">:</span>    <span class="mi">4</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">16</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">5</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">20</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">2</span>

<span class="n">Rank</span><span class="p">:</span>    <span class="mi">6</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">88</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">7</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">92</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span>

<span class="n">Rank</span><span class="p">:</span>    <span class="mi">8</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">96</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">9</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">100</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">4</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">10</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">11</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">108</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">5</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">12</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">13</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">14</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">15</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">12</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">1</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">16</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">16</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">17</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">20</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">2</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">18</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">88</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">19</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">92</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">20</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">96</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">21</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">100</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">4</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">22</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">23</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">108</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a01n01</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">5</span>

<span class="n">summit</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="multiple-task-multiple-gpu-per-rs">
<h5>Multiple Task, Multiple GPU per RS<a class="headerlink" href="#multiple-task-multiple-gpu-per-rs" title="Permalink to this headline">¶</a></h5>
<p>The following example will create 4 resource sets each with 6 tasks and
3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5
will have access to GPUs 0 - 2 on the first socket of the first node (
red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the
second socket of the first node ( green resource set). This pattern will
continue until 4 resource sets have been created. The following jsrun
command will request 4 resource sets (<code class="docutils literal notranslate"><span class="pre">-n4</span></code>). Each resource set will
contain 6 MPI tasks (<code class="docutils literal notranslate"><span class="pre">-a6</span></code>), 3 GPUs (<code class="docutils literal notranslate"><span class="pre">-g3</span></code>), and 6 cores
(<code class="docutils literal notranslate"><span class="pre">-c6</span></code>).</p>
<img alt="../_images/RS-summit-example-24Tasks-3GPU-6Cores.png" class="align-center" src="../_images/RS-summit-example-24Tasks-3GPU-6Cores.png" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">4</span> <span class="o">-</span><span class="n">a</span> <span class="mi">6</span> <span class="o">-</span><span class="n">c</span> <span class="mi">6</span> <span class="o">-</span><span class="n">g</span> <span class="mi">3</span> <span class="o">-</span><span class="n">d</span> <span class="n">packed</span> <span class="o">-</span><span class="n">l</span> <span class="n">GPU</span><span class="o">-</span><span class="n">CPU</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">0</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">1</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">2</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">3</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">12</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">4</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">16</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">5</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">20</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>

<span class="n">Rank</span><span class="p">:</span>    <span class="mi">6</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">88</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">7</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">92</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">8</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">96</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>    <span class="mi">9</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">100</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">10</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">11</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">108</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">12</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">13</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">14</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>   <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">15</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">12</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">16</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">16</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">17</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">20</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>

<span class="n">Rank</span><span class="p">:</span>   <span class="mi">18</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">88</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">19</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">92</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">20</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span>  <span class="mi">96</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">21</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">100</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">22</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">Rank</span><span class="p">:</span>   <span class="mi">23</span><span class="p">;</span> <span class="n">NumRanks</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">108</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">GPU</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">summit</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="single-task-multiple-gpus-multiple-threads-per-rs">
<h5>Single Task, Multiple GPUs, Multiple Threads per RS<a class="headerlink" href="#single-task-multiple-gpus-multiple-threads-per-rs" title="Permalink to this headline">¶</a></h5>
<p>The following example will create 12 resource sets each with 1 task, 4
threads, and 1 GPU. Each MPI task will start 4 threads and have access
to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the
first socket of the first node ( red resource set). Rank 2 will have
access to GPU 1 and start 4 threads on the second socket of the first
node ( green resource set). This pattern will continue until 12 resource
sets have been created. The following jsrun command will create 12
resource sets (<code class="docutils literal notranslate"><span class="pre">-n12</span></code>). Each resource set will contain 1 MPI task
(<code class="docutils literal notranslate"><span class="pre">-a1</span></code>), 1 GPU (<code class="docutils literal notranslate"><span class="pre">-g1</span></code>), and 4 cores (<code class="docutils literal notranslate"><span class="pre">-c4</span></code>). Notice that
more cores are requested than MPI tasks; the extra cores will be needed
to place threads. Without requesting additional cores, threads will be
placed on a single core.</p>
<img alt="../_images/RS-summit-example-4Threads-4Core-1GPU.png" class="align-center" src="../_images/RS-summit-example-4Threads-4Core-1GPU.png" />
<p><strong>Requesting Cores for Threads:</strong> The <code class="docutils literal notranslate"><span class="pre">-c</span></code> flag should be used to
request additional cores for thread placement. Without requesting
additional cores, threads will be placed on a single core.</p>
<p><strong>Binding Cores to Tasks:</strong> The <code class="docutils literal notranslate"><span class="pre">-b</span></code> binding flag should be used to
bind cores to tasks. Without specifying binding, all threads will be
bound to the first core.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">setenv</span> <span class="n">OMP_NUM_THREADS</span> <span class="mi">4</span>
<span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n12</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">c4</span> <span class="o">-</span><span class="n">g1</span> <span class="o">-</span><span class="n">b</span> <span class="n">packed</span><span class="p">:</span><span class="mi">4</span> <span class="o">-</span><span class="n">d</span> <span class="n">packed</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">},{</span><span class="mi">4</span><span class="p">},{</span><span class="mi">8</span><span class="p">},{</span><span class="mi">12</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">4</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">},{</span><span class="mi">4</span><span class="p">},{</span><span class="mi">8</span><span class="p">},{</span><span class="mi">12</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">2</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">8</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">},{</span><span class="mi">4</span><span class="p">},{</span><span class="mi">8</span><span class="p">},{</span><span class="mi">12</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">3</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">12</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">},{</span><span class="mi">4</span><span class="p">},{</span><span class="mi">8</span><span class="p">},{</span><span class="mi">12</span><span class="p">}</span>

<span class="n">Rank</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">16</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">16</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">16</span><span class="p">},{</span><span class="mi">20</span><span class="p">},{</span><span class="mi">24</span><span class="p">},{</span><span class="mi">28</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">16</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">20</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">16</span><span class="p">},{</span><span class="mi">20</span><span class="p">},{</span><span class="mi">24</span><span class="p">},{</span><span class="mi">28</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">16</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">2</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">24</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">16</span><span class="p">},{</span><span class="mi">20</span><span class="p">},{</span><span class="mi">24</span><span class="p">},{</span><span class="mi">28</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">16</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">3</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">28</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n06</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">16</span><span class="p">},{</span><span class="mi">20</span><span class="p">},{</span><span class="mi">24</span><span class="p">},{</span><span class="mi">28</span><span class="p">}</span>

<span class="o">...</span>

<span class="n">Rank</span><span class="p">:</span> <span class="mi">10</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">104</span><span class="p">},{</span><span class="mi">108</span><span class="p">},{</span><span class="mi">112</span><span class="p">},{</span><span class="mi">116</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">10</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">108</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">104</span><span class="p">},{</span><span class="mi">108</span><span class="p">},{</span><span class="mi">112</span><span class="p">},{</span><span class="mi">116</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">10</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">2</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">112</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">104</span><span class="p">},{</span><span class="mi">108</span><span class="p">},{</span><span class="mi">112</span><span class="p">},{</span><span class="mi">116</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">10</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">104</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">3</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">116</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">104</span><span class="p">},{</span><span class="mi">108</span><span class="p">},{</span><span class="mi">112</span><span class="p">},{</span><span class="mi">116</span><span class="p">}</span>

<span class="n">Rank</span><span class="p">:</span> <span class="mi">11</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">120</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">120</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">120</span><span class="p">},{</span><span class="mi">124</span><span class="p">},{</span><span class="mi">128</span><span class="p">},{</span><span class="mi">132</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">11</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">120</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">1</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">124</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">120</span><span class="p">},{</span><span class="mi">124</span><span class="p">},{</span><span class="mi">128</span><span class="p">},{</span><span class="mi">132</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">11</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">120</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">2</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">128</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">120</span><span class="p">},{</span><span class="mi">124</span><span class="p">},{</span><span class="mi">128</span><span class="p">},{</span><span class="mi">132</span><span class="p">}</span>
<span class="n">Rank</span><span class="p">:</span> <span class="mi">11</span><span class="p">;</span> <span class="n">RankCore</span><span class="p">:</span> <span class="mi">120</span><span class="p">;</span> <span class="n">Thread</span><span class="p">:</span> <span class="mi">3</span><span class="p">;</span> <span class="n">ThreadCore</span><span class="p">:</span> <span class="mi">132</span><span class="p">;</span> <span class="n">Hostname</span><span class="p">:</span> <span class="n">a33n05</span><span class="p">;</span> <span class="n">OMP_NUM_PLACES</span><span class="p">:</span> <span class="p">{</span><span class="mi">120</span><span class="p">},{</span><span class="mi">124</span><span class="p">},{</span><span class="mi">128</span><span class="p">},{</span><span class="mi">132</span><span class="p">}</span>

<span class="n">summit</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="hardware-threads-multiple-threads-per-core">
<h5>Hardware Threads: Multiple Threads per Core<a class="headerlink" href="#hardware-threads-multiple-threads-per-core" title="Permalink to this headline">¶</a></h5>
<p>Each physical core on Summit contains 4 hardware threads. The SMT level
can be set using LSF flags:</p>
<p>SMT1</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#BSUB -alloc_flags smt1</span>
<span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">c1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">bpacked</span><span class="p">:</span><span class="mi">4</span> <span class="n">csh</span> <span class="o">-</span><span class="n">c</span> <span class="s1">&#39;echo $OMP_PLACES’</span>
<span class="mi">0</span>
</pre></div>
</div>
<p>SMT2</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#BSUB -alloc_flags smt2</span>
<span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">c1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">bpacked</span><span class="p">:</span><span class="mi">4</span> <span class="n">csh</span> <span class="o">-</span><span class="n">c</span> <span class="s1">&#39;echo $OMP_PLACES’</span>
<span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>SMT4</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#BSUB -alloc_flags smt4</span>
<span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">c1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">bpacked</span><span class="p">:</span><span class="mi">4</span> <span class="n">csh</span> <span class="o">-</span><span class="n">c</span> <span class="s1">&#39;echo $OMP_PLACES’</span>
<span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">}</span>
</pre></div>
</div>
<img alt="../_images/FS-summit-example-MultiThreadPerCore.png" class="align-center" src="../_images/FS-summit-example-MultiThreadPerCore.png" />
</div>
<div class="section" id="common-use-cases">
<h5>Common Use Cases<a class="headerlink" href="#common-use-cases" title="Permalink to this headline">¶</a></h5>
<p>The following table provides a quick reference for creating resource
sets of various common use cases. The <code class="docutils literal notranslate"><span class="pre">-n</span></code> flag can be altered to
specify the number of resource sets needed.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="16%" />
<col width="12%" />
<col width="10%" />
<col width="17%" />
<col width="8%" />
<col width="37%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Resource Sets</th>
<th class="head">MPI Tasks</th>
<th class="head">Threads</th>
<th class="head">Physical Cores</th>
<th class="head">GPUs</th>
<th class="head">jsrun Command</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>42</td>
<td>0</td>
<td>42</td>
<td>0</td>
<td>jsrun -n1 -a42 -c42 -g0</td>
</tr>
<tr class="row-odd"><td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>jsrun -n1 -a1 -c1 -g1</td>
</tr>
<tr class="row-even"><td>1</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>jsrun -n1 -a2 -c2 -g1</td>
</tr>
<tr class="row-odd"><td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>jsrun -n1 -a1 -c1 -g2</td>
</tr>
<tr class="row-even"><td>1</td>
<td>1</td>
<td>21</td>
<td>21</td>
<td>3</td>
<td>jsrun -n1 -a1 -c21 -g3 -bpacked:21</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="concurrent-job-steps">
<h5>Concurrent Job Steps<a class="headerlink" href="#concurrent-job-steps" title="Permalink to this headline">¶</a></h5>
<p>By default, multiple invocations of <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> in a job script will execute
serially. To execute multiple job steps concurrently, standard UNIX process
backgrounding can be used by adding a <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> at the end of the command. This
will return control to the job script and execute the next command immediately.
A <code class="docutils literal notranslate"><span class="pre">wait</span></code> command must follow all backgrounded processes to prevent the job
from appearing completed and exiting prematurely.</p>
<p>The following example executes three backgrounded job steps and waits for them
to finish before the job ends.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 1
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
jsrun &lt;options&gt; ./a.out &amp;
jsrun &lt;options&gt; ./a.out &amp;
jsrun &lt;options&gt; ./a.out &amp;
wait
</pre></div>
</div>
<p>As submission scripts (and interactive sessions) are executed on batch nodes,
the number of concurrent job steps is limited by the per-user process limit on
a batch node, where a single user is only permitted 4096 simultaneous
processes. This limit is per user on each batch node, not per batch job.</p>
<p>Each job step will create 3 processes, and JSM management may create up to ~23
processes. This creates an upper-limit of ~1350 simultaneous job steps.</p>
<p>If JSM or PMIX errors occur as the result of backgrounding many job steps, using the
<code class="docutils literal notranslate"><span class="pre">--immediate</span></code> option to <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> may help, as shown in the following example.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 1
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
jsrun &lt;options&gt; --immediate ./a.out
jsrun &lt;options&gt; --immediate ./a.out
jsrun &lt;options&gt; --immediate ./a.out
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">By default, <code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">--immediate</span></code> does not produce <code class="docutils literal notranslate"><span class="pre">stdout</span></code> or
<code class="docutils literal notranslate"><span class="pre">stderr</span></code>. To capture <code class="docutils literal notranslate"><span class="pre">stdout</span></code> and/or <code class="docutils literal notranslate"><span class="pre">stderr</span></code> when using this option,
additionally include <code class="docutils literal notranslate"><span class="pre">--stdio_stdout</span></code>/<code class="docutils literal notranslate"><span class="pre">-o</span></code> and/or
<code class="docutils literal notranslate"><span class="pre">--stdio_stderr</span></code>/<code class="docutils literal notranslate"><span class="pre">-k</span></code>.</p>
</div>
</div>
</div>
<div class="section" id="explicit-resource-files-erf">
<h4>Explicit Resource Files (ERF)<a class="headerlink" href="#explicit-resource-files-erf" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/jsm/10.3/base/erf_format.html" target="_blank">Explicit Resource Files</a>
provide even more fine-granied control over how processes are mapped onto
compute nodes. ERFs can define job step options such as rank placement/binding,
SMT/CPU/GPU resources, compute hosts, among many others. If you find that the
most common jsrun options do not readily provide the resource layout you need,
we recommend considering ERF files.</p>
<p>A common source of confusion when using ERFs is how physical cores are
enumerated. See the tutorial on <a class="reference external" href="https://github.com/olcf-tutorials/ERF-CPU-Indexing" target="_blank">ERF CPU
Indexing</a> for a
discussion of the <code class="docutils literal notranslate"><span class="pre">cpu_index_using</span></code> control and its interaction with various
SMT modes.</p>
</div>
<div class="section" id="jsrun-tools">
<h4>jsrun Tools<a class="headerlink" href="#jsrun-tools" title="Permalink to this headline">¶</a></h4>
<p>This section describes tools that users might find helpful to better
understand the jsrun job launcher.</p>
<div class="section" id="hello-jsrun">
<h5>Hello_jsrun<a class="headerlink" href="#hello-jsrun" title="Permalink to this headline">¶</a></h5>
<p>Hello_jsrun is a “Hello World”-type program that users can run on
Summit nodes to better understand how MPI ranks and OpenMP threads are
mapped to the hardware. <a class="reference external" href="https://code.ornl.gov/t4p/Hello_jsrun" target="_blank">https://code.ornl.gov/t4p/Hello_jsrun</a> A
screencast showing how to use Hello_jsrun is also available:
<a class="reference external" href="https://vimeo.com/261038849" target="_blank">https://vimeo.com/261038849</a></p>
</div>
<div class="section" id="job-step-viewer">
<h5>Job Step Viewer<a class="headerlink" href="#job-step-viewer" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://jobstepviewer.olcf.ornl.gov/" target="_blank">Job Step Viewer</a> provides a graphical view of an application’s runtime layout on Summit.
It allows users to preview and quickly iterate with multiple <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> options to
understand and optimize job launch.</p>
<p>For bug reports or suggestions, please email <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
<div class="section" id="usage">
<h6>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>Request a Summit allocation</dt><dd><ul>
<li><code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-W</span> <span class="pre">10</span> <span class="pre">-nnodes</span> <span class="pre">2</span> <span class="pre">-P</span> <span class="pre">$OLCF_PROJECT_ID</span> <span class="pre">-Is</span> <span class="pre">$SHELL</span></code></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Load the <code class="docutils literal notranslate"><span class="pre">job-step-viewer</span></code> module</dt><dd><ul>
<li><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">job-step-viewer</span></code></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Test out a <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> line by itself, or provide an executable as normal</dt><dd><ul>
<li><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n12</span> <span class="pre">-r6</span> <span class="pre">-c7</span> <span class="pre">-g1</span> <span class="pre">-a1</span> <span class="pre">EOMP_NUM_THREADS=7</span> <span class="pre">-brs</span></code></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Visit the provided URL</dt><dd><ul>
<li><a class="reference external" href="https://jobstepviewer.olcf.ornl.gov/summit/871957-1" target="_blank">https://jobstepviewer.olcf.ornl.gov/summit/871957-1</a></li>
</ul>
</dd>
</dl>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Most Terminal applications have built-in shortcuts to directly open
web addresses in the default browser.</p>
<ul class="last simple">
<li>MacOS Terminal.app: hold Command (⌘) and double-click on the URL</li>
<li>iTerm2: hold Command (⌘) and single-click on the URL</li>
</ul>
</div>
</div>
<div class="section" id="limitations">
<h6>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h6>
<ul class="simple">
<li>(currently) Compiled with GCC toolchain only</li>
<li>Does not support MPMD-mode via ERF</li>
<li>OpenMP only supported with use of the <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> environment variable.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="more-information">
<h4>More Information<a class="headerlink" href="#more-information" title="Permalink to this headline">¶</a></h4>
<p>This section provides some of the most commonly used LSF commands as
well as some of the most useful options to those commands and
information on <code class="docutils literal notranslate"><span class="pre">jsrun</span></code>, Summit’s job launch command. Many commands
have much more information than can be easily presented here. More
information about these commands is available via the online manual
(i.e. <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">jsrun</span></code>). Additional LSF information can be found on <a class="reference external" href="https://www.ibm.com/support/knowledgecenter/en/SSWRJV/product_welcome_spectrum_lsf.html" target="_blank">IBM’s
website</a>.</p>
</div>
</div>
<div class="section" id="cuda-aware-mpi">
<h3>CUDA-Aware MPI<a class="headerlink" href="#cuda-aware-mpi" title="Permalink to this headline">¶</a></h3>
<p>CUDA-Aware MPI and GPUDirect are often used interchangeably, but they
are distinct topics.</p>
<p>CUDA-Aware MPI allows GPU buffers (e.g., GPU memory allocated with
<code class="docutils literal notranslate"><span class="pre">cudaMalloc</span></code>) to be used directly in MPI calls rather than requiring
data to be manually transferred to/from a CPU buffer (e.g., using
<code class="docutils literal notranslate"><span class="pre">cudaMemcpy</span></code>) before/after passing data in MPI calls. By itself,
CUDA-Aware MPI does not specify whether data is staged through
CPU memory or, for example, transferred directly between GPUs when
passing GPU buffers to MPI calls. That is where GPUDirect comes in.</p>
<p>GPUDirect is a technology that can be implemented on a system to enhance
CUDA-Aware MPI by allowing data transfers directly between GPUs on the
same node (peer-to-peer) and/or directly between GPUs on different nodes
(with RDMA support) without the need to stage data through CPU memory.
On Summit, both peer-to-peer and RDMA support are implemented. To enable
CUDA-Aware MPI in a job, use the following argument to <code class="docutils literal notranslate"><span class="pre">jsrun</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jsrun</span> <span class="o">--</span><span class="n">smpiargs</span><span class="o">=</span><span class="s2">&quot;-gpu&quot;</span> <span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="monitoring-jobs">
<h3>Monitoring Jobs<a class="headerlink" href="#monitoring-jobs" title="Permalink to this headline">¶</a></h3>
<p>LSF provides several utilities with which you can monitor jobs. These
include monitoring the queue, getting details about a particular job,
viewing STDOUT/STDERR of running jobs, and more.</p>
<p>The most straightforward monitoring is with the <code class="docutils literal notranslate"><span class="pre">bjobs</span></code> command. This
command will show the current queue, including both pending and running
jobs. Running <code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-l</span></code> will provide much more detail about a job (or
group of jobs). For detailed output of a single job, specify the job id
after the <code class="docutils literal notranslate"><span class="pre">-l</span></code>. For example, for detailed output of job 12345, you can
run <code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-l</span> <span class="pre">12345</span></code> . Other options to <code class="docutils literal notranslate"><span class="pre">bjobs</span></code> are shown below. In
general, if the command is specified with <code class="docutils literal notranslate"><span class="pre">-u</span> <span class="pre">all</span></code> it will show
information for all users/all jobs. Without that option, it only shows
your jobs. Note that this is not an exhaustive list. See <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">bjobs</span></code>
for more information.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="22%" />
<col width="78%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span></code></td>
<td>Show your current jobs in the queue</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-u</span> <span class="pre">all</span></code></td>
<td>Show currently queued jobs for all users</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-P</span> <span class="pre">ABC123</span></code></td>
<td>Shows currently-queued jobs for project ABC123</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-UF</span></code></td>
<td>Don’t format output (might be useful if you’re using the output in a script)</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-a</span></code></td>
<td>Show jobs in all states, including recently finished jobs</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-l</span></code></td>
<td>Show long/detailed output</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-l</span> <span class="pre">12345</span></code></td>
<td>Show long/detailed output for jobs 12345</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-d</span></code></td>
<td>Show details for recently completed jobs</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-s</span></code></td>
<td>Show suspended jobs, including the reason(s) they’re suspended</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-r</span></code></td>
<td>Show running jobs</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-p</span></code></td>
<td>Show pending jobs</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-w</span></code></td>
<td>Use “wide” formatting for output</td>
</tr>
</tbody>
</table>
<p>If you want to check the STDOUT/STDERR of a currently running job, you
can do so with the <code class="docutils literal notranslate"><span class="pre">bpeek</span></code> command. The command supports several
options:</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="21%" />
<col width="79%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bpeek</span> <span class="pre">-J</span> <span class="pre">jobname</span></code></td>
<td>Show STDOUT/STDERR for the job you’ve most recently submitted with the name jobname</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bpeek</span> <span class="pre">12345</span></code></td>
<td>Show STDOUT/STDERR for job 12345</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bpeek</span> <span class="pre">-f</span> <span class="pre">...</span></code></td>
<td>Used with other options. Makes <code class="docutils literal notranslate"><span class="pre">bpeek</span></code> use <code class="docutils literal notranslate"><span class="pre">tail</span> <span class="pre">-f</span></code> and exit once the job completes.</td>
</tr>
</tbody>
</table>
<p>The OLCF also provides <code class="docutils literal notranslate"><span class="pre">jobstat</span></code>, which adds dividers in the queue to
identify jobs as running, eligible, or blocked. Run without arguments,
<code class="docutils literal notranslate"><span class="pre">jobstat</span></code> provides a snapshot of the entire batch queue. Additional
information, including the number of jobs in each state, total nodes
available, and relative job priority are also included.</p>
<p><code class="docutils literal notranslate"><span class="pre">jobstat</span> <span class="pre">-u</span> <span class="pre">&lt;username&gt;</span></code> restricts output to only the jobs of a
specific user. See the <code class="docutils literal notranslate"><span class="pre">jobstat</span></code> man page for a full list of
formatting arguments.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ jobstat -u &lt;user&gt;
--------------------------- Running Jobs: 2 (4544 of 4604 nodes, 98.70%) ---------------------------
JobId    Username   Project          Nodes Remain     StartTime       JobName
331590   user     project           2     57:06      04/09 10:06:23  Not_Specified
331707   user     project           40    39:47      04/09 11:04:04  runA
----------------------------------------- Eligible Jobs: 3 -----------------------------------------
JobId    Username   Project          Nodes Walltime   QueueTime       Priority JobName
331712   user     project           80    45:00      04/09 11:06:23  501.00   runB
331713   user     project           90    45:00      04/09 11:07:19  501.00   runC
331714   user     project           100   45:00      04/09 11:07:49  501.00   runD
----------------------------------------- Blocked Jobs: 1 ------------------------------------------
JobId    Username   Project          Nodes Walltime   BlockReason
331715   user        project           12    2:00:00    Job dependency condition not satisfied
</pre></div>
</div>
<div class="section" id="inspecting-backfill">
<h4>Inspecting Backfill<a class="headerlink" href="#inspecting-backfill" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">bjobs</span></code> and <code class="docutils literal notranslate"><span class="pre">jobstat</span></code> help to identify what’s currently running and
scheduled to run, but sometimes it’s beneficial to know how much of the
system is <em>not</em> currently in use or scheduled for use.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">bslots</span></code> command can be used to inspect backfill windows and answer
the question “How many nodes are currently available, and for how long
will they remain available?” This can be thought of as identifying gaps in
the system’s current job schedule. By intentionally requesting resources
within the parameters of a backfill window, one can potentially shorten
their queued time and improve overall system utilization.</p>
<p>LSF uses “slots” to describe allocatable resources. Summit compute nodes have 1
slot per CPU core, for a total of 42 per node ([2x] Power9 CPUs, each
with 21 cores). Since Summit nodes are scheduled in whole-node
allocations, the output from <code class="docutils literal notranslate"><span class="pre">bslots</span></code> can be divided by 42 to see how
many nodes are currently available.</p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">bslots</span></code> output includes launch node slots, which can
cause unwanted and inflated fractional node values. The output can
be adjusted to reflect only available compute node slots with the
flag  <code class="docutils literal notranslate"><span class="pre">-R”select[CN]”</span></code>. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ bslots -R&quot;select[CN]&quot;
SLOTS          RUNTIME
42             25 hours 42 minutes 51 seconds
27384          1 hours 11 minutes 50 seconds
</pre></div>
</div>
<p>27384 compute node slots / 42 slots per node = 652 compute nodes are
available for 1 hour, 11 minutes, 50 seconds.</p>
<p>A more specific <code class="docutils literal notranslate"><span class="pre">bslots</span></code> query could check for a backfill window with
space to fit a 1000 node job for 10 minutes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ bslots -R&quot;select[CN]&quot; -n $((1000*42)) -W10
SLOTS          RUNTIME
127764         22 minutes 55 seconds
</pre></div>
</div>
<p>There is no guarantee that the slots reported by <code class="docutils literal notranslate"><span class="pre">bslots</span></code> will still
be available at time of new job submission.</p>
</div>
</div>
<div class="section" id="interacting-with-jobs">
<h3>Interacting With Jobs<a class="headerlink" href="#interacting-with-jobs" title="Permalink to this headline">¶</a></h3>
<p>Sometimes it’s necessary to interact with a batch job after it has been
submitted. LSF provides several commands for interacting with
already-submitted jobs.</p>
<p>Many of these commands can operate on either one job or a group of jobs.
In general, they only operate on the most recently submitted job that
matches other criteria provided unless “0” is specified as the job id.</p>
<div class="section" id="suspending-and-resuming-jobs">
<h4>Suspending and Resuming Jobs<a class="headerlink" href="#suspending-and-resuming-jobs" title="Permalink to this headline">¶</a></h4>
<p>LSF supports user-level suspension and resumption of jobs. Jobs are
suspended with the <code class="docutils literal notranslate"><span class="pre">bstop</span></code> command and resumed with the <code class="docutils literal notranslate"><span class="pre">bresume</span></code>
command. The simplest way to invoke these commands is to list the job id
to be suspended/resumed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bstop</span> <span class="mi">12345</span>
<span class="n">bresume</span> <span class="mi">12345</span>
</pre></div>
</div>
<p>Instead of specifying a job id, you can specify other criteria that will
allow you to suspend some/all jobs that meet other criteria such as a
job name, a queue name, etc. These are described in the manpages for
<code class="docutils literal notranslate"><span class="pre">bstop</span></code> and <code class="docutils literal notranslate"><span class="pre">bresume</span></code>.</p>
</div>
<div class="section" id="signaling-jobs">
<h4>Signaling Jobs<a class="headerlink" href="#signaling-jobs" title="Permalink to this headline">¶</a></h4>
<p>You can send signals to jobs with the <code class="docutils literal notranslate"><span class="pre">bkill</span></code> command. While the
command name suggests its only purpose is to terminate jobs, this is not
the case. Similar to the <code class="docutils literal notranslate"><span class="pre">kill</span></code> command found in Unix-like operating
systems, this command can be used to send various signals (not just
<code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> and <code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code>) to jobs. The command can accept both
numbers and names for signals. For a list of accepted signal names, run
<code class="docutils literal notranslate"><span class="pre">bkill</span> <span class="pre">-l</span></code>. Common ways to invoke the command include:</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="25%" />
<col width="75%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bkill</span> <span class="pre">12345</span></code></td>
<td>Force a job to stop by sending <code class="docutils literal notranslate"><span class="pre">SIGINT</span></code>,
<code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code>, and <code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code>. These signals are sent in that order, so users
can write applications such that they will trap <code class="docutils literal notranslate"><span class="pre">SIGINT</span></code> and/or <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code>
and exit in a controlled manner.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bkill</span> <span class="pre">-s</span> <span class="pre">USR1</span> <span class="pre">12345</span></code></td>
<td>Send <code class="docutils literal notranslate"><span class="pre">SIGUSR1</span></code> to job 12345 NOTE: When
specifying a signal by name, omit SIG from the name. Thus, you specify <code class="docutils literal notranslate"><span class="pre">USR1</span></code>
and not <code class="docutils literal notranslate"><span class="pre">SIGUSR1</span></code> on the <code class="docutils literal notranslate"><span class="pre">bkill</span></code> command line.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bkill</span> <span class="pre">-s</span> <span class="pre">9</span> <span class="pre">12345</span></code></td>
<td>Send signal 9 to job 12345</td>
</tr>
</tbody>
</table>
<p>Like <code class="docutils literal notranslate"><span class="pre">bstop</span></code> and <code class="docutils literal notranslate"><span class="pre">bresume</span></code>, <code class="docutils literal notranslate"><span class="pre">bkill</span></code> command also supports
identifying the job(s) to be signaled by criteria other than the job id.
These include some/all jobs with a given name, in a particular queue,
etc. See <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">bkill</span></code> for more information.</p>
</div>
<div class="section" id="checkpointing-jobs">
<h4>Checkpointing Jobs<a class="headerlink" href="#checkpointing-jobs" title="Permalink to this headline">¶</a></h4>
<p>LSF documentation mentions the <code class="docutils literal notranslate"><span class="pre">bchkpnt</span></code> and <code class="docutils literal notranslate"><span class="pre">brestart</span></code> commands for
checkpointing and restarting jobs, as well as the <code class="docutils literal notranslate"><span class="pre">-k</span></code> option to
<code class="docutils literal notranslate"><span class="pre">bsub</span></code> for configuring checkpointing. Since checkpointing is very
application specific and a wide range of applications run on OLCF
resources, this type of checkpointing is not configured on Summit. If
you wish to use checkpointing (which is highly encouraged), you’ll need
to configure it within your application.</p>
<p>If you wish to implement some form of on-demand checkpointing, keep in mind
the <code class="docutils literal notranslate"><span class="pre">bkill</span></code> command is really a signaling command and you can have your
job script/application checkpoint as a response to certain signals (such
as <code class="docutils literal notranslate"><span class="pre">SIGUSR1</span></code>).</p>
</div>
</div>
<div class="section" id="other-lsf-commands">
<h3>Other LSF Commands<a class="headerlink" href="#other-lsf-commands" title="Permalink to this headline">¶</a></h3>
<p>The table below summarizes some additional LSF commands that might be
useful.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="19%" />
<col width="81%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bparams</span> <span class="pre">-a</span></code></td>
<td>Show current parameters for LSF. The behavior/available
options for some LSF commands depend on settings in various configuration
files. This command shows those settings without having to search for the
actual files.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjdepinfo</span></code></td>
<td>Show job dependency information (could be useful in
determining what job is keeping another job in a pending state)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="pbs-torque-moab-to-lsf-translation">
<h3>PBS/Torque/MOAB-to-LSF Translation<a class="headerlink" href="#pbs-torque-moab-to-lsf-translation" title="Permalink to this headline">¶</a></h3>
<p>More details about these commands are given elsewhere in this section;
the table below is simply for your convenience in looking up various LSF
commands.</p>
<p>Users of other OLCF resources are likely familiar with
PBS-like commands which are used by the Torque/Moab instances on other
systems. The table below summarizes the equivalent LSF command for
various PBS/Torque/Moab commands.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="23%" />
<col width="30%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">LSF Command</th>
<th class="head">PBS/Torque/Moab Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">job.sh</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qsub</span> <span class="pre">job.sh</span></code></td>
<td>Submit the job script job.sh to the batch system</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-Is</span> <span class="pre">/bin/bash</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qsub</span> <span class="pre">-I</span></code></td>
<td>Submit an interactive batch job</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-u</span> <span class="pre">all</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qstat</span> <span class="pre">showq</span></code></td>
<td>Show jobs currently in the queue NOTE: without the
-u all argument, bjobs will only show your jobs</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-l</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">checkjob</span></code></td>
<td>Get information about a specific job</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-d</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">showq</span> <span class="pre">-c</span></code></td>
<td>Get information about completed jobs</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-p</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">showq</span> <span class="pre">-i</span></code>
<code class="docutils literal notranslate"><span class="pre">showq</span> <span class="pre">-b</span></code>
<code class="docutils literal notranslate"><span class="pre">checkjob</span></code></td>
<td>Get information about pending jobs</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjobs</span> <span class="pre">-r</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">showq</span> <span class="pre">-r</span></code></td>
<td>Get information about running jobs</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bkill</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qsig</span></code></td>
<td>Send a signal to a job</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bkill</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qdel</span></code></td>
<td>Terminate/Kill a job</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bstop</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qhold</span></code></td>
<td>Hold a job/stop a job from running</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bresume</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qrls</span></code></td>
<td>Release a held job</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">bqueues</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">qstat</span> <span class="pre">-q</span></code></td>
<td>Get information about queues</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">bjdepinfo</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">checkjob</span></code></td>
<td>Get information about job dependencies</td>
</tr>
</tbody>
</table>
<p>The table below shows shows LSF (bsub) command-line/batch script options
and the PBS/Torque/Moab (qsub) options that provide similar
functionality.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="23%" />
<col width="34%" />
<col width="43%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">LSF Option</th>
<th class="head">PBS/Torque/Moab Option</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">60</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#PBS</span> <span class="pre">-l</span> <span class="pre">walltime=1:00:00</span></code></td>
<td>Request a walltime of 1 hour</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1024</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#PBS</span> <span class="pre">-l</span> <span class="pre">nodes=1024</span></code></td>
<td>Request 1024 nodes</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">ABC123</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#PBS</span> <span class="pre">-A</span> <span class="pre">ABC123</span></code></td>
<td>Charge the job to project ABC123</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-alloc_flags</span> <span class="pre">gpumps</span></code></td>
<td>No equivalent (set via environment variable)</td>
<td>Enable multiple MPI tasks to simultaneously access a GPU</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="easy-mode-vs-expert-mode">
<span id="easy-mode-v-expert-mode"></span><h3>Easy Mode vs. Expert Mode<a class="headerlink" href="#easy-mode-vs-expert-mode" title="Permalink to this headline">¶</a></h3>
<p>The Cluster System Management (CSM) component of the job launch
environment supports two methods of job submission, termed “easy” mode
and “expert” mode. The difference in the modes is where the
responsibility for creating the LSF resource string is placed.</p>
<p>In easy mode, the system software converts options such as -nnodes in
a batch script into the resource string needed by the scheduling system.
In expert mode, the user is responsible for creating this string and
options such as -nnodes cannot be used. In easy mode, you will not be
able to use <code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-R</span></code> to create resource strings. The system will
automatically create the resource string based on your other <code class="docutils literal notranslate"><span class="pre">bsub</span></code>
options. In expert mode, you will be able to use <code class="docutils literal notranslate"><span class="pre">-R</span></code>, but you will
not be able to use the following options to <code class="docutils literal notranslate"><span class="pre">bsub</span></code>: <code class="docutils literal notranslate"><span class="pre">-ln_slots</span></code>,
<code class="docutils literal notranslate"><span class="pre">-ln_mem</span></code>, <code class="docutils literal notranslate"><span class="pre">-cn_cu</span></code>, or <code class="docutils literal notranslate"><span class="pre">-nnodes</span></code>.</p>
<p>Most users will want to use easy mode. However, if you need precise
control over your job’s resources, such as placement on (or avoidance
of) specific nodes, you will need to use expert mode. To use expert
mode, add <code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-csm</span> <span class="pre">y</span></code> to your batch script (or <code class="docutils literal notranslate"><span class="pre">-csm</span> <span class="pre">y</span></code> to
your <code class="docutils literal notranslate"><span class="pre">bsub</span></code> command line).</p>
</div>
<div class="section" id="system-service-core-isolation">
<h3>System Service Core Isolation<a class="headerlink" href="#system-service-core-isolation" title="Permalink to this headline">¶</a></h3>
<p>One core per socket is set aside for system service tasks. The cores are
not available to jsrun. When listing available resources through jsrun,
you will not see cores with hyperthreads 84-87 and 172-175. Isolating a
socket’s system services to a single core helps to reduce jitter and
improve performance of tasks performed on the socket’s remaining cores.</p>
<p>The isolated core always operates at SMT4 regardless of the batch job’s
SMT level.</p>
<div class="section" id="gpfs-system-service-isolation">
<h4>GPFS System Service Isolation<a class="headerlink" href="#gpfs-system-service-isolation" title="Permalink to this headline">¶</a></h4>
<p>By default, GPFS system service tasks are forced onto only the isolated
cores. This can be overridden at the batch job level using the
<code class="docutils literal notranslate"><span class="pre">maximizegpfs</span></code> argument to LSF’s <code class="docutils literal notranslate"><span class="pre">alloc_flags</span></code>. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#BSUB -alloc_flags maximizegpfs</span>
</pre></div>
</div>
<p>The maximizegpfs flag will allow GPFS tasks to utilize any core on the
compute node. This may be beneficial because it provides more resources
for GPFS service tasks, but it may also cause resource contention for
the jsrun compute job.</p>
</div>
</div>
<div class="section" id="resource-accounting">
<h3>Resource Accounting<a class="headerlink" href="#resource-accounting" title="Permalink to this headline">¶</a></h3>
<p>While logged into Summit, users can show their YTD usage and allocation
by project using the <code class="docutils literal notranslate"><span class="pre">showusage</span></code> command. System specific details can
be obtained with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ showusage -s summit

summit usage for the project&#39;s current allocation period:
                                  Project Totals          [USERID]
 Project      Allocation        Usage    Remaining          Usage
__________________________|____________________________|_____________
 [PROJID1]        50000   |      15728        34272    |         65
 [PROJID2]        20000   |       1234        18766    |          0
</pre></div>
</div>
<p>For additional details, please see the help message printed when using
the <code class="docutils literal notranslate"><span class="pre">-h</span></code> flag:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ showusage -h
</pre></div>
</div>
</div>
<div class="section" id="other-notes">
<h3>Other Notes<a class="headerlink" href="#other-notes" title="Permalink to this headline">¶</a></h3>
<p>Compute nodes are only allocated to one job at a time; they are not
shared. This is why users request nodes (instead of some other resource
such as cores or GPUs) in batch jobs and is why projects are charged
based on the number of nodes allocated multiplied by the amount of time
for which they were allocated. Thus, a job using only 1 core on each of
its nodes is charged the same as a job using every core and every GPU on
each of its nodes.</p>
</div>
</div>
<div class="section" id="debugging">
<span id="id18"></span><h2>Debugging<a class="headerlink" href="#debugging" title="Permalink to this headline">¶</a></h2>
<div class="section" id="arm-ddt">
<h3>Arm DDT<a class="headerlink" href="#arm-ddt" title="Permalink to this headline">¶</a></h3>
<p>Arm DDT is an advanced debugging tool used for scalar, multi-threaded,
and large-scale parallel applications. In addition to traditional
debugging features (setting breakpoints, stepping through code,
examining variables), DDT also supports attaching to already-running
processes and memory debugging. In-depth details of DDT can be found in
the <a class="reference external" href="https://www.allinea.com/user-guide/forge/userguide.html" target="_blank">Official DDT User
Guide</a>, and
instructions for how to use it on OLCF systems can be found on the
<a class="reference external" href="https://www.olcf.ornl.gov/software_package/forge/" target="_blank">Forge (DDT/MAP) Software Page</a>. DDT is the
OLCF’s recommended debugging software for large parallel applications.</p>
</div>
<div class="section" id="gdb">
<h3>GDB<a class="headerlink" href="#gdb" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://www.gnu.org/software/gdb/" target="_blank">GDB</a>, the GNU Project Debugger,
is a command-line debugger useful for traditional debugging and
investigating code crashes. GDB lets you debug programs written in Ada,
C, C++, Objective-C, Pascal (and many other languages). GDB is available
on Summit under all compiler families:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">gdb</span>
</pre></div>
</div>
<p>Additional information about GDB usage and OLCF-provided builds can be
found on the <a class="reference external" href="https://www.olcf.ornl.gov/software_package/gdb/" target="_blank">GDB Software Page</a>.</p>
</div>
<div class="section" id="valgrind">
<h3>Valgrind<a class="headerlink" href="#valgrind" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://valgrind.org" target="_blank">Valgrind</a> is an instrumentation framework for
building dynamic analysis tools. There are Valgrind tools that can
automatically detect many memory management and threading bugs, and
profile your programs in detail. You can also use Valgrind to build new
tools.</p>
<p>The Valgrind distribution currently includes five production-quality
tools: a memory error detector, a thread error detector, a cache and
branch-prediction profiler, a call-graph generating cache profiler,
and a heap profiler. It also includes two experimental tools: a data
race detector, and an instant memory leak detector.</p>
<p>The Valgrind tool suite provides a number of debugging and
profiling tools. The most popular is Memcheck, a memory checking tool
which can detect many common memory errors such as:</p>
<ul class="simple">
<li>Touching memory you shouldn’t (eg. overrunning heap block boundaries,
or reading/writing freed memory).</li>
<li>Using values before they have been initialized.</li>
<li>Incorrect freeing of memory, such as double-freeing heap blocks.</li>
<li>Memory leaks.</li>
</ul>
<p>Valgrind is available on Summit under all compiler families:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">valgrind</span>
</pre></div>
</div>
<p>Additional information about Valgrind usage and OLCF-provided builds can
be found on the <a class="reference external" href="https://www.olcf.ornl.gov/software_package/valgrind/" target="_blank">Valgrind Software
Page</a>.</p>
</div>
</div>
<div class="section" id="optimizing-and-profiling">
<span id="id19"></span><h2>Optimizing and Profiling<a class="headerlink" href="#optimizing-and-profiling" title="Permalink to this headline">¶</a></h2>
<div class="section" id="profiling-gpu-code-with-nvidia-developer-tools">
<h3>Profiling GPU Code with NVIDIA Developer Tools<a class="headerlink" href="#profiling-gpu-code-with-nvidia-developer-tools" title="Permalink to this headline">¶</a></h3>
<p>NVIDIA provides developer tools for profiling any code that runs on NVIDIA
GPUs. These are the <a class="reference external" href="https://developer.nvidia.com/tools-overview" target="_blank">Nsight suite of developer tools</a>: NVIDIA Nsight Systems for
collecting a timeline of your application, and NVIDIA Nsight Compute for
collecting detailed performance information about specific GPU kernels.</p>
<div class="section" id="nvidia-nsight-systems">
<h4>NVIDIA Nsight Systems<a class="headerlink" href="#nvidia-nsight-systems" title="Permalink to this headline">¶</a></h4>
<p>The first step to GPU profiling is collecting a timeline of your application.
(This operation is also sometimes called “tracing,” that is, finding
the start and stop timestamps of all activities that occurred on the GPU
or involved the GPU, such as copying data back and forth.) To do this, we
can collect a timeline using the command-line interface, <code class="docutils literal notranslate"><span class="pre">nsys</span></code>. To use
this tool, load the <code class="docutils literal notranslate"><span class="pre">nsight-systems</span></code> module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">module</span> <span class="n">load</span> <span class="n">nsight</span><span class="o">-</span><span class="n">systems</span>
</pre></div>
</div>
<p>For example, we can profile the <code class="docutils literal notranslate"><span class="pre">vectorAdd</span></code> CUDA sample (the CUDA samples
can be found in <code class="docutils literal notranslate"><span class="pre">$OLCF_CUDA_ROOT/samples</span></code> if the <code class="docutils literal notranslate"><span class="pre">cuda</span></code> module is loaded.)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nsys</span> <span class="n">profile</span> <span class="o">-</span><span class="n">o</span> <span class="n">vectorAdd</span> <span class="o">--</span><span class="n">stats</span><span class="o">=</span><span class="n">true</span> <span class="o">./</span><span class="n">vectorAdd</span>
</pre></div>
</div>
<p>(Note that even if you do not ask for Nsight Systems to create an output file,
but just ask it to print summary statistics with <code class="docutils literal notranslate"><span class="pre">--stats=true</span></code>, it will create
a temporary file for storing the profiling data, so you will need to work on a
file system that can be written to from a compute node such as GPFS.)</p>
<p>The profiler will print several sections including information about the
CUDA API calls made by the application, as well as any GPU kernels that were
launched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC,
OpenMP offload, and other programming models that target NVIDIA GPUs, because
under the hood they all ultimately take the same path for generating the binary
code that runs on the GPU.</p>
<p>If you add the <code class="docutils literal notranslate"><span class="pre">-o</span></code> option, as above, the report will be saved to file
with the extension <code class="docutils literal notranslate"><span class="pre">.qdrep</span></code>. That report file can later be analyzed in
the Nsight Systems UI by selecting File &gt; Open and locating the <code class="docutils literal notranslate"><span class="pre">vectorAdd.qdrep</span></code>
file on your filesystem. Nsight Systems does not currently have a Power9
version of the UI, so you will need to <a class="reference external" href="https://developer.nvidia.com/nsight-systems" target="_blank">download the UI for your local system</a>, which is supported on
Windows, Mac, and Linux (x86). Then use <code class="docutils literal notranslate"><span class="pre">scp</span></code> or some other file transfer
utility for copying the report file from Summit to your local machine.</p>
<p>Nsight Systems can be used for MPI runs with multiple ranks, but it is
not a parallel profiler and cannot combine output from multiple ranks.
Instead, each rank must be profiled and analyzed independently. The file
name should be unique for every rank. Nsight Systems knows how to parse
environment variables with the syntax <code class="docutils literal notranslate"><span class="pre">%q{ENV_VAR}</span></code>, and since Spectrum
MPI provides an environment variable for every process with its MPI rank,
you can do</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n6</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nsys</span> <span class="n">profile</span> <span class="o">-</span><span class="n">o</span> <span class="n">vectorAdd_</span><span class="o">%</span><span class="n">q</span><span class="p">{</span><span class="n">OMPI_COMM_WORLD_RANK</span><span class="p">}</span> <span class="o">./</span><span class="n">vectorAdd</span>
</pre></div>
</div>
<p>Then you will have <code class="docutils literal notranslate"><span class="pre">vectorAdd_0.qdrep</span></code> through <code class="docutils literal notranslate"><span class="pre">vectorAdd_5.qdrep</span></code>.
(Of course, in this case each rank does the same thing as this is not
an MPI application, but it works the same way for an MPI code.)</p>
<p>For more details about Nsight Systems, consult the <a class="reference external" href="https://developer.nvidia.com/nsight-systems" target="_blank">product page</a> and the <a class="reference external" href="https://docs.nvidia.com/nsight-systems/index.html" target="_blank">documentation</a>. If you previously
used <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> and would like to start using the Nsight Developer Tools,
check out <a class="reference external" href="https://devblogs.nvidia.com/migrating-nvidia-nsight-tools-nvvp-nvprof/" target="_blank">this transition guide</a>.
Also, in March 2020 NVIDIA presented a webinar on Nsight Systems which you
can <a class="reference external" href="https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-systems/" target="_blank">watch on demand</a>.</p>
</div>
<div class="section" id="nvidia-nsight-compute">
<h4>NVIDIA Nsight Compute<a class="headerlink" href="#nvidia-nsight-compute" title="Permalink to this headline">¶</a></h4>
<p>Individual GPU kernels (the discrete chunks of work that are launched by
programming languages such as CUDA and OpenACC) can be profiled in detail
with NVIDIA Nsight Compute. The typical workflow is to profile your code
with Nsight Systems and identify the major performance bottleneck in your
application. If that performance bottleneck is on the CPU, it means more
code should be ported to the GPU; or, if that bottleneck is in memory
management, such as copying data back and forth between the CPU and GPU,
you should look for opportunities to reduce that data motion. But if that
bottleneck is a GPU kernel, then Nsight Compute can be used to collect
performance counters to understand whether the kernel is running efficiently
and if there’s anything you can do to improve.</p>
<p>The Nsight Compute command-line interface, <code class="docutils literal notranslate"><span class="pre">nv-nsight-cu-cli</span></code>, can be
prefixed to your application to collect a report.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">module</span> <span class="n">load</span> <span class="n">nsight</span><span class="o">-</span><span class="n">compute</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nv</span><span class="o">-</span><span class="n">nsight</span><span class="o">-</span><span class="n">cu</span><span class="o">-</span><span class="n">cli</span> <span class="o">./</span><span class="n">vectorAdd</span>
</pre></div>
</div>
<p>Similar to Nsight Systems, Nsight Compute will create a temporary report file,
even when <code class="docutils literal notranslate"><span class="pre">-o</span></code> is not specified.</p>
<p>The most important output to look at is the “GPU Speed of Light” section,
which tells you what fraction of peak memory throughput and what fraction
of peak compute throughput you achieved. Typically if you have achieved
higher than 60% of the peak of either subsystem, your kernel would be
considered memory-bound or compute-bound (respectively), and if you have
not achieved 60% of either this is often a latency-bound kernel. (A common
cause of latency issues is not exposing enough parallelism to saturate
the GPU’s compute capacity – peak GPU performance can only be achieved when
there is enough work to hide the latency of memory accesses and to keep all
compute pipelines busy.)</p>
<p>By default, Nsight Compute will collect this performance data for every kernel
in your application. This will take a long time in a real-world application.
It is recommended that you identify a specific kernel to profile and then use
the <code class="docutils literal notranslate"><span class="pre">-k</span></code> argument to just profile that kernel. (If you don’t know the name of
your kernel, use <code class="docutils literal notranslate"><span class="pre">nsys</span></code> to obtain that. The flag will pattern match on any
substring of the kernel name.) You can also use the <code class="docutils literal notranslate"><span class="pre">-s</span></code> option to skip some
number of kernel calls and the <code class="docutils literal notranslate"><span class="pre">-c</span></code> option to specify how many invocations of
that kernel you want to profile.</p>
<p>If you want to collect information on just a specific performance measurement,
for example the number of bytes written to DRAM, you can do so with the
<code class="docutils literal notranslate"><span class="pre">--metrics</span></code> option:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nv</span><span class="o">-</span><span class="n">nsight</span><span class="o">-</span><span class="n">cu</span><span class="o">-</span><span class="n">cli</span> <span class="o">-</span><span class="n">k</span> <span class="n">vectorAdd</span> <span class="o">--</span><span class="n">metrics</span> <span class="n">dram__bytes_write</span><span class="o">.</span><span class="n">sum</span> <span class="o">./</span><span class="n">vectorAdd</span>
</pre></div>
</div>
<p>The list of available metrics can be obtained with <code class="docutils literal notranslate"><span class="pre">nv-nsight-cu-cli</span>
<span class="pre">--query-metrics</span></code>. Most metrics have both a base name and suffix. Together
these  make up the full metric name to pass to <code class="docutils literal notranslate"><span class="pre">nv-nsight-cu-cli</span></code>. To list
the full names for a collection of metrics, use <code class="docutils literal notranslate"><span class="pre">--query-metrics-mode</span> <span class="pre">suffix</span>
<span class="pre">--metrics</span> <span class="pre">&lt;metrics</span> <span class="pre">list&gt;</span></code>.</p>
<p>As with Nsight Systems, there is a graphical user interface you can load a
report file into (The GUI is only available for Windows, x86_64 Linux and Mac).
Use the <code class="docutils literal notranslate"><span class="pre">-o</span></code> flag to create a file (the added report extension will be
<code class="docutils literal notranslate"><span class="pre">.nsight-cuprof-report</span></code>), copy it to your local system, and use the File &gt;
Open File menu item. If you are using multiple MPI ranks, make sure you name
each one independently. Nsight Compute does not yet support the <code class="docutils literal notranslate"><span class="pre">%q</span></code> syntax
(this will come in a future release), so your job script will have to do the
naming manually; for example, you can create a simple shell script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat run.sh
#!/bin/bash

nv-nsight-cu-cli -o vectorAdd_$OMPI_COMM_WORLD_RANK ./vectorAdd
</pre></div>
</div>
<p>For more details on Nsight Compute, check out the <a class="reference external" href="https://developer.nvidia.com/nsight-compute" target="_blank">product page</a> and the <a class="reference external" href="https://docs.nvidia.com/nsight-compute/index.html" target="_blank">documentation</a>. If you previously used
<code class="docutils literal notranslate"><span class="pre">nvprof</span></code> and would like to start using Nsight Compute, check out <a class="reference external" href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-guide" target="_blank">this transition
guide</a>.
Also, in March 2020 NVIDIA presented a webinar on Nsight Compute which you can <a class="reference external" href="https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-compute/" target="_blank">watch on
demand</a>.</p>
</div>
<div class="section" id="nvprof-and-nvvp">
<h4>nvprof and nvvp<a class="headerlink" href="#nvprof-and-nvvp" title="Permalink to this headline">¶</a></h4>
<p>Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling
tool was <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>, which provides both tracing and kernel profiling
capabilities. Like with Nsight Systems and Nsight Compute, the profiler data
output can be saved and imported into the NVIDIA Visual Profiler for additional
graphical analysis. <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> is in maintenance mode now: it still works on
Summit and significant bugs will be fixed, but no new feature development is
occurring on this tool.</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>, the <code class="docutils literal notranslate"><span class="pre">cuda</span></code> module must be loaded.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">module</span> <span class="n">load</span> <span class="n">cuda</span>
</pre></div>
</div>
<p>A simple “Hello, World!” run using <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> can be done by adding
“nvprof” to the jsrun (see: <a class="reference internal" href="#job-launcher-jsrun"><span class="std std-ref">Job Launcher (jsrun)</span></a>)
line in your batch script (see <a class="reference internal" href="#batch-scripts"><span class="std std-ref">Batch Scripts</span></a>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nvprof</span> <span class="o">./</span><span class="n">hello_world_gpu</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Although <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> doesn’t provide aggregated MPI data, the <code class="docutils literal notranslate"><span class="pre">%h</span></code> and
<code class="docutils literal notranslate"><span class="pre">%p</span></code> output file modifiers can be used to create separate output files
for each host and process.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nvprof</span> <span class="o">-</span><span class="n">o</span> <span class="n">output</span><span class="o">.%</span><span class="n">h</span><span class="o">.%</span><span class="n">p</span> <span class="o">./</span><span class="n">hello_world_gpu</span>
<span class="o">...</span>
</pre></div>
</div>
<p>There are many various metrics and events that the profiler can capture.
For example, to output the number of double-precision FLOPS, you may use
the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">jsrun</span> <span class="o">-</span><span class="n">n1</span> <span class="o">-</span><span class="n">a1</span> <span class="o">-</span><span class="n">g1</span> <span class="n">nvprof</span> <span class="o">--</span><span class="n">metrics</span> <span class="n">flops_dp</span> <span class="o">-</span><span class="n">o</span> <span class="n">output</span><span class="o">.%</span><span class="n">h</span><span class="o">.%</span><span class="n">p</span> <span class="o">./</span><span class="n">hello_world_gpu</span>
<span class="o">...</span>
</pre></div>
</div>
<p>To see a list of all available metrics and events, use the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">summit</span><span class="o">&gt;</span> <span class="n">nvprof</span> <span class="o">--</span><span class="n">query</span><span class="o">-</span><span class="n">metrics</span>
<span class="n">summit</span><span class="o">&gt;</span> <span class="n">nvprof</span> <span class="o">--</span><span class="n">query</span><span class="o">-</span><span class="n">events</span>
</pre></div>
</div>
<p>While using <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> on the command-line is a quick way to gain
insight into your CUDA application, a full visual profile is often even
more useful. For information on how to view the output of <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> in
the NVIDIA Visual Profiler, see the <a class="reference external" href="http://docs.nvidia.com/cuda/profiler-users-guide/#nvprof-overview" target="_blank">NVIDIA
Documentation</a>.</p>
</div>
</div>
<div class="section" id="score-p">
<h3>Score-P<a class="headerlink" href="#score-p" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="http://score-p.org/" target="_blank">Score-P</a> measurement infrastructure is a
highly scalable and easy-to-use tool suite for profiling, event
tracing, and online analysis of HPC applications. Score-P supports
analyzing C, C++ and Fortran applications that make use of
multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and
accelerators (CUDA, OpenCL, OpenACC) and combinations.</p>
<p>For detailed information about using Score-P on Summit and the
builds available, please see the
<a class="reference external" href="https://www.olcf.ornl.gov/software_package/score-p/" target="_blank">Score-P Software Page.</a></p>
</div>
<div class="section" id="vampir">
<h3>Vampir<a class="headerlink" href="#vampir" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://vampir.eu/" target="_blank">Vampir</a> is a software performance visualizer focused on highly
parallel applications. It presents a unified view on an application
run including the use of programming paradigms like MPI, OpenMP,
PThreads, CUDA, OpenCL and OpenACC. It also incorporates file I/O,
hardware performance counters and other performance data sources.
Various interactive displays offer detailed insight into the performance
behavior of the analyzed application. Vampir’s scalable analysis
server and visualization engine enable interactive navigation of
large amounts of performance data. <a class="reference external" href="https://olcf.ornl.gov/software_package/score-p" target="_blank">Score-P</a>
and <a class="reference external" href="https://www.olcf.ornl.gov/software_package/tau" target="_blank">TAU</a> generate OTF2
trace files for Vampir to visualize.</p>
<p>For detailed information about using Vampir on Summit and the builds available,
please see the <a class="reference external" href="https://www.olcf.ornl.gov/software_package/vampir/" target="_blank">Vampir Software Page</a>.</p>
</div>
</div>
<div class="section" id="nvidia-tesla-v100">
<span id="nvidia-v100-gpus"></span><span id="id20"></span><h2>NVIDIA V100 GPUs<a class="headerlink" href="#nvidia-tesla-v100" title="Permalink to this headline">¶</a></h2>
<p>The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s
(double-precision) and contributes to a majority of the computational
work performed on Summit. Each V100 contains 80 streaming
multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth
memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The
GigaThread Engine is responsible for distributing work among the SMs and
(8) 512-bit memory controllers control access to the 16 GB (32 GB on
high-memory nodes) of HBM2 memory. The V100 uses NVIDIA’s NVLink interconnect
to pass data between GPUs as well as from CPU-to-GPU.</p>
<img alt="../_images/GV100_FullChip_Diagram_FINAL2_a.png" class="align-center" src="../_images/GV100_FullChip_Diagram_FINAL2_a.png" />
<div class="section" id="nvidia-v100-sm">
<h3>NVIDIA V100 SM<a class="headerlink" href="#nvidia-v100-sm" title="Permalink to this headline">¶</a></h3>
<p>Each SM on the V100 contains 32 FP64 (double-precision) cores, 64 FP32
(single-precision) cores, 64 INT32 cores, and 8 tensor cores. A 128-KB
combined memory block for shared memory and L1 cache can be configured
to allow up to 96 KB of shared memory. In addition, each SM has 4
texture units which use the (configured size of the) L1 cache.</p>
<img alt="../_images/GV100_SM_Diagram-FINAL2.png" class="align-center" src="../_images/GV100_SM_Diagram-FINAL2.png" />
</div>
<div class="section" id="hbm2">
<h3>HBM2<a class="headerlink" href="#hbm2" title="Permalink to this headline">¶</a></h3>
<p>Each V100 has access to 16 GB (32GB for high-memory nodes) of
high-bandwidth memory (HBM2), which can be accessed at speeds of
up to 900 GB/s. Access to this memory is controlled by (8) 512-bit
memory controllers, and all accesses to the high-bandwidth memory
go through the 6 MB L2 cache.</p>
</div>
<div class="section" id="nvidia-nvlink">
<h3>NVIDIA NVLink<a class="headerlink" href="#nvidia-nvlink" title="Permalink to this headline">¶</a></h3>
<p>The processors within a node are connected by NVIDIA’s NVLink
interconnect. Each link has a peak bandwidth of 25 GB/s (in each
direction), and since there are 2 links between processors, data can be
transferred from GPU-to-GPU and CPU-to-GPU at a peak rate of 50 GB/s.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The 50-GB/s peak bandwidth stated above is for data transfers
in a single direction. However, this bandwidth can be achieved in both
directions simultaneously, giving a peak “bi-directional” bandwidth of
100 GB/s between processors.</p>
</div>
<p>The figure below shows a schematic of the NVLink connections between the
CPU and GPUs on a single socket of a Summit node.</p>
<img alt="../_images/NVLink2.png" class="align-center" src="../_images/NVLink2.png" />
</div>
<div class="section" id="volta-multi-process-service">
<h3>Volta Multi-Process Service<a class="headerlink" href="#volta-multi-process-service" title="Permalink to this headline">¶</a></h3>
<p>When a CUDA program begins, each MPI rank creates a separate CUDA
context on the GPU, but the scheduler on the GPU only allows one CUDA
context (and so one MPI rank) at a time to launch on the GPU. This means
that multiple MPI ranks can share access to the same GPU, but each rank
gets exclusive access while the other ranks wait (time-slicing). This
can cause the GPU to become underutilized if a rank (that has exclusive
access) does not perform enough work to saturate the resources of the
GPU. The following figure depicts such time-sliced access to a pre-Volta
GPU.</p>
<img alt="../_images/nv_mps_1.png" class="align-center" src="../_images/nv_mps_1.png" />
<p>The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to
<em>concurrently</em> share the resources on a single GPU. This is accomplished by
starting an MPS server process, which funnels the work from multiple CUDA
contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some
cases, this can increase performance due to better utilization of the resources.
The figure below illustrates MPS on a pre-Volta GPU.</p>
<a class="reference internal image-reference" href="../_images/nv_mps_2.png"><img alt="../_images/nv_mps_2.png" class="align-center" src="../_images/nv_mps_2.png" style="width: 65.0%;" /></a>
<p>Volta GPUs improve MPS with new capabilities. For instance, each Volta
MPS client (MPI rank) is assigned a “subcontext” that has its own GPU
address space, instead of sharing the address space with other clients.
This isolation helps protect MPI ranks from out-of-range reads/writes
performed by other ranks within CUDA kernels. Because each subcontext
manages its own GPU resources, it can submit work directly to the GPU
without the need to first pass through the MPS server. In addition,
Volta GPUs support up to 48 MPS clients (up from 16 MPS clients on
Pascal).</p>
<a class="reference internal image-reference" href="../_images/nv_mps_3.png"><img alt="../_images/nv_mps_3.png" class="align-center" src="../_images/nv_mps_3.png" style="width: 65.0%;" /></a>
<p>For more information, please see the following document from NVIDIA:
<a class="reference external" href="https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf" target="_blank">https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf</a></p>
</div>
<div class="section" id="unified-memory">
<h3>Unified Memory<a class="headerlink" href="#unified-memory" title="Permalink to this headline">¶</a></h3>
<p>Unified memory is a single virtual address space that is accessible to
any processor in a system (within a node). This means that programmers
only need to allocate a single unified-memory pointer (e.g. using
cudaMallocManaged) that can be accessed by both the CPU and GPU, instead
of requiring separate allocations for each processor. This “managed
memory” is automatically migrated to the accessing processor, which
eliminates the need for explicit data transfers.</p>
<a class="reference internal image-reference" href="../_images/nv_um_1.png"><img alt="../_images/nv_um_1.png" class="align-center" src="../_images/nv_um_1.png" style="width: 60.0%;" /></a>
<p>On Pascal-generation GPUs and later, this automatic migration is
enhanced with hardware support. A page migration engine enables GPU page
faulting, which allows the desired pages to be migrated to the GPU “on
demand” instead of the entire “managed” allocation. In addition, 49-bit
virtual addressing allows programs using unified memory to access the
full system memory size. The combination of GPU page faulting and larger
virtual addressing allows programs to oversubscribe the system memory,
so very large data sets can be processed. In addition, new CUDA API
functions introduced in CUDA8 allow users to fine tune the use of
unified memory.</p>
<p>Unified memory is further improved on Volta GPUs through
the use of access counters that can be used to automatically tune
unified memory by determining where a page is most often accessed.</p>
<p>For more information, please see the following section of NVIDIA’s
CUDA Programming Guide:
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd" target="_blank">http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd</a></p>
</div>
<div class="section" id="independent-thread-scheduling">
<h3>Independent Thread Scheduling<a class="headerlink" href="#independent-thread-scheduling" title="Permalink to this headline">¶</a></h3>
<p>The V100 supports independent thread scheduling, which allows threads to
synchronize and cooperate at sub-warp scales. Pre-Volta GPUs implemented
warps (groups of 32 threads which execute instructions in
single-instruction, multiple thread - SIMT - mode) with a single call
stack and program counter for a warp as a whole.</p>
<img alt="../_images/nv_ind_threads_1.png" class="align-center" src="../_images/nv_ind_threads_1.png" />
<p>Within a warp, a mask is used to specify which threads are currently
active when divergent branches of code are encountered. The (active)
threads within each branch execute their statements serially before
threads in the next branch execute theirs. This means that programs on
pre-Volta GPUs should avoid sub-warp synchronization; a sync point in
the branches could cause a deadlock if all threads in a warp do not
reach the synchronization point.</p>
<img alt="../_images/nv_ind_threads_2.png" class="align-center" src="../_images/nv_ind_threads_2.png" />
<p>The Tesla V100 introduces warp-level synchronization by implementing warps with
a program counter and call stack for each individual thread (i.e.  independent
thread scheduling).</p>
<img alt="../_images/nv_ind_threads_3.png" class="align-center" src="../_images/nv_ind_threads_3.png" />
<p>This implementation allows threads to diverge and synchronize at the sub-warp
level using the __syncwarp() function. The independent thread scheduling
enables the thread scheduler to stall execution of any thread, allowing other
threads in the warp to execute different statements. This means that threads in
one branch can stall at a sync point and wait for the threads in the other
branch to reach their sync point.</p>
<img alt="../_images/nv_ind_threads_4.png" class="align-center" src="../_images/nv_ind_threads_4.png" />
<p>For more information, please see the following section of NVIDIA’s CUDA
Programming Guide:
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x" target="_blank">http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x</a></p>
</div>
<div class="section" id="tensor-cores">
<h3>Tensor Cores<a class="headerlink" href="#tensor-cores" title="Permalink to this headline">¶</a></h3>
<p>The Tesla V100 contains 640 tensor cores (8 per SM) intended to enable
faster training of large neural networks. Each tensor core performs a
<code class="docutils literal notranslate"><span class="pre">D</span> <span class="pre">=</span> <span class="pre">AB</span> <span class="pre">+</span> <span class="pre">C</span></code> operation on 4x4 matrices. A and B are FP16 matrices,
while C and D can be either FP16 or FP32:</p>
<a class="reference internal image-reference" href="../_images/nv_tensor_core_1.png"><img alt="../_images/nv_tensor_core_1.png" class="align-center" src="../_images/nv_tensor_core_1.png" style="width: 85.0%;" /></a>
<p>Each of the 16 elements that result from the AB matrix multiplication
come from 4 floating-point fused-multiply-add (FMA) operations
(basically a dot product between a row of A and a column of B). Each
FP16 multiply yields a full-precision product which is accumulated in a
FP32 result:</p>
<a class="reference internal image-reference" href="../_images/nv_tc_1.png"><img alt="../_images/nv_tc_1.png" class="align-center" src="../_images/nv_tc_1.png" style="width: 85.0%;" /></a>
<p>Each tensor core performs 64 of these FMA operations per clock. The 4x4
matrix operations outlined here can be combined to perform matrix
operations on larger (and higher dimensional) matrices.</p>
<div class="section" id="using-the-tensor-cores-on-summit">
<h4>Using the Tensor Cores on Summit<a class="headerlink" href="#using-the-tensor-cores-on-summit" title="Permalink to this headline">¶</a></h4>
<p>The NVIDIA Tesla V100 GPUs in Summit are capable of over 7TF/s of
double-precision and 15 TF/s of single-precision floating point performance.
Additionally, the V100 is capable of over 120 TF/s of half-precision floating
point performance when using its Tensor Core feature. The Tensor Cores are
purpose-built accelerators for half-precision matrix multiplication operations.
While they were designed especially to accelerate machine learning workflows,
they are exposed through several other APIs that are useful to other HPC
applications. This section provides information for using the V100 Tensor
Cores.</p>
<p>The V100 Tensor Cores perform a warp-synchronous multiply and accumulate of
16-bit matrices in the form of D = A * B + C. The operands of this matrix
multiplication are 16-bit A and B matrices, while the C and D accumulation
matrices may be 16 or 32-bit matrices with comparable performance for either
precision.</p>
<a class="reference internal image-reference" href="../_images/nv_tc_2.png"><img alt="../_images/nv_tc_2.png" class="align-center" src="../_images/nv_tc_2.png" style="width: 85.0%;" /></a>
<p>Half precision floating point representation has a dramatically lower range of
numbers than Double or Single precision. Half precision representation consists
of 1 sign bit, a 5-bit exponent, and a 10-bit mantissa. This results in a
dynamic range of 5.96e-8 to 65,504</p>
<div class="section" id="tensor-core-programming-models">
<h5>Tensor Core Programming Models<a class="headerlink" href="#tensor-core-programming-models" title="Permalink to this headline">¶</a></h5>
<p>This section details a variety of high and low-level Tensor Core programming
models. Which programming model is appropriate to a given application is highly
situational, so this document will present multiple programming models to allow
the reader to evaluate each for their merits within the needs of the
application.</p>
<div class="section" id="cublas-library">
<h6>cuBLAS Library<a class="headerlink" href="#cublas-library" title="Permalink to this headline">¶</a></h6>
<p>cuBLAS is NVIDIA’s implementation of the Basic Linear Algebra Subroutines
library for GPUs. It contains not only the Level 1, 2, and 3 BLAS routines, but
several extensions to these routines that add important capabilities to the
library, such as the ability to batch operations and work with varying
precisions.</p>
<p>The cuBLAS libraries provides access to the TensorCores using 3 different
routines, depending on the application needs. The <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemm" target="_blank">cublasHgemm</a> routine
performs a general matrix multiplication of half-precision matrices. The
numerical operands to this routine must be of type half and math mode must be
set to CUBLAS_TENSOR_OP_MATH to enable Tensor Core use. Additionally, if the
<a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemm" target="_blank">cublasSgemm</a> routine
will down-convert from single precision to half precision when the math mode is
set to CUBLAS_TENSOR_OP_MATH, enabling simple conversion from SGEMM to HGEMM
using Tensor Cores. For either of these two methods the <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublassetmathmode" target="_blank">cublasSetMathMode</a> function
must be used to change from CUBLAS_DEFAULT_MATH to CUBLAS_TENSOR_OP_MATH mode.</p>
<p>cuBLAS provides a non-standard extension of GEMM with the <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-GemmEx" target="_blank">cublasGemmEx</a> routine, which
provides additional flexibility about the data types of the operands. In
particular, the A, B, and C matrices can be of arbitrary and different types,
with the types of each declared using the Atype, Btype, and Ctype parameters.
The algo parameter works similar to the math mode above. If the math mode is
set to CUBLAS_TESNOR_OP_MATH and the algo parameter is set to
CUBLAS_GEMM_DEFAULT, then the Tensor Cores will be used. If algo is
CUBLAS_GEMM_DEFAULT_TENSOR_OP or CUBLAS_GEMM_ALGO{0-15}_TENSOR_OP, then the
Tensor Cores will be used regardless of the math setting. The table below
outlines the rules stated in the past two paragraphs.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="44%" />
<col width="27%" />
<col width="29%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">&#160;</th>
<th class="head"><code class="docutils literal notranslate"><span class="pre">mathMode</span> <span class="pre">=</span> <span class="pre">CUBLAS_DEFAULT_MATH</span></code></th>
<th class="head"><code class="docutils literal notranslate"><span class="pre">mathMode</span> <span class="pre">=</span> <span class="pre">CUBLAS_TENSOR_OP_MATH</span></code></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">cublasHgemm,</span> <span class="pre">cublasSgemm,</span> <span class="pre">cublasGemmEx(algo=DEFAULT)</span></code></td>
<td>Disallowed</td>
<td>Allowed</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">cublasGemmEx(algo=*_TENSOR_OP)</span></code></td>
<td>Allowed</td>
<td>Allowed</td>
</tr>
</tbody>
</table>
<p>When using any of these methods to access the Tensor Cores, the M, N, K, LDA,
LDB, LDC, and A, B, and C pointers must all be aligned to 8 bytes due to the
high bandwidth necessary to utilize the Tensor Cores effective.</p>
<p>Many of the routines listed above are also available in batched form, see the
<a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html" target="_blank">cuBLAS documentation</a> for
more information. Advanced users wishing to have increased control over the
specifics of data layout, type, and underlying algorithms may wish to use the
more advanced <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#using-the-cublasLt-api" target="_blank">cuBLAS-Lt interface</a>. This
interface uses the same underlying GPU kernels, but provides developers with a
higher degree of control.</p>
</div>
<div class="section" id="iterative-refinement-of-linear-solvers">
<h6>Iterative Refinement of Linear Solvers<a class="headerlink" href="#iterative-refinement-of-linear-solvers" title="Permalink to this headline">¶</a></h6>
<p>Iterative Refinement is a technique for performing linear algebra solvers in a
reduced precision, then iterating to improve the results and return them to
full precision. This technique has been used for several years to use 32-bit
math operations and achieve 64-bit results, which often results in a speed-up
due to single precision math often have a 2X performance advantage on modern
CPUs and many GPUs. NVIDIA and the University of Tennessee have been working to
extend this technique to perform operations in half-precision and obtain higher
precision results. One such place where this technique has been applied is in
calculating an LU factorization of the linear system Ax = B. This operation is
dominated by a matrix multiplication operation, which is illustrated in green
in the image below. It is possible to perform the GEMM operations at a reduced
precision, while leaving the panel and trailing matrices in a higher precision.
This technique allows for the majority of the math operations to be done at the
higher FP16 throughput. The matrix used in the GEMM is generally not square,
which is often the best performing GEMM operation, but is referred to as rank-k
and generally still very fast when using matrix multiplication libraries.</p>
<a class="reference internal image-reference" href="../_images/nv_tc_3.png"><img alt="../_images/nv_tc_3.png" class="align-center" src="../_images/nv_tc_3.png" style="width: 85.0%;" /></a>
<p>A summary of the algorithm used for calculating in mixed precision is in the
following image.</p>
<a class="reference internal image-reference" href="../_images/nv_tc_4.png"><img alt="../_images/nv_tc_4.png" class="align-center" src="../_images/nv_tc_4.png" style="width: 85.0%;" /></a>
<p>We see in the graph below that it is possible to achieved a 3-4X performance
improvement over the double-precision solver, while achieving the same level of
accuracy. It has also been observed that the use of Tensor Cores makes the
problem more likely to converge than strict half-precision GEMMs due to the
ability to accumulate into 32-bit results.</p>
<a class="reference internal image-reference" href="../_images/nv_tc_5.png"><img alt="../_images/nv_tc_5.png" class="align-center" src="../_images/nv_tc_5.png" style="width: 85.0%;" /></a>
<p>NVIDIA will be shipping official support for IR solvers in their cuSOLVER
library in the latter half of 2019. The image below provides estimated release
dates, which are subject to change.</p>
<a class="reference internal image-reference" href="../_images/nv_tc_6.png"><img alt="../_images/nv_tc_6.png" class="align-center" src="../_images/nv_tc_6.png" style="width: 85.0%;" /></a>
</div>
<div class="section" id="automatic-mixed-precision-amp-in-machine-learning-frameworks">
<h6>Automatic Mixed Precision (AMP) in Machine Learning Frameworks<a class="headerlink" href="#automatic-mixed-precision-amp-in-machine-learning-frameworks" title="Permalink to this headline">¶</a></h6>
<p>NVIDIA has a Training With Mixed Precision guide available for developers
wishing to explicitly use mixed precision and Tensor Cores in their training of
neural networks. This is a good place to start when investigating Tensor Cores
for machine learning applications. Developers should specifically read the
Optimizing For Tensor Cores section.</p>
<p>NVIDIA has also integrated a technology called Automatic Mixed Precision (AMP)
into several common frameworks, TensorFlow, PyTorch, and MXNet at time of
writing. In most cases AMP can be enabled via a small code change or via
setting and environment variable. AMP does not strictly replace all matrix
multiplication operations with half precision, but uses graph optimization
techniques to determine whether a given layer is best run in full or half
precision.</p>
<p>Examples are provided for using AMP, but the following sections summarize the
usage in the three supported frameworks.</p>
<div class="section" id="tensorflow">
<h7>TensorFlow<a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h7>
<p>With TensorFlow AMP can be enabled using one of the following techniques.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_ENABLE_AUTO_MIXED_PRECISION&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
</pre></div>
</div>
<p>OR</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">TF_ENABLE_AUTO_MIXED_PRECISION</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>Explicit optimizer wrapper available in NVIDIA Container 19.07+, TF 1.14+, TF
2.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">enable_mixed_precision_graph_rewrite</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch">
<h7>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h7>
<p>Adding the following to a PyTorch model will enable AMP:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="s2">&quot;O1&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
  <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="mxnet">
<h7>MXNet<a class="headerlink" href="#mxnet" title="Permalink to this headline">¶</a></h7>
<p>The code below will enable AMP for MXNet:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">amp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">amp</span><span class="o">.</span><span class="n">init_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
<span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
  <span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">scaled_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="wmma">
<h6>WMMA<a class="headerlink" href="#wmma" title="Permalink to this headline">¶</a></h6>
<p>The Warp Matrix Multiply and Accumulate (WMMA) API was introduced in CUDA 9
explicitly for programming the Tesla V100 Tensor Cores. This is a low-level API
that supports loading matrix data into fragments within the threads of a warp,
applying a Tensor Core multiplication on that data, and then restoring it to
the main GPU memory. This API is called within CUDA kernels and all WMMA
operations are warp-synchronous, meaning the threads in a warp will leave the
operation synchronously. Examples are available for using the WMMA instructions
in C++ and CUDA Fortran. The image below demonstrates the general pattern for
WMMA usage.</p>
<a class="reference internal image-reference" href="../_images/nv_tc_7.png"><img alt="../_images/nv_tc_7.png" class="align-center" src="../_images/nv_tc_7.png" style="width: 85.0%;" /></a>
<p>The example above performs a 16-bit accumulate operation, but 32-bit is also
supported. Please see the provided samples and the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma" target="_blank">WMMA documentation</a> for
more details.</p>
<p>CUDA 10 introduced a lower-level alternative to WMMA with the mma.sync()
instruction. This is a very low-level instruction that requires the programmer
handle the data movement provided by WMMA explicitly, but is capable of higher
performance. Details of <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-wmma-mma" target="_blank">mma.sync</a>
can be found in the PTX documentation and examples for using this feature via
CUTLASS cane be found in the second half of this <a class="reference external" href="https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s9593-cutensor%3a+high-performance+tensor+operations+in+cuda" target="_blank">GTC presentation</a>.</p>
</div>
<div class="section" id="cutlass">
<h6>CUTLASS<a class="headerlink" href="#cutlass" title="Permalink to this headline">¶</a></h6>
<p><a class="reference external" href="https://github.com/nvidia/cutlass/" target="_blank">CUTLASS</a> is an open-source library
provided by NVIDIA for building matrix multiplication operations using C++
templates. The goal is to provide performance that is nearly as good as the
hand-tuned cuBLAS library, but in a more expressive, composible manner.</p>
<p>The CUTLASS library provides a variety of primitives that are optimized for
proper data layout and movement to achieve the maximum possible performance of
a matrix multiplation on an NVIDIA GPU. These include iterators for blocking,
loading, and storing matrix tiles, plus optimized classes for transforming the
data and performing the actual multiplication. CUTLASS provides <a class="reference external" href="https://github.com/NVIDIA/cutlass/blob/master/CUTLASS.md" target="_blank">extensive
documentation</a> of
these features and examples have been provided. Interested developers are also
encouraged to watch the <a class="reference external" href="https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s8854-cutlass%3a+software+primitives+for+dense+linear+algebra+at+all+levels+and+scales+within+cuda" target="_blank">CUTLASS introduction video</a>
from GTC2018.</p>
</div>
</div>
<div class="section" id="measuring-tensor-core-utilization">
<h5>Measuring Tensor Core Utilization<a class="headerlink" href="#measuring-tensor-core-utilization" title="Permalink to this headline">¶</a></h5>
<p>When attempting to use Tensor Cores it is useful to measure and confirm that
the Tensor Cores are being used within your code. For implicit use via a
library like cuBLAS, the Tensor Cores will only be used above a certain
threshold, so Tensor Core use should not be assumed. The NVIDIA Tools provide a
performance metric to measure Tensor Core utilization on a scale from 0 (Idle)
to 10 (Max) utilization.</p>
<p>When using NVIDIA’s nvprof profiler, one should add the <cite>-m
tensor_precision_fu_utilization</cite> option to measure Tensor Core utilization.
Below is the output from measuring this metric on one of the example programs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ nvprof -m tensor_precision_fu_utilization ./simpleCUBLAS
==43727== NVPROF is profiling process 43727, command: ./simpleCUBLAS
GPU Device 0: &quot;Tesla V100-SXM2-16GB&quot; with compute capability 7.0

simpleCUBLAS test running..
simpleCUBLAS test passed.
==43727== Profiling application: ./simpleCUBLAS
==43727== Profiling result:
==43727== Metric result:
Invocations                               Metric Name                           Metric Description         Min         Max         Avg
Device &quot;Tesla V100-SXM2-16GB (0)&quot;
    Kernel: volta_h884gemm_128x64_ldg8_nn
          1           tensor_precision_fu_utilization   Tensor-Precision Function Unit Utilization     Low (3)     Low (3)     Low (3)
</pre></div>
</div>
<p>NVIDIA’s Nsight Compute may also be used to measure tensor core utilization via
the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric, as
follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ nv-nsight-cu-cli --metrics sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active ./cudaTensorCoreGemm

[  compute_gemm, 2019-Aug-08 12:48:39, Context 1, Stream 7
      Section: Command line profiler metrics
      ----------------------------------------------------------------------
      sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active                    %                       43.44
      ----------------------------------------------------------------------
</pre></div>
</div>
</div>
<div class="section" id="when-to-try-tensor-cores">
<h5>When to Try Tensor Cores<a class="headerlink" href="#when-to-try-tensor-cores" title="Permalink to this headline">¶</a></h5>
<p>Tensor Cores provide the potential for an enormous performance boost over
full-precision operations, but when their use is appropriate is highly
application and even problem independent. Iterative Refinement techniques can
suffer from slow or possible a complete lack of convergence if the condition
number of the matrix is very large. By using Tensor Cores, which support 32-bit
accumulation, rather than strict 16-bit math operations, iterative refinement
becomes a viable option in a much larger number of cases, so it should be
attempted when an application is already using a supported solver.</p>
<p>Even if iterative techniques are not available for an application, direct use
of Tensor Cores may be beneficial if at least the A and B matrices can be
constructed from the input data without significant loss of precision. Since
the C and D matrices may be 32-bit, the output may have a higher degree of
precision than the input. It may be possible to try these operations
automatically by setting the math mode in cuBLAS, as detailed above, to
determine whether the loss of precision is an acceptable trade-off for
increased performance in a given application. If it is, the cublasGemmEx API
allows the programmer to control when the conversion to 16-bit occurs, which
may result in higher throughput than allowing the cuBLAS library to do the
conversion at call time.</p>
<p>Some non-traditional uses of Tensor Cores can come from places where integers
that fall within the FP16 range are used in an application. For instance, in
“Attacking the Opioid Epidemic: Determining the Epistatic and Pleiotropic
Genetic Architectures for Chronic Pain and Opioid Addiction,” a 2018 Gordon
Bell Prize-winning paper, the authors used Tensor Cores in place of small
integers, allowing them very high performance over performing the same
calculation in integer space. This technique is certainly not applicable to all
applications, but does show that Tensor Cores may be used in algorithms that
might not have been represented by a floating point matrix multiplication
otherwise.</p>
<p>Lastly, when performing the training step of a deep learning application it is
often beneficial to do at least some of the layer calculations in reduced
precision. The AMP technique described above can be tried with little to know
code changes, making it highly advisable to attempt in any machine learning
application.</p>
</div>
<div class="section" id="tensor-core-examples-and-other-materials">
<h5>Tensor Core Examples and Other Materials<a class="headerlink" href="#tensor-core-examples-and-other-materials" title="Permalink to this headline">¶</a></h5>
<p>NVIDIA has provided several example codes for using Tensor Cores from a variety
of the APIs listed above. These examples can be found on <a class="reference external" href="https://github.com/olcf/NVIDIA-tensor-core-examples" target="_blank">GitHub</a>.</p>
<p>NVIDIA Tensor Core Workshop (August 2018): <a class="reference external" href="https://www.olcf.ornl.gov/wp-content/uploads/2019/11/ORNL_Tensor_Core_Training_Aug2019.pdf" target="_blank">slides</a>,
recording (coming soon)</p>
</div>
</div>
</div>
<div class="section" id="tesla-v100-specifications">
<h3>Tesla V100 Specifications<a class="headerlink" href="#tesla-v100-specifications" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils align-default">
<colgroup>
<col width="65%" />
<col width="35%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Compute Capability</td>
<td>7.0</td>
</tr>
<tr class="row-even"><td>Peak double precision floating point performance</td>
<td>7.8 TFLOP/s</td>
</tr>
<tr class="row-odd"><td>Peak single precision floating point performance</td>
<td>15.7 TFLOP/s</td>
</tr>
<tr class="row-even"><td>Single precision CUDA cores</td>
<td>5120</td>
</tr>
<tr class="row-odd"><td>Double precision CUDA cores</td>
<td>2560</td>
</tr>
<tr class="row-even"><td>Tensor cores</td>
<td>640</td>
</tr>
<tr class="row-odd"><td>Clock frequency</td>
<td>1530 MHz</td>
</tr>
<tr class="row-even"><td>Memory Bandwidth</td>
<td>900 GB/s</td>
</tr>
<tr class="row-odd"><td>Memory size (HBM2)</td>
<td>16 or 32 GB</td>
</tr>
<tr class="row-even"><td>L2 cache</td>
<td>6 MB</td>
</tr>
<tr class="row-odd"><td>Shared memory size / SM</td>
<td>Configurable up to 96 KB</td>
</tr>
<tr class="row-even"><td>Constant memory</td>
<td>64 KB</td>
</tr>
<tr class="row-odd"><td>Register File Size</td>
<td>256 KB (per SM)</td>
</tr>
<tr class="row-even"><td>32-bit Registers</td>
<td>65536 (per SM)</td>
</tr>
<tr class="row-odd"><td>Max registers per thread</td>
<td>255</td>
</tr>
<tr class="row-even"><td>Number of multiprocessors (SMs)</td>
<td>80</td>
</tr>
<tr class="row-odd"><td>Warp size</td>
<td>32 threads</td>
</tr>
<tr class="row-even"><td>Maximum resident warps per SM</td>
<td>64</td>
</tr>
<tr class="row-odd"><td>Maximum resident blocks per SM</td>
<td>32</td>
</tr>
<tr class="row-even"><td>Maximum resident threads per SM</td>
<td>2048</td>
</tr>
<tr class="row-odd"><td>Maximum threads per block</td>
<td>1024</td>
</tr>
<tr class="row-even"><td>Maximum block dimensions</td>
<td>1024, 1024, 64</td>
</tr>
<tr class="row-odd"><td>Maximum grid dimensions</td>
<td>2147483647, 65535, 65535</td>
</tr>
<tr class="row-even"><td>Maximum number of MPS clients</td>
<td>48</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="further-reading">
<h3>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h3>
<p>For more information on the NVIDIA Volta architecture, please visit the
following (outside) links.</p>
<ul class="simple">
<li><a class="reference external" href="http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" target="_blank">NVIDIA Volta Architecture White Paper</a></li>
<li><a class="reference external" href="https://devblogs.nvidia.com/parallelforall/inside-volta/" target="_blank">NVIDIA PARALLEL FORALL blog article</a></li>
</ul>
</div>
</div>
<div class="section" id="burst-buffer">
<span id="id22"></span><h2>Burst Buffer<a class="headerlink" href="#burst-buffer" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nvme-xfs">
<h3>NVMe (XFS)<a class="headerlink" href="#nvme-xfs" title="Permalink to this headline">¶</a></h3>
<p>Each compute node on Summit has a 1.6TB&nbsp;<strong>N</strong>on-<strong>V</strong>olatile <strong>Me</strong>mory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a “Burst Buffer” with
theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading.
100GB of each NVMe is reserved for NFS cache to help speed access to common
libraries. When calculating maximum usable storage size, this cache and
formatting overhead should be considered; We recommend a maximum storage of
1.4TB (6TB for high-memory nodes). The NVMes could be used to reduce the time that applications wait for
I/O. Using an SSD drive per compute node, the burst buffer will be used to
transfer data to or from the drive before the application reads a file or
after it writes a file.  The result will be that the application benefits from
native SSD performance for a portion of its I/O requests. Users are not
required to use the NVMes.  Data can also be written directly to the parallel
filesystem.</p>
<div class="figure align-center" id="id27">
<img alt="../_images/nvme_arch.jpg" src="../_images/nvme_arch.jpg" />
<p class="caption"><span class="caption-text">The NVMes on Summit are local to each node.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="current-nvme-usage">
<h3>Current NVMe Usage<a class="headerlink" href="#current-nvme-usage" title="Permalink to this headline">¶</a></h3>
<p>Tools for using the burst buffers are still under development.  Currently, the
user will have access to a writeable directory on each node’s NVMe and then
explicitly move data to and from the NVMes with posix commands during a job.
This mode of usage only supports writing&nbsp;file-per-process or file-per-node.
It&nbsp;does not support automatic “n to 1” file writing, writing from multiple nodes
to a single file.  After a job completes the NVMes are trimmed, a process
that&nbsp;irreversibly&nbsp;deletes data from the&nbsp;devices, so all desired data from the
NVMes will need to be copied back to the parallel filesystem before the job
ends. This largely manual mode of usage will not be the recommended way to use
the burst buffer for most applications because tools are actively being
developed to automate and improve the NVMe transfer and data management process.
Here are the basic steps for using the BurstBuffers in their current limited
mode of usage:</p>
<ol class="arabic simple">
<li>Modify your application to write to /mnt/bb/$USER, a directory that will be
created on each NVMe.</li>
<li>Modify either your application or your job submission script to copy the
desired data from /mnt/bb/$USER&nbsp;back to the parallel filesystem before the
job ends.</li>
<li>Modify your job submission script to include the <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">NVME</span></code> &nbsp;bsub
option. Then on each reserved Burst Buffer node will be available a directory
called /mnt/bb/$USER.</li>
<li>Submit your bash script or run the application.</li>
<li>Assemble the resulting data as needed.</li>
</ol>
</div>
<div class="section" id="interactive-jobs-using-the-nvme">
<h3>Interactive Jobs Using the NVMe<a class="headerlink" href="#interactive-jobs-using-the-nvme" title="Permalink to this headline">¶</a></h3>
<p>The NVMe can be setup for test usage within an interactive job as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bsub</span> <span class="o">-</span><span class="n">W</span> <span class="mi">30</span> <span class="o">-</span><span class="n">nnodes</span> <span class="mi">1</span> <span class="o">-</span><span class="n">alloc_flags</span> <span class="s2">&quot;NVME&quot;</span> <span class="o">-</span><span class="n">P</span> <span class="n">project123</span> <span class="o">-</span><span class="n">Is</span> <span class="n">bash</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span> <span class="pre">NVME</span></code> option will create a directory called&nbsp;/mnt/bb/$USER on
each requested node’s NVMe. The <code class="docutils literal notranslate"><span class="pre">/mnt/bb/$USER</span></code> directories will be&nbsp;writeable
and readable until the interactive job ends. Outside of a job <code class="docutils literal notranslate"><span class="pre">/mnt/bb/</span></code> will
be empty and you will not be able to write to it.</p>
</div>
<div class="section" id="nvme-usage-example">
<h3>NVMe Usage Example<a class="headerlink" href="#nvme-usage-example" title="Permalink to this headline">¶</a></h3>
<p>The following example illustrates how to use the burst buffers (NVMes) by
default on Summit. This example uses a submission script, check_nvme.lsf. It is
assumed that the files are saved in the user’s GPFS scratch area,
/gpfs/alpine/scratch/$USER/projid, and that the user is operating from there as
well. Do not forget that for all the commands on NVMe, it is required to use
jsrun. This will submit a job to run on one node.</p>
<p><strong>Job submssion script: check_nvme.lsf.</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#BSUB -P project123
#BSUB -J name_test
#BSUB -o nvme_test.o%J
#BSUB -W 2
#BSUB -nnodes 1
#BSUB -alloc_flags NVME

#Declare your project in the variable
projid=xxxxx
cd /gpfs/alpine/scratch/$USER/$projid

#Save the hostname of the compute node in a file
jsrun -n 1 echo $HOSTNAME &gt; test_file

#Check what files are saved on the NVMe, always use jsrun to access the NVMe devices
jsrun -n 1 ls -l /mnt/bb/$USER/

#Copy the test_file in your NVMe
jsrun -n 1 cp test_file /mnt/bb/$USER/

#Delete the test_file from your local space
rm test_file

#Check again what the NVMe folder contains
jsrun -n 1 ls -l /mnt/bb/$USER/

#Output of the test_file contents
jsrun -n 1 cat /mnt/bb/$USER/test_file

#Copy the file from the NVMe to your local space
jsrun -n 1 cp /mnt/bb/$USER/test_file .

#Check the file locally
ls -l test_file
</pre></div>
</div>
<p>To run this example: <code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">./check_nvme.lsf</span></code>. &nbsp; We could include all the
commands in a script and call this file as a jsrun argument in an interactive
job, in order to avoid changing numbers of processes for all the jsrun
calls. You can see in the table below an example of the differences in a
submission script for executing an application on GPFS and NVMe. In the example,
a binary <code class="docutils literal notranslate"><span class="pre">./btio</span></code> reads input from an input file and generates output files.
In this particular case we copy the binary and the input file onto the NVMe, but
this depends on the application as it is not always necessary, we can execute
the binary on the GPFS and write/read the data from NVMe if it is supported by
the application.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="47%" />
<col width="16%" />
<col width="0%" />
<col width="37%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><em>Using GPFS</em></td>
<td colspan="3"><em>Using NVMe</em></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">xxx</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">xxx</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-J</span> <span class="pre">NAS-BTIO</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-J</span> <span class="pre">NAS-BTIO</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-o</span> <span class="pre">nasbtio.o%J</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-o</span> <span class="pre">nasbtio.o%J</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-e</span> <span class="pre">nasbtio.e%J</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-e</span> <span class="pre">nasbtio.e%J</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">10</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">10</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1</span></code></td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1</span></code></td>
</tr>
<tr class="row-odd"><td rowspan="7"><p class="first"><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">-a</span> <span class="pre">16</span> <span class="pre">-c</span> <span class="pre">16</span> <span class="pre">-r</span> <span class="pre">1</span> <span class="pre">./btio</span></code></p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">-l</span></code>             `</p>
</td>
<td colspan="3"><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-alloc_flags</span> <span class="pre">nvme</span></code></td>
</tr>
<tr class="row-even"><td colspan="3"><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">BBPATH=/mnt/bb/$USER/</span></code></td>
</tr>
<tr class="row-odd"><td colspan="3"><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">btio</span> <span class="pre">${BBPATH}</span></code></td>
</tr>
<tr class="row-even"><td colspan="3"><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">input*</span> <span class="pre">${BBPATH}</span></code></td>
</tr>
<tr class="row-odd"><td colspan="3"><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">-a</span> <span class="pre">16</span> <span class="pre">-c</span> <span class="pre">16</span> <span class="pre">-r</span> <span class="pre">1</span> <span class="pre">${BBPATH}/btio</span></code></td>
</tr>
<tr class="row-even"><td colspan="3"><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">ls</span> <span class="pre">-l</span> <span class="pre">${BBPATH}/</span></code></td>
</tr>
<tr class="row-odd"><td colspan="3"><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">${BBPATH}/*</span> <span class="pre">.</span></code></td>
</tr>
</tbody>
</table>
<p>When a user occupies more than one compute node, then they are using more NVMes
and the I/O can scale linearly. For example in the following plot you can observe
the scalability of the IOR benchmark on 2048 compute nodes on Summit where the
write performance achieves 4TB/s and the read 11.3 TB/s</p>
<img alt="../_images/nvme_ior_summit.png" class="align-center" src="../_images/nvme_ior_summit.png" />
<p>Remember that by default NVMe support one file per MPI process up to one file
per compute node. If users desire a single file as output from data staged on
the NVMe they will need to construct it.  Tools to save automatically checkpoint
files from NVMe to GPFS as also methods that allow automatic n to 1 file writing
with NVMe staging are under development. &nbsp; Tutorials about NVME: &nbsp; Burst Buffer
on Summit (<a class="reference external" href="https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_BB_markomanolis.pdf" target="_blank">slides</a>,
<a class="reference external" href="https://vimeo.com/306890779" target="_blank">video</a>) Summit Burst Buffer Libraries (<a class="reference external" href="https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_BB_zimmer.pdf" target="_blank">slides</a>,
<a class="reference external" href="https://vimeo.com/306891012" target="_blank">video</a>).</p>
</div>
<div class="section" id="spectral-library">
<span id="id23"></span><h3>Spectral Library<a class="headerlink" href="#spectral-library" title="Permalink to this headline">¶</a></h3>
<p>Spectral is a portable and transparent middleware library to enable use of the
node-local burst buffers for accelerated application output on Summit. It is
used to transfer files from node-local NVMe back to the parallel GPFS file
system without the need of the user to interact during the job execution.
Spectral runs on the isolated core of each reserved node, so it does not occupy
resources and based on some parameters the user could define which folder to be
copied to the GPFS. In order to use Spectral, the user has to do the following
steps in the submission script:</p>
<ol class="arabic simple">
<li>Request Spectral resources instead of NVMe</li>
<li>Declare the path where the files will be saved in the node-local NVMe
(PERSIST_DIR)</li>
<li>Declare the path on GPFS where the files will be copied (PFS_DIR)</li>
<li>Execute the script spectral_wait.py when the application is finished in order
to copy the files from NVMe to GPFS</li>
</ol>
<p>The following table shows the differences of executing an application on GPFS,
NVMe, and NVMe with Spectral. This example is using one compute node. We copy
the executable and input file for the NVMe cases but this is not always
necessary. Depending on the application, you could execute the binary from the
GPFS and save the output files on NVMe. Adjust your parameters to copy, if
necessary, the executable and input files onto all the NVMe devices.</p>
<table border="1" class="docutils align-default">
<colgroup>
<col width="29%" />
<col width="35%" />
<col width="35%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><em>Using GPFS</em></td>
<td><em>Using NVMe</em></td>
<td><em>Using NVME with Spectral library</em></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#!/bin/bash</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">xxx</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">xxx</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-P</span> <span class="pre">xxx</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-J</span> <span class="pre">NAS-BTIO</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-J</span> <span class="pre">NAS-BTIO</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-J</span> <span class="pre">NAS-BTIO</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-o</span> <span class="pre">nasbtio.o%J</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-o</span> <span class="pre">nasbtio.o%J</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-o</span> <span class="pre">nasbtio.o%J</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-e</span> <span class="pre">nasbtio.e%J</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-e</span> <span class="pre">nasbtio.e%J</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-e</span> <span class="pre">nasbtio.e%J</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">10</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">10</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-W</span> <span class="pre">10</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-nnodes</span> <span class="pre">1</span></code></td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-alloc_flags</span> <span class="pre">nvme</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">#BSUB</span> <span class="pre">-alloc_flags</span> <span class="pre">spectral</span></code></td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">spectral</span></code></td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">BBPATH=/mnt/bb/$USER</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">BBPATH=/mnt/bb/$USER</span></code></td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">PERSIST_DIR=${BBPATH}</span></code></td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">PFS_DIR=$PWD/spect/</span></code></td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">btio</span> <span class="pre">${BBPATH}</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">btio</span> <span class="pre">${BBPATH}</span></code></td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">input*</span> <span class="pre">${BBPATH}</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">input*</span> <span class="pre">${BBPATH}</span></code></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">-a</span> <span class="pre">16</span> <span class="pre">-c</span> <span class="pre">16</span> <span class="pre">-r</span> <span class="pre">1</span> <span class="pre">./btio</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">-a</span> <span class="pre">16</span> <span class="pre">-c</span> <span class="pre">16</span> <span class="pre">-r</span> <span class="pre">1</span> <span class="pre">${BBPATH}/btio</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">-a</span> <span class="pre">16</span> <span class="pre">-c</span> <span class="pre">16</span> <span class="pre">-r</span> <span class="pre">1</span> <span class="pre">${BBPATH}/btio</span></code></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">-l</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">ls</span> <span class="pre">-l</span> <span class="pre">${BBPATH}/</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">ls</span> <span class="pre">-l</span> <span class="pre">${BBPATH}/</span></code></td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-n</span> <span class="pre">1</span> <span class="pre">cp</span> <span class="pre">${BBPATH}/*</span> <span class="pre">.</span></code></td>
<td><code class="docutils literal notranslate"><span class="pre">spectral_wait.py</span></code></td>
</tr>
</tbody>
</table>
<p>When the Spectral library is not used, any output data produced has to be copied
back from NVMe.  You can observe that with the Spectral library there is no reason
to explicitly ask for the data to be copied to GPFS as it is done automatically
through the spectral_wait.py script. Also a log file called spectral.log will be
created with information on the files that were copied.</p>
</div>
</div>
<div class="section" id="known-issues">
<span id="id24"></span><h2>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this headline">¶</a></h2>
<p>Last Updated: 03 April 2020</p>
<div class="section" id="open-issues">
<h3>Open Issues<a class="headerlink" href="#open-issues" title="Permalink to this headline">¶</a></h3>
<div class="section" id="setting-tmpdir-causes-jsm-jsrun-errors-job-state-flip-flop">
<h4>Setting <code class="docutils literal notranslate"><span class="pre">TMPDIR</span></code> causes JSM (<code class="docutils literal notranslate"><span class="pre">jsrun</span></code>) errors / job state flip-flop<a class="headerlink" href="#setting-tmpdir-causes-jsm-jsrun-errors-job-state-flip-flop" title="Permalink to this headline">¶</a></h4>
<p>Setting the <code class="docutils literal notranslate"><span class="pre">TMPDIR</span></code> environment variable causes jobs to fail with JSM
(<code class="docutils literal notranslate"><span class="pre">jsrun</span></code>) errors and can also cause jobs to bounce back and forth between
eligible and running states until a retry limit has been reached and the job is
placed in a blocked state (NOTE: This “bouncing” of job state can be caused for
multiple reasons. Please see the known issue <a class="reference internal" href="#jobs-suspended-due-to-retry-limit-queued-job-flip-flops-between-queued-running-states">Jobs suspended due to retry limit
/ Queued job flip-flops between queued/running states</a> if you are not setting
<code class="docutils literal notranslate"><span class="pre">TMPDIR</span></code>). A bug has been filed with IBM to address this issue.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">TMPDIR</span></code> is set within a running job (i.e., in an interactive session or
within a batch script), any attempt to call <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> will lead to a job
failure with the following error message:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Error</span><span class="p">:</span> <span class="n">Remote</span> <span class="n">JSM</span> <span class="n">server</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">responding</span> <span class="n">on</span> <span class="n">host</span> <span class="n">batch503</span><span class="o">-</span><span class="mi">25</span><span class="o">-</span><span class="mi">2020</span> <span class="mi">15</span><span class="p">:</span><span class="mi">29</span><span class="p">:</span><span class="mi">45</span><span class="p">:</span><span class="mi">920</span> <span class="mi">90012</span> <span class="n">main</span><span class="p">:</span> <span class="n">Error</span> <span class="n">initializing</span> <span class="n">RM</span> <span class="n">connection</span><span class="o">.</span> <span class="n">Exiting</span><span class="o">.</span>
</pre></div>
</div>
<p>When <code class="docutils literal notranslate"><span class="pre">TMPDIR</span></code> is set before submitting a job (i.e., in the shell/environment
where a job is submitted from), the job will bounce back and forth between a
running and eligible state until its retry limit has been reached and the job
will end up in a blocked state. This is true for both interactive jobs and jobs
submitted with a batch script, but interactive jobs will hang without dropping
you into your interactive shell. In both cases, JSM log files (e.g.,
<code class="docutils literal notranslate"><span class="pre">jsm-lsf-wait.username.1004985.log</span></code>) will be created in the location set for
<code class="docutils literal notranslate"><span class="pre">TMPDIR</span></code> containing the same error message as shown above.</p>
</div>
<div class="section" id="segfault-when-running-executables-on-login-nodes">
<h4>Segfault when running executables on login nodes<a class="headerlink" href="#segfault-when-running-executables-on-login-nodes" title="Permalink to this headline">¶</a></h4>
<p>Executing a parallel binary on the login node or a batch node without using the
job step launcher <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> will result in a segfault.</p>
<p>This also can be encountered when importing parallel Python libraries like
<code class="docutils literal notranslate"><span class="pre">mpi4py</span></code> and <code class="docutils literal notranslate"><span class="pre">h5py</span></code> directly on these nodes.</p>
<p>The issue has been reported to IBM. The current workaround is to run the binary
inside an interactive or batch job via <code class="docutils literal notranslate"><span class="pre">jsrun</span></code>.</p>
</div>
<div class="section" id="nsight-compute-cannot-be-used-with-mpi-programs">
<h4>Nsight Compute cannot be used with MPI programs<a class="headerlink" href="#nsight-compute-cannot-be-used-with-mpi-programs" title="Permalink to this headline">¶</a></h4>
<p>When profiling an MPI application using NVIDIA Nsight Compute, like the following,
you may see an error message in Spectrum MPI that aborts the program:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jsrun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">1</span> <span class="o">-</span><span class="n">a</span> <span class="mi">1</span> <span class="o">-</span><span class="n">g</span> <span class="mi">1</span> <span class="n">nv</span><span class="o">-</span><span class="n">nsight</span><span class="o">-</span><span class="n">cu</span><span class="o">-</span><span class="n">cli</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>

<span class="n">Error</span><span class="p">:</span> <span class="n">common_pami</span><span class="o">.</span><span class="n">c</span><span class="p">:</span><span class="mi">1049</span> <span class="o">-</span> <span class="n">ompi_common_pami_init</span><span class="p">()</span> <span class="n">Unable</span> <span class="n">to</span> <span class="n">create</span> <span class="n">PAMI</span> <span class="n">client</span> <span class="p">(</span><span class="n">rc</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">--------------------------------------------------------------------------</span>
<span class="n">No</span> <span class="n">components</span> <span class="n">were</span> <span class="n">able</span> <span class="n">to</span> <span class="n">be</span> <span class="n">opened</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">pml</span> <span class="n">framework</span><span class="o">.</span>

<span class="n">This</span> <span class="n">typically</span> <span class="n">means</span> <span class="n">that</span> <span class="n">either</span> <span class="n">no</span> <span class="n">components</span> <span class="n">of</span> <span class="n">this</span> <span class="nb">type</span> <span class="n">were</span>
<span class="n">installed</span><span class="p">,</span> <span class="ow">or</span> <span class="n">none</span> <span class="n">of</span> <span class="n">the</span> <span class="n">installed</span> <span class="n">components</span> <span class="n">can</span> <span class="n">be</span> <span class="n">loaded</span><span class="o">.</span>
<span class="n">Sometimes</span> <span class="n">this</span> <span class="n">means</span> <span class="n">that</span> <span class="n">shared</span> <span class="n">libraries</span> <span class="n">required</span> <span class="n">by</span> <span class="n">these</span>
<span class="n">components</span> <span class="n">are</span> <span class="n">unable</span> <span class="n">to</span> <span class="n">be</span> <span class="n">found</span><span class="o">/</span><span class="n">loaded</span><span class="o">.</span>

<span class="n">Host</span><span class="p">:</span>      <span class="o">&lt;</span><span class="n">host</span><span class="o">&gt;</span>
<span class="n">Framework</span><span class="p">:</span> <span class="n">pml</span>
<span class="o">--------------------------------------------------------------------------</span>
<span class="n">PML</span> <span class="n">pami</span> <span class="n">cannot</span> <span class="n">be</span> <span class="n">selected</span>
</pre></div>
</div>
<p>This is due to an incompatibility in the 2019.x versions of Nsight Compute with
Spectrum MPI. As a workaround, you can disable CUDA hooks in Spectrum MPI using</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jsrun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">1</span> <span class="o">-</span><span class="n">a</span> <span class="mi">1</span> <span class="o">-</span><span class="n">g</span> <span class="mi">1</span> <span class="o">--</span><span class="n">smpiargs</span><span class="o">=</span><span class="s2">&quot;-disable_gpu_hooks&quot;</span> <span class="n">nv</span><span class="o">-</span><span class="n">nsight</span><span class="o">-</span><span class="n">cu</span><span class="o">-</span><span class="n">cli</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
</pre></div>
</div>
<p>Unfortunately, this is incompatible with using CUDA-aware MPI in your application.</p>
<p>This will be resolved in a future release of CUDA.</p>
</div>
<div class="section" id="cuda-hook-error-when-program-uses-cuda-without-first-calling-mpi-init">
<h4>CUDA hook error when program uses CUDA without first calling MPI_Init()<a class="headerlink" href="#cuda-hook-error-when-program-uses-cuda-without-first-calling-mpi-init" title="Permalink to this headline">¶</a></h4>
<p>Serial applications, that are not MPI enabled, often face the following
issue when compiled with Spectrum MPI’s wrappers and run with jsrun:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CUDA</span> <span class="n">Hook</span> <span class="n">Library</span><span class="p">:</span> <span class="n">Failed</span> <span class="n">to</span> <span class="n">find</span> <span class="n">symbol</span> <span class="n">mem_find_dreg_entries</span><span class="p">,</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span><span class="p">:</span> <span class="n">undefined</span> <span class="n">symbol</span><span class="p">:</span> <span class="n">__PAMI_Invalidate_region</span>
</pre></div>
</div>
<p>The same issue can occur if CUDA API calls that interact with the GPU
(e.g. allocating memory) are called before MPI_Init() in an MPI enabled
application. Depending on context, this error can either be harmless or
it can be fatal.</p>
<p>The reason this occurs is that the PAMI messaging backend, used by Spectrum
MPI by default, has a “CUDA hook” that records GPU memory allocations.
This record is used later during CUDA-aware MPI calls to efficiently detect
whether a given message is sent from the CPU or the GPU. This is done by
design in the IBM implementation and is unlikely to be changed.</p>
<p>There are two main ways to work around this problem. If CUDA-aware MPI is
not a relevant factor for your work (which is naturally true for serial
applications) then you can simply disable the CUDA hook with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">smpiargs</span><span class="o">=</span><span class="s2">&quot;-disable_gpu_hooks&quot;</span>
</pre></div>
</div>
<p>as an argument to jsrun. Note that this is not compatible with the <code class="docutils literal notranslate"><span class="pre">-gpu</span></code>
argument to <code class="docutils literal notranslate"><span class="pre">--smpiargs</span></code>, since that is what enables CUDA-aware MPI and
the CUDA-aware MPI functionality depends on the CUDA hook.</p>
<p>If you do need CUDA-aware MPI functionality, then the only known working
solution to this problem is to refactor your code so that no CUDA calls
occur before MPI_Init(). (This includes any libraries or programming models
such as OpenACC or OpenMP that would use CUDA behind the scenes.) While it
is not explicitly codified in the standard, it is worth noting that the major
MPI implementations all recommend doing as little as possible before MPI_Init(),
and this recommendation is consistent with that.</p>
</div>
<div class="section" id="spindle-is-not-currently-supported">
<h4>Spindle is not currently supported<a class="headerlink" href="#spindle-is-not-currently-supported" title="Permalink to this headline">¶</a></h4>
<p>Users should not use <code class="docutils literal notranslate"><span class="pre">USE_SPINDLE=1</span></code> or <code class="docutils literal notranslate"><span class="pre">LOAD_SPINDLE=1</span></code> in their
<code class="docutils literal notranslate"><span class="pre">~/.jsm.conf</span></code> file at this time. A bug has been filed with IBM to
address this issue.</p>
</div>
<div class="section" id="spectrum-mpi-tunings-needed-for-maximum-bandwidth">
<h4>Spectrum MPI tunings needed for maximum bandwidth<a class="headerlink" href="#spectrum-mpi-tunings-needed-for-maximum-bandwidth" title="Permalink to this headline">¶</a></h4>
<p>By default, Spectrum MPI is configured for minimum latency. If your
application needs maximum bandwidth, the following settings are
recommended:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ export PAMI_ENABLE_STRIPING=1
$ export PAMI_IBV_ADAPTER_AFFINITY=1
$ export PAMI_IBV_DEVICE_NAME=&quot;mlx5_0:1,mlx5_3:1&quot;
$ export PAMI_IBV_DEVICE_NAME_1=&quot;mlx5_3:1,mlx5_0:1&quot;
</pre></div>
</div>
</div>
<div class="section" id="debugging-slow-application-startup-or-slow-performance">
<h4>Debugging slow application startup or slow performance<a class="headerlink" href="#debugging-slow-application-startup-or-slow-performance" title="Permalink to this headline">¶</a></h4>
<p>In order for debugging and profiling tools to work, you need to unload
Darshan</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ module unload darshan-runtime
</pre></div>
</div>
<p>Spectrum MPI provides a tracing library that can be helpful to gather
more detail information about the MPI communication of your job. To
gather MPI tracing data, you can set
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMPI_LD_PRELOAD_POSTPEND=$OLCF_SPECTRUM_MPI_ROOT/lib/libmpitrace.so</span></code>
in your environment. This will generate profile files with timings for
the individual processes of your job.</p>
<p>In addition, to debug slow startup JSM provides the option to create a
progress file. The file will show information that can be helpful to
pinpoint if a specific node is hanging or slowing down the job step
launch. To enable it, you can use: <code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">--progress</span>
<span class="pre">./my_progress_file_output.txt</span></code>.</p>
</div>
<div class="section" id="a-flag-ignored-when-using-a-jsrun-resource-set-file-with-u">
<h4>-a flag ignored when using a jsrun resource set file with -U<a class="headerlink" href="#a-flag-ignored-when-using-a-jsrun-resource-set-file-with-u" title="Permalink to this headline">¶</a></h4>
<p>When using file-based specification of resource sets with <code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">-U</span></code>,
the <code class="docutils literal notranslate"><span class="pre">-a</span></code> flag (number of tasks per resource set) is ignored. This has
been reported to IBM and they are investigating. It is generally
recommended to use jsrun explicit resource files (ERF) with
<code class="docutils literal notranslate"><span class="pre">--erf_input</span></code> and <code class="docutils literal notranslate"><span class="pre">--erf_output</span></code> instead of <code class="docutils literal notranslate"><span class="pre">-U</span></code>.</p>
</div>
<div class="section" id="jobs-suspended-due-to-retry-limit-queued-job-flip-flops-between-queued-running-states">
<h4>Jobs suspended due to retry limit / Queued job flip-flops between queued/running states<a class="headerlink" href="#jobs-suspended-due-to-retry-limit-queued-job-flip-flops-between-queued-running-states" title="Permalink to this headline">¶</a></h4>
<p>Some users have reported seeing their jobs transition from the normal
queued state, into a running state, and then back again to queued.
Sometimes this can happen multiple times. Eventually, internal limits in
the LSF scheduler will be reached, at which point the job will no longer
be eligible for running. The <code class="docutils literal notranslate"><span class="pre">bhist</span></code> command can be used to see if a job
is cycling between running and eligible states. The pending reason given
by <code class="docutils literal notranslate"><span class="pre">bhist</span></code> can also be useful to debug. This can happen due to
modifications that the user has made to their environment on the system,
incorrect SSH key setup, attempting to load unavailable/broken modules.
or system problems with individual nodes. When jobs are observed to
flip-flop between running and queued, and/or become ineligible without
explanation, then deeper investigation is required and the user should
write to <a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">help<span>&#64;</span>olcf<span>&#46;</span>ornl<span>&#46;</span>gov</a>.</p>
</div>
<div class="section" id="jsrun-explicit-resource-file-erf-output-format-error">
<h4>jsrun explicit resource file (ERF) output format error<a class="headerlink" href="#jsrun-explicit-resource-file-erf-output-format-error" title="Permalink to this headline">¶</a></h4>
<p>jsrun’s option to create an explicit resource file (<code class="docutils literal notranslate"><span class="pre">--erf_output</span></code>) will
incorrectly create a file with one line per rank. When reading the file
in with (<code class="docutils literal notranslate"><span class="pre">--erf_input</span></code>) you will see warnings for overlapping resource
sets. This issue has been reported. The workaround is to manually
update the created ERF file to contain a single line per resource set
with multiple ranks per line.</p>
</div>
<div class="section" id="jsrun-latency-priority-capitalization-allocates-incorrect-resources">
<h4>jsrun latency priority capitalization allocates incorrect resources<a class="headerlink" href="#jsrun-latency-priority-capitalization-allocates-incorrect-resources" title="Permalink to this headline">¶</a></h4>
<p>jsrun’s latency priority (<code class="docutils literal notranslate"><span class="pre">-l</span></code>) flag can be given lowercase values
(i.e. gpu-cpu) or capitalized values (i.e. GPU-CPU).</p>
<p><strong>Expected behavior</strong>:</p>
<blockquote>
<div>When capitalized, jsrun should not compromise on the resource layout,
and will wait to begin the job step until the ideal resources are
available. When given a lowercase value, jsrun will not wait, but
initiate the job step with the most ideal layout as is available at the
time. This also means that when there’s no resource contention, such as
running a single job step at a time, capitalization should not matter,
as they should both yield the same resources.</div></blockquote>
<p><strong>Actual behavior</strong>:</p>
<blockquote>
<div>Capitalizing the latency priority value may allocate incorrect
resources, or even cause the job step to fail entirely.</div></blockquote>
<p><strong>Recommendation</strong>:</p>
<blockquote>
<div>It is currently recommended to only use the lowercase values to (<code class="docutils literal notranslate"><span class="pre">-l</span></code> /
<code class="docutils literal notranslate"><span class="pre">--latency_priority</span></code>). The system default is: gpu-cpu,cpu-mem,cpu-cpu.
Since this ordering is used implicitly when the <code class="docutils literal notranslate"><span class="pre">-l</span></code> flag is omitted, this
issue only impacts submissions which explicitly include a latency
priority in the jsrun command.</div></blockquote>
</div>
<div class="section" id="error-when-using-complex-datatypes-with-mpi-collectives-and-gpudirect">
<h4>Error when using complex datatypes with MPI Collectives and GPUDirect<a class="headerlink" href="#error-when-using-complex-datatypes-with-mpi-collectives-and-gpudirect" title="Permalink to this headline">¶</a></h4>
<p>Users have reported errors when using complex datatypes with MPI
Collectives and GPUDirect:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jsrun</span> <span class="o">--</span><span class="n">smpiargs</span><span class="o">=</span><span class="s2">&quot;-gpu&quot;</span> <span class="o">-</span><span class="n">n</span> <span class="mi">6</span> <span class="o">-</span><span class="n">a</span> <span class="mi">1</span> <span class="o">-</span><span class="n">g</span> <span class="mi">1</span>   <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="n">coll</span><span class="p">:</span><span class="n">ibm</span><span class="p">:</span><span class="n">allreduce</span><span class="p">:</span> <span class="n">GPU</span> <span class="n">awareness</span> <span class="ow">in</span> <span class="n">PAMI</span> <span class="n">requested</span><span class="o">.</span> <span class="n">It</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">safe</span> <span class="n">to</span> <span class="n">defer</span> <span class="n">to</span> <span class="n">another</span> <span class="n">component</span><span class="o">.</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="o">***</span> <span class="n">An</span> <span class="n">error</span> <span class="n">occurred</span> <span class="ow">in</span> <span class="n">MPI_Allreduce</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="o">***</span> <span class="n">reported</span> <span class="n">by</span> <span class="n">process</span> <span class="p">[</span><span class="mi">3199551009</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="o">***</span> <span class="n">on</span> <span class="n">communicator</span> <span class="n">MPI_COMM_WORLD</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="o">***</span> <span class="n">MPI_ERR_UNSUPPORTED_OPERATION</span><span class="p">:</span> <span class="n">operation</span> <span class="ow">not</span> <span class="n">supported</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="o">***</span> <span class="n">MPI_ERRORS_ARE_FATAL</span> <span class="p">(</span><span class="n">processes</span> <span class="ow">in</span> <span class="n">this</span> <span class="n">communicator</span> <span class="n">will</span> <span class="n">now</span> <span class="n">abort</span><span class="p">,</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113506</span><span class="p">]</span> <span class="o">***</span>    <span class="ow">and</span> <span class="n">potentially</span> <span class="n">your</span> <span class="n">MPI</span> <span class="n">job</span><span class="p">)</span>
<span class="p">[</span><span class="n">h35n05</span><span class="p">:</span><span class="mi">113509</span><span class="p">]</span> <span class="n">coll</span><span class="p">:</span><span class="n">ibm</span><span class="p">:</span><span class="n">allreduce</span><span class="p">:</span> <span class="n">GPU</span> <span class="n">awareness</span> <span class="ow">in</span> <span class="n">PAMI</span> <span class="n">requested</span><span class="o">.</span> <span class="n">It</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">safe</span> <span class="n">to</span> <span class="n">defer</span> <span class="n">to</span> <span class="n">another</span> <span class="n">component</span><span class="o">.</span>
</pre></div>
</div>
<p>This is a known issue with libcoll and the SMPI team is working to
resolve it. In the meantime, a workaround is to treat the complex array
as a real array with double the length if the operation is not
MPI_Prod. Note: This requires code modification. An alternative
workaround is to disable IBM optimized collectives. This will impact
performance however but requires no code changes and should be correct
for all MPI_Allreduce operations. You can do this by adding the
following option to your jsrun command line:
<code class="docutils literal notranslate"><span class="pre">--smpiargs=&quot;-HCOLL</span> <span class="pre">-FCA</span> <span class="pre">-mca</span> <span class="pre">coll_hcoll_enable</span> <span class="pre">1</span> <span class="pre">-mca</span> <span class="pre">coll_hcoll_np</span> <span class="pre">0</span>
<span class="pre">-mca</span> <span class="pre">coll</span> <span class="pre">^basic</span> <span class="pre">-mca</span> <span class="pre">coll</span> <span class="pre">^ibm</span> <span class="pre">-async&quot;</span></code></p>
</div>
</div>
<div class="section" id="resolved-issues">
<h3>Resolved Issues<a class="headerlink" href="#resolved-issues" title="Permalink to this headline">¶</a></h3>
<div class="section" id="jsm-fault-tolerance-causes-jobs-to-fail-to-start">
<h4>JSM Fault Tolerance causes jobs to fail to start<a class="headerlink" href="#jsm-fault-tolerance-causes-jobs-to-fail-to-start" title="Permalink to this headline">¶</a></h4>
<p>Adding <code class="docutils literal notranslate"><span class="pre">FAULT_TOLERANCE=1</span></code> in your individual <code class="docutils literal notranslate"><span class="pre">~/.jsm.conf</span></code> file,
will result in LSF jobs failing to successfully start.</p>
<p>The following issues were resolved with the July 16, 2019 software upgrade:</p>
</div>
<div class="section" id="default-nvprof-setting-clobbers-ld-preload-interfering-with-spectrummpi-resolved-july-16-2019">
<h4>Default nvprof setting clobbers <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>, interfering with SpectrumMPI (Resolved: July 16, 2019)<a class="headerlink" href="#default-nvprof-setting-clobbers-ld-preload-interfering-with-spectrummpi-resolved-july-16-2019" title="Permalink to this headline">¶</a></h4>
<p>CUDA 10 adds a new feature to profile CPU side OpenMP constructs (see
<a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp" target="_blank">https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp</a>).
This feature is enabled by default and has a bug which will cause it to
overwrite the contents of <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>. SpectrumMPI requires a library
(<code class="docutils literal notranslate"><span class="pre">libpami_cuda_hook.so</span></code>) to be preloaded in order to function. All MPI
applications on Summit will break when run in nvprof with default
settings. The workaround is to disable the new OpenMP profiling feature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ jsrun  nvprof --openmp-profiling off
</pre></div>
</div>
</div>
<div class="section" id="csm-based-launch-is-not-currently-supported-resolved-july-16-2019">
<h4>CSM-based launch is not currently supported (Resolved: July 16, 2019)<a class="headerlink" href="#csm-based-launch-is-not-currently-supported-resolved-july-16-2019" title="Permalink to this headline">¶</a></h4>
<p>Users should not use <code class="docutils literal notranslate"><span class="pre">JSMD_LAUNCH_MODE=csm</span></code> in their <code class="docutils literal notranslate"><span class="pre">~/.jsm.conf</span></code>
file at this time. A bug has been filed with IBM to address this issue.</p>
</div>
<hr class="docutils" />
<div class="section" id="parallel-i-o-crash-on-gpfs-with-latest-mpi-romio">
<h4>Parallel I/O crash on GPFS with latest MPI ROMIO<a class="headerlink" href="#parallel-i-o-crash-on-gpfs-with-latest-mpi-romio" title="Permalink to this headline">¶</a></h4>
<p>In some cases with large number of MPI processes when there is not
enough memory available on the compute node, the Abstract-Device
Interface for I/O (ADIO) driver can break with this error:</p>
<p>Out of memory
in file
../../../../../../../opensrc/ompi/ompi/mca/io/romio321/romio/adio/ad_gpfs/ad_gpfs_rdcoll.c,
line 1178</p>
<p>The solution is to declare in your submission script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">GPFSMPIO_COMM</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>This command will use non-blocking MPI calls and not MPI_Alltoallv for
exchange of data between the MPI I/O aggregators which requires
significant more amount of memory.</p>
<hr class="docutils" />
<p>The following issues were resolved
with the May 21, 2019 upgrade:</p>
</div>
<div class="section" id="g-flag-causes-internal-compiler-error-with-xl-compiler-resolved-may-21-2019">
<h4>-g flag causes internal compiler error with XL compiler (Resolved: May 21, 2019)<a class="headerlink" href="#g-flag-causes-internal-compiler-error-with-xl-compiler-resolved-may-21-2019" title="Permalink to this headline">¶</a></h4>
<p>Some users have reported an internal compiler error when compiling their
code with XL with the `-g` flag. This has been reported to IBM and
they are investigating.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This bug was fixed in xl/16.1.1-3</p>
</div>
</div>
<div class="section" id="issue-with-cuda-aware-mpi-with-1-resource-set-per-node-resolved-may-21-2019">
<h4>Issue with CUDA Aware MPI with &gt;1 resource set per node (Resolved: May 21, 2019)<a class="headerlink" href="#issue-with-cuda-aware-mpi-with-1-resource-set-per-node-resolved-may-21-2019" title="Permalink to this headline">¶</a></h4>
<p>Attempting to run an application with CUDA aware MPI using more than one
resource set per node with produce the following error on each MPI rank:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">__SMPI_build_dir__________________________________________</span><span class="o">/</span><span class="n">ibmsrc</span><span class="o">/</span><span class="n">pami</span><span class="o">/</span><span class="n">ibm</span><span class="o">-</span><span class="n">pami</span><span class="o">/</span><span class="n">buildtools</span><span class="o">/</span><span class="n">pami_build_port</span><span class="o">/../</span><span class="n">pami</span><span class="o">/</span><span class="n">components</span><span class="o">/</span><span class="n">devices</span><span class="o">/</span><span class="n">ibvdevice</span><span class="o">/</span><span class="n">CudaIPCPool</span><span class="o">.</span><span class="n">h</span><span class="p">:</span><span class="mi">300</span><span class="p">:</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="n">Error</span> <span class="n">opening</span> <span class="n">IPC</span> <span class="n">Memhandle</span> <span class="kn">from</span> <span class="nn">peer</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">invalid</span> <span class="n">argument</span>
<span class="n">CUDA</span> <span class="n">level</span> <span class="n">IPC</span> <span class="n">failure</span><span class="p">:</span> <span class="n">this</span> <span class="n">has</span> <span class="n">been</span> <span class="n">observed</span> <span class="ow">in</span> <span class="n">environments</span> <span class="n">where</span> <span class="n">cgroups</span> <span class="n">separate</span> <span class="n">the</span> <span class="n">visible</span> <span class="n">GPUs</span> <span class="n">between</span> <span class="n">ranks</span><span class="o">.</span> <span class="n">The</span> <span class="n">option</span> <span class="o">-</span><span class="n">x</span> <span class="n">PAMI_DISABLE_IPC</span><span class="o">=</span><span class="mi">1</span> <span class="n">can</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">disable</span> <span class="n">CUDA</span> <span class="n">level</span> <span class="n">IPC</span><span class="o">.</span><span class="p">[:]</span> <span class="o">***</span> <span class="n">Process</span> <span class="n">received</span> <span class="n">signal</span> <span class="o">***</span>
</pre></div>
</div>
<p>Spectrum MPI relies on CUDA Inter-process Communication (CUDA IPC) to
provide fast on-node between GPUs. At present this capability cannot
function with more than one resource set per node.</p>
<ol class="arabic simple">
<li>Set the environment variable <code class="docutils literal notranslate"><span class="pre">PAMI_DISABLE_IPC=1</span></code> to force Spectrum
MPI to not use fast GPU Peer-to-peer communication. This option will
allow your code to run with more than one resource set per host, but
you may see slower GPU to GPU communication.</li>
<li>Run in a single resource set per host, i.e. with
<code class="docutils literal notranslate"><span class="pre">jsrun</span> <span class="pre">--gpu_per_rs</span> <span class="pre">6</span></code></li>
</ol>
<p>If on-node MPI communication between GPUs is critical to your
application performance, option B is recommended but you’ll need to set
the GPU affinity manually. This could be done with an API call in your
code (e.g. <code class="docutils literal notranslate"><span class="pre">cudaSetDevice</span></code>), or by using a wrapper script.</p>
</div>
<div class="section" id="simultaneous-backgrounded-jsruns-resolved-may-21-2019">
<h4>Simultaneous backgrounded jsruns (Resolved: May 21, 2019)<a class="headerlink" href="#simultaneous-backgrounded-jsruns-resolved-may-21-2019" title="Permalink to this headline">¶</a></h4>
<p>We have seen occasional errors from batch jobs with multiple
simultaneous backgrounded jsrun commands. Jobs may see pmix errors
during the noted failures.</p>
<hr class="docutils" />
<p>The following issue was resolved with the software default changes from
March 12, 2019 that set Spectrum MPI 10.2.0.11 (20190201) as default and
moved ROMIO to version 3.2.1:</p>
</div>
<div class="section" id="slow-performance-using-parallel-hdf5-resolved-march-12-2019">
<h4>Slow performance using parallel HDF5 (Resolved: March 12, 2019)<a class="headerlink" href="#slow-performance-using-parallel-hdf5-resolved-march-12-2019" title="Permalink to this headline">¶</a></h4>
<p>A performance issue has been identified using parallel HDF5 with the
default version of ROMIO provided in
<code class="docutils literal notranslate"><span class="pre">spectrum-mpi/10.2.0.10-20181214</span></code>. To fully take advantage of parallel
HDF5, users need to switch to the newer version of ROMIO and use ROMIO
hints. The following shows recommended variables and hints for a 2 node
job. Please note that hints must be tuned for a specific job.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ module unload darshan-runtime
$ export OMPI_MCA_io=romio321
$ export ROMIO_HINTS=./my_romio_hints
$ cat $ROMIO_HINTS
romio_cb_write enable
romio_ds_write enable
cb_buffer_size 16777216
cb_nodes 2
</pre></div>
</div>
</div>
<div class="section" id="job-hangs-in-mpi-finalize-resolved-march-12-2019">
<h4>Job hangs in MPI_Finalize (Resolved: March 12, 2019)<a class="headerlink" href="#job-hangs-in-mpi-finalize-resolved-march-12-2019" title="Permalink to this headline">¶</a></h4>
<p>There is a known issue in Spectrum MPI 10.2.0.10 provided by the
<code class="docutils literal notranslate"><span class="pre">spectrum-mpi/10.2.0.10-20181214</span></code> modulefile that causes a hang in
<code class="docutils literal notranslate"><span class="pre">MPI_Finalize</span></code> when ROMIO 3.2.1 is being used and the
<code class="docutils literal notranslate"><span class="pre">darshan-runtime</span></code> modulefile is loaded. The recommended and default
Spectrum MPI version as of March 3, 2019 is Spectrum MPI 10.2.0.11
provided by the <code class="docutils literal notranslate"><span class="pre">spectrum-mpi/10.2.0.11-20190201</span></code> modulefile. If you
are seeing this issue, please make sure that you are using the latest
version of Spectrum MPI. If you need to use a previous version of
Spectrum MPI, your options are:</p>
<ul class="simple">
<li>Unload the <code class="docutils literal notranslate"><span class="pre">darshan-runtime</span></code> modulefile.</li>
<li>Alternatively, set <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMPI_MCA_io=romio314</span></code> in your
environment to use the previous version of ROMIO. Please note that
this version has known performance issues with parallel HDF5 (see
“Slow performance using parallel HDF5” issue below).</li>
</ul>
<hr class="docutils" />
<p>The following issues were resolved with the February 19, 2019 upgrade:</p>
</div>
<div class="section" id="job-step-cgroups-are-not-currently-supported-resolved-february-19-2019">
<h4>Job step cgroups are not currently supported (Resolved: February 19, 2019)<a class="headerlink" href="#job-step-cgroups-are-not-currently-supported-resolved-february-19-2019" title="Permalink to this headline">¶</a></h4>
<p>A regression was introduced in JSM 10.02.00.10rtm2 that prevents job
step cgroups from being created as a result, JSM, is defaulting to
setting <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> in order to allocate GPUs to specific
resource sets. Because of this issue, even if using <code class="docutils literal notranslate"><span class="pre">--gpu_per_rs</span> <span class="pre">0</span></code>
or <code class="docutils literal notranslate"><span class="pre">-g</span> <span class="pre">0</span></code>, every resource set in the step will be able to see all 6
GPUs in a node.</p>
</div>
<div class="section" id="jsm-stdio-options-do-not-create-files-resolved-february-19-2019">
<h4>JSM stdio options do not create files (Resolved: February 19, 2019)<a class="headerlink" href="#jsm-stdio-options-do-not-create-files-resolved-february-19-2019" title="Permalink to this headline">¶</a></h4>
<p>When using <code class="docutils literal notranslate"><span class="pre">--stdio_stdout</span></code> or <code class="docutils literal notranslate"><span class="pre">--stdio_stderr</span></code> users must use
absolute paths. Using relative paths (e.g. <code class="docutils literal notranslate"><span class="pre">./my_stdout</span></code>) will not
successfully create the file in the user’s current working directory. An
bug has been filed with IBM to fix this issue and allow relative paths.</p>
</div>
<div class="section" id="jsm-crash-when-using-different-number-of-resource-sets-per-host-resolved-february-19-2019">
<h4>JSM crash when using different number of resource sets per host (Resolved: February 19, 2019)<a class="headerlink" href="#jsm-crash-when-using-different-number-of-resource-sets-per-host-resolved-february-19-2019" title="Permalink to this headline">¶</a></h4>
<p>In some cases users will encounter a segmentation fault when running job
steps that have uneven number of resource sets per node. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ jsrun --nrs 41 -c 21 -a 1 --bind rs ./a.out
[a03n07:74208] *** Process received signal ***
[a03n07:74208] Signal: Segmentation fault (11)
[a03n07:74208] Signal code: Address not mapped (1)
[a03n07:74208] Failing at address: (nil)
...
</pre></div>
</div>
<p>As a workaround, two environment variables are set as default in the
user environment <code class="docutils literal notranslate"><span class="pre">PAMI_PMIX_USE_OLD_MAPCACHE=1</span></code> and
<code class="docutils literal notranslate"><span class="pre">OMPI_MCA_coll_ibm_xml_disable_cache=1</span></code>.</p>
</div>
</div>
<div class="section" id="cuda-10-1-known-issues">
<h3>CUDA 10.1 Known Issues<a class="headerlink" href="#cuda-10-1-known-issues" title="Permalink to this headline">¶</a></h3>
<div class="section" id="intermittent-failures-with-nvprof-identified-july-11-2019">
<h4>Intermittent failures with `nvprof` (Identified: July 11, 2019)<a class="headerlink" href="#intermittent-failures-with-nvprof-identified-july-11-2019" title="Permalink to this headline">¶</a></h4>
<p>We are seeing an intermittent issue that causes an error when
profiling a code using <cite>nvprof</cite> from CUDA 10.1.168. We have filed
a bug with NVIDIA (NV bug 2645669) and they have reproduced the
problem. An update will be posted when a fix becomes available.</p>
<p>When this issue is encountered, the profiler will exit with the
following error message:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==</span><span class="mi">99756</span><span class="o">==</span> <span class="n">NVPROF</span> <span class="ow">is</span> <span class="n">profiling</span> <span class="n">process</span> <span class="mi">99756</span><span class="p">,</span> <span class="n">command</span><span class="p">:</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
<span class="o">==</span><span class="mi">99756</span><span class="o">==</span> <span class="n">Error</span><span class="p">:</span> <span class="n">Internal</span> <span class="n">profiling</span> <span class="n">error</span> <span class="mi">4306</span><span class="p">:</span><span class="mf">999.</span>
<span class="o">========</span> <span class="n">Profiling</span> <span class="n">result</span><span class="p">:</span>
<span class="o">========</span> <span class="n">Metric</span> <span class="n">result</span><span class="p">:</span>
</pre></div>
</div>
</div>
<div class="section" id="mpi-annotation-may-cause-segfaults-with-applications-using-mpi-init-thread">
<h4>MPI annotation may cause segfaults with applications using MPI_Init_thread<a class="headerlink" href="#mpi-annotation-may-cause-segfaults-with-applications-using-mpi-init-thread" title="Permalink to this headline">¶</a></h4>
<p>Users on Summit can have MPI calls automatically annotated in <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>
timelines using the <code class="docutils literal notranslate"><span class="pre">nvprof</span> <span class="pre">--annotate-mpi</span> <span class="pre">openmpi</span></code> option. If the
user calls <code class="docutils literal notranslate"><span class="pre">MPI_Init_thread</span></code> instead of <code class="docutils literal notranslate"><span class="pre">MPI_Init</span></code>, <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> may
segfault, as <code class="docutils literal notranslate"><span class="pre">MPI_Init_thread</span></code> is currently not being wrapped by
<code class="docutils literal notranslate"><span class="pre">nvprof</span></code>. The current alternative is to build and follow the
instructions from
<a class="reference external" href="https://github.com/NVIDIA/cuda-profiler/tree/mpi_init_thread" target="_blank">https://github.com/NVIDIA/cuda-profiler/tree/mpi_init_thread</a>.</p>
</div>
<div class="section" id="cudamemadvise-before-context-creation-leads-to-a-kernel-panic">
<h4>cudaMemAdvise before context creation leads to a kernel panic<a class="headerlink" href="#cudamemadvise-before-context-creation-leads-to-a-kernel-panic" title="Permalink to this headline">¶</a></h4>
<p>There is a (very rare) driver bug involving cudaManagedMemory that can
cause a kernel panic. If you encounter this bug, please contact the
<a class="reference external" href="mailto:help&#37;&#52;&#48;olcf&#46;ornl&#46;gov" target="_blank">OLCF User Support</a> team. The easiest
mitigation is for the user code to initialize a context on every GPU
with which it intends to interact (for example by calling
<code class="docutils literal notranslate"><span class="pre">cudaFree(0)</span></code> while each device is active).</p>
</div>
<div class="section" id="some-uses-of-thrust-complex-vectors-fail-at-compile-time-with-warnings-of-identifiers-being-undefined-in-device-code">
<h4>Some uses of Thrust complex vectors fail at compile time with warnings of identifiers being <code class="docutils literal notranslate"><span class="pre">undefined</span> <span class="pre">in</span> <span class="pre">device</span> <span class="pre">code</span></code><a class="headerlink" href="#some-uses-of-thrust-complex-vectors-fail-at-compile-time-with-warnings-of-identifiers-being-undefined-in-device-code" title="Permalink to this headline">¶</a></h4>
<p>This issue comes from the fact that <code class="docutils literal notranslate"><span class="pre">std::complex</span></code> is not
<code class="docutils literal notranslate"><span class="pre">__host__</span></code>/<code class="docutils literal notranslate"><span class="pre">__device__</span></code> annotated, so all its functions are
implicitly <code class="docutils literal notranslate"><span class="pre">__host__</span></code>. There is a mostly simple workaround, assuming
this is compiled as C++11: in <code class="docutils literal notranslate"><span class="pre">complex.h</span></code> and <code class="docutils literal notranslate"><span class="pre">complex.inl</span></code>,
annotate the functions that deal with <code class="docutils literal notranslate"><span class="pre">std::complex</span></code> as
<code class="docutils literal notranslate"><span class="pre">__host__</span> <span class="pre">__device__</span></code> (they are the ones that are annotated only as
<code class="docutils literal notranslate"><span class="pre">__host__</span></code> right now), and then compile with
<code class="docutils literal notranslate"><span class="pre">--expt-relaxed-constexpr</span></code>.</p>
<p>Users that encounter this issue, can use
the following workaround. copy the entirety of
<code class="docutils literal notranslate"><span class="pre">${OLCF_CUDA_ROOT}/include/thrust</span></code> to a private location, make the
above edits to <code class="docutils literal notranslate"><span class="pre">thrust/complex.h</span></code> and
<code class="docutils literal notranslate"><span class="pre">thrust/detail/complex/complex.inl</span></code>, and then add that to your include
path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ nvcc -ccbin=g++ --expt-relaxed-constexpr assignment.cu -I./
</pre></div>
</div>
<p>A permanent fix of this issue is expected in the version of Thrust
packed with CUDA 10.1 update 1</p>
</div>
<div class="section" id="breakpoints-in-cuda-kernels-recommendation">
<h4>Breakpoints in CUDA kernels recommendation<a class="headerlink" href="#breakpoints-in-cuda-kernels-recommendation" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">cuda-gdb</span></code> allows for breakpoints to be set inside CUDA kernels to
inspect the program state on the GPU. This can be a valuable debugging
tool but breaking inside kernels does incur significant overhead that
should be included in your expected runtime.</p>
<p>The time required to hit a breakpoint inside a CUDA kernel depends on
how many CUDA threads are used to execute the kernel. It may take
several seconds to stop at kernel breakpoints for very large numbers
of threads. For this reason, it is recommended to choose breakpoints
judiciously, especially when running the debugger in “batch” or
“offline” mode where this overhead may be misperceived as the code
hanging. If possible, debugging a smaller problem size with fewer
active threads can be more pleasant.</p>
<hr class="docutils" />
</div>
</div>
</div>
<div class="section" id="training-system-ascent">
<span id="id25"></span><h2>Training System (Ascent)<a class="headerlink" href="#training-system-ascent" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ascent is a training system that is not intended to be used as
an OLCF user resource. Access to the system is only obtained through
OLCF training events.</p>
</div>
<p>Ascent is an 18-node stand-alone system with the same architecture as
Summit (see <a class="reference internal" href="#summit-nodes"><span class="std std-ref">Summit Nodes</span></a> section above), so most of this Summit User Guide can be referenced for
Ascent as well. However, aside from the number of compute nodes, there
are other differences between the two systems. Most notably, Ascent sits
in the NCCS Open Security Enclave, which is subject to fewer
restrictions than the Moderate Security Enclave that systems such as
Summit belong to. This means that participants in OLCF training events
can go through a streamlined version of the approval process before
gaining access to the system. The remainder of this section of the user
guide describes “Ascent-specific” information intended for participants
of OLCF training events.</p>
<div class="section" id="id26">
<h3>File Systems<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<p>It is important to note that because Ascent sits in the NCCS Open
Security Enclave, it also mounts different file systems than Summit.
These file systems provide both user-affiliated and project-affiliated
storage areas for each user.</p>
<div class="section" id="nfs-directories">
<h4>NFS Directories<a class="headerlink" href="#nfs-directories" title="Permalink to this headline">¶</a></h4>
<p>Upon logging into Ascent, users will be placed in their own personal
home (NFS) directory, <code class="docutils literal notranslate"><span class="pre">/ccsopen/home/[userid]</span></code>, which can only be
accessed by that user. Users also have access to an NFS project
directory, <code class="docutils literal notranslate"><span class="pre">/ccsopen/proj/[projid]</span></code>, which is visible to all members
of a project. Both of these NFS directories are commonly used to store
source code and build applications.</p>
</div>
<div class="section" id="gpfs-directories">
<h4>GPFS Directories<a class="headerlink" href="#gpfs-directories" title="Permalink to this headline">¶</a></h4>
<p>Users also have access to a (GPFS) parallel file system, called wolf,
which is where data should be written when running on Ascent’s compute
nodes. Under <code class="docutils literal notranslate"><span class="pre">/gpfs/wolf/[projid]</span></code>, there are 3 directories:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ls /gpfs/wolf/[projid]
proj-shared  scratch  world-shared
</pre></div>
</div>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">proj-shared</span></code> can be accessed by all members of a project.</li>
<li><code class="docutils literal notranslate"><span class="pre">scratch</span></code> contains directories for each user of a project and only
that user can access their own directory.</li>
<li><code class="docutils literal notranslate"><span class="pre">world-shared</span></code> can be accessed by any users on the system in any
project.</li>
</ul>
</div>
</div>
<div class="section" id="obtaining-access-to-ascent">
<h3>Obtaining Access to Ascent<a class="headerlink" href="#obtaining-access-to-ascent" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ascent is a training system that is not intended to be used as
an OLCF user resource. Access to the system is only obtained through
OLCF training events.</p>
</div>
<p>This sub-section describes the process of obtaining access to Ascent for
an OLCF training event. Please follow the steps below to request access.</p>
<div class="section" id="step-1-fill-out-and-submit-an-olcf-account-application-form">
<h4>Step 1: Fill out and submit an <a class="reference external" href="https://www.olcf.ornl.gov/for-users/documents-forms/olcf-account-application" target="_blank">OLCF Account Application Form</a><a class="headerlink" href="#step-1-fill-out-and-submit-an-olcf-account-application-form" title="Permalink to this headline">¶</a></h4>
<p>Enter the requested information into the form. For “Project
Information”, enter the following:</p>
<img alt="../_images/Ascent_Account_Application_1.png" class="align-center" src="../_images/Ascent_Account_Application_1.png" />
<p>For “Project Information”, enter the following:</p>
<img alt="../_images/Ascent_Account_Application_2.png" class="align-center" src="../_images/Ascent_Account_Application_2.png" />
<p>For “Account Information”, enter the following:</p>
<img alt="../_images/Ascent_Account_Application_3.png" class="align-center" src="../_images/Ascent_Account_Application_3.png" />
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">After submitting your application, it will need to pass
through the approval process. Depending on when you submit, approval
might not occur until the next business day.</p>
</div>
</div>
<div class="section" id="step-2-set-your-xcams-ucams-password">
<h4>Step 2: Set Your XCAMS/UCAMS Password<a class="headerlink" href="#step-2-set-your-xcams-ucams-password" title="Permalink to this headline">¶</a></h4>
<p>Once approved, if you are a new user, your account will be created and
an email will be sent asking you to set up a password. If you already
had an XCAMS/UCAMS account, you will not be sent the email asking you to
setup a new password (simply use your existing credentials). Once
passwords are known, users can log in to Ascent using their XCAMS/UCAMS
username and password (see the next section)</p>
</div>
</div>
<div class="section" id="logging-in-to-ascent">
<h3>Logging In to Ascent<a class="headerlink" href="#logging-in-to-ascent" title="Permalink to this headline">¶</a></h3>
<p>To log in to Ascent, please use your XCAMS/UCAMS username and password:</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">ssh</span> <span class="pre">USERNAME&#64;login1.ascent.olcf.ornl.gov</span></code></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You do not need to use an RSA token to log in to Ascent.
Please use your XCAMS/UCAMS username and password (which is different
from the username and PIN + RSA token code used to log in to other OLCF
systems such as Summit).</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It will take ~5 minutes for your directories to be created, so
if your account was just created and you log in and you do not have a
home directory, this is likely the reason.</p>
</div>
</div>
</div>
<div class="section" id="preparing-for-frontier">
<h2>Preparing For Frontier<a class="headerlink" href="#preparing-for-frontier" title="Permalink to this headline">¶</a></h2>
<p>This section of the Summit User Guide is intended to show current OLCF
users how to start preparing their applications to run on the upcoming
Frontier system. We will continue to add more topics to this section in
the coming months. Please see the topics below to get started.</p>
<div class="section" id="hip">
<h3>HIP<a class="headerlink" href="#hip" title="Permalink to this headline">¶</a></h3>
<p>HIP (Heterogeneous-Compute Interface for Portability) is a C++ runtime
API that allows developers to write portable code to run on AMD and NVIDIA
GPUs. It is an interface that uses the underlying Radeon Open Compute (ROCm)
or CUDA platform that is installed on a system. The API is similar to CUDA
so porting existing codes from CUDA to HIP should be fairly straightforward
in most cases. In addition, HIP provides porting tools which can be used to
help port CUDA codes to the HIP layer, with no overhead compared to the
original CUDA application. HIP is not intended to be a drop-in replacement
for CUDA, so some manual coding and performance tuning work should be
expected to complete the port.</p>
<p>Key features include:</p>
<ul class="simple">
<li>HIP is a thin layer and has little or no performance impact over
coding directly in CUDA.</li>
<li>HIP allows coding in a single-source C++ programming language including
features such as templates, C++11 lambdas, classes, namespaces, and more.</li>
<li>The “hipify” tools automatically convert source from CUDA to HIP.</li>
<li>Developers can specialize for the platform (CUDA or HIP) to tune for
performance or handle tricky cases.</li>
</ul>
</div>
<div class="section" id="using-hip-on-summit">
<h3>Using HIP on Summit<a class="headerlink" href="#using-hip-on-summit" title="Permalink to this headline">¶</a></h3>
<p>As mentioned above, HIP can be used on systems running on either the ROCm
or CUDA platform, so OLCF users can start preparing their applications for
Frontier today on Summit. To use HIP on Summit, you must load the HIP module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ module load hip
</pre></div>
</div>
<p>This will automatically load the appropriate CUDA module as well.</p>
</div>
<div class="section" id="learning-to-program-with-hip">
<h3>Learning to Program with HIP<a class="headerlink" href="#learning-to-program-with-hip" title="Permalink to this headline">¶</a></h3>
<p>The HIP API is very similar to CUDA, so if you are already familiar with
using CUDA, the transition to using HIP should be fairly straightforward.
Whether you are already familiar with CUDA or not, the best place to start
learning about HIP is this Introduction to HIP webinar that was recently
given by AMD:</p>
<ul class="simple">
<li><strong>Introduction to AMD GPU Programming with HIP</strong>:
(<a class="reference external" href="https://www.exascaleproject.org/wp-content/uploads/2017/05/ORNL_HIP_webinar_20190606_final.pdf" target="_blank">slides</a> | <a class="reference external" href="https://youtu.be/3ZXbRJVvgJs" target="_blank">recording</a>)</li>
</ul>
<p>More useful resources, provided by AMD, can be found here:</p>
<ul class="simple">
<li><a class="reference external" href="https://rocm-documentation.readthedocs.io/en/latest/Programming_Guides/HIP-GUIDE.html" target="_blank">HIP Programming Guide</a></li>
<li><a class="reference external" href="https://rocm-documentation.readthedocs.io/en/latest/ROCm_API_References/HIP-API.html" target="_blank">HIP API Documentation</a></li>
<li><a class="reference external" href="https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_porting_guide.md" target="_blank">HIP Porting Guide</a></li>
</ul>
<p>The OLCF is currently adding some simple HIP tutorials here as well:</p>
<ul class="simple">
<li>OLCF Tutorials – <a class="reference external" href="https://github.com/olcf-tutorials/simple_HIP_examples" target="_blank">Simple HIP Examples</a></li>
</ul>
</div>
<div class="section" id="previous-frontier-training-events">
<h3>Previous Frontier Training Events<a class="headerlink" href="#previous-frontier-training-events" title="Permalink to this headline">¶</a></h3>
<p>The links below point to event pages from previous Frontier training events. Under the “Presentations” tab on each event page, you will find the presentations given during the event.</p>
<p><a class="reference external" href="https://www.olcf.ornl.gov/frontier-application-readiness-kick-off-workshop/" target="_blank">Frontier Application Readiness Kick-Off Workshop (October 2019)</a></p>
<p>Please check back to this section regularly as we will continue
to add new content for our users.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rhea_user_guide.html" class="btn btn-neutral float-right" title="Rhea User Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Systems" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, OLCF

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    



</body>
</html>